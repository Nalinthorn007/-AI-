{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import List, Optional\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from groq import Groq\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    @staticmethod\n",
    "    def load_docs(file_paths: List[str]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        ໂຫລດ PDF documents ໂດຍໃຊ້ LangChain PyPDFLoader ເພາະຍັງແອັດຈັງມັກເພາະມັນສ້າງ Metadata ໃຫ້ Auto \n",
    "\n",
    "        Metadata ຄືຍັງ ? \n",
    "        Metadata ຄືຂໍ້ມູນເພີ່ມເຕີມກ່ຽວກັບເອກະສານ\n",
    "        ແຕ່ລະ Document ຈະມີ 2 ສ່ວນຫຼັກ:\n",
    "        1. page_content: ເນື້ອໃນຂໍ້ຄວາມຈິງໆ\n",
    "        2. metadata: ຂໍ້ມູນລາຍລະອຽດກ່ຽວກັບເອກະສານ ເພື່ອບົງບອກວ່າ ເວລາເຮົາ ຄົ້ນຫາຂໍ້ມູນ ແຫຼ່ງຂໍ້ມູນນັ້ນມາຈາກໄສ\n",
    "        \n",
    "        Args:\n",
    "            file_paths (list): List of PDF file paths\n",
    "        \n",
    "        Returns:\n",
    "            List[Document]: List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        \n",
    "        all_docs = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Warning: File {file_path} not found. Skipping...\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                file_extension = os.path.splitext(file_path)[1].lower()\n",
    "                \n",
    "                # Check if file is PDF\n",
    "                if file_extension != '.pdf':\n",
    "                    print(f\"Warning: {file_path} is not a PDF file. Skipping...\")\n",
    "                    continue\n",
    "                \n",
    "                # Load PDF using LangChain PyPDFLoader\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                documents = loader.load()\n",
    "                \n",
    "                # Add enhanced metadata to all documents\n",
    "                for doc in documents:\n",
    "                    if doc.metadata is None:\n",
    "                        doc.metadata = {}\n",
    "                        \n",
    "                    doc.metadata.update({\n",
    "                        'source_file': os.path.basename(file_path),\n",
    "                        'file_type': file_extension,\n",
    "                        'file_path': file_path,\n",
    "                        'file_size': os.path.getsize(file_path) if os.path.exists(file_path) else 0,\n",
    "                    })\n",
    "                \n",
    "                all_docs.extend(documents)\n",
    "                print(f\"✅ Processed PDF: {file_path} ({len(documents)} pages)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {file_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"📚 Total PDF documents loaded: {len(all_docs)}\")\n",
    "        return all_docs\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_documents_standard(\n",
    "        docs: List[Document], \n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        tokenizer_model: str = \"D:/model/BAAI-bge-m3\",\n",
    "        max_token_limit: int = 8192\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        ໃຊ້ Lanchain ໃນການເຮັດ chunking ຂໍ້ມູນ ເພື່ອການຄົ້ນຫາຂໍ້ມູນດ້ວຍ FAISS  \n",
    "\n",
    "        Chunk_size: ແມ່ນຈຳນວນຂໍ້ມູນທີ່ຈະເຮັດ chunking ຕໍ່ຫນ່ວຍ ເພາະຍັງ ເຮົາບໍ່ສາມາດເອົາເອກະສານທັ້ງໝົດໃຫ້ AI ຕອບໄດ້ ເນື່ອງຈາກບາງເອກະສານມີຫລາຍຫນ້າ\n",
    "        Chunk_overlap: ແມ່ນຈຳນວນຂໍ້ມູນທີ່ຈະເຮັດ chunking ຕໍ່ຫນ່ວຍ ເພາະຍັງ ເຮົາບໍ່ສາມາດເອົາເອກະສານທັ້ງໝົດໃຫ້ AI ຕອບໄດ້ ເນື່ອງຈາກບາງເອກະສານມີຫລາຍຫນ້າ\n",
    "        Tokenizer_model: ແມ່ນ Model ທີ່ເຮົາຈະໃຊ້ໃນການເຮັດ chunking ຂໍ້ມູນ ເພື່ອການຄົ້ນຫາຂໍ້ມູນດ້ວຍ FAISS\n",
    "        Max_token_limit: ແມ່ນການແບ່ງສັດສ່ວນໃຫ້ເຫມາະສົມກັບ chunk_size\n",
    "        \n",
    "        Args:\n",
    "            docs: List of LangChain Document objects\n",
    "            chunk_size: Target size for each chunk in tokens\n",
    "            chunk_overlap: Number of overlapping tokens between chunks\n",
    "            tokenizer_model: Path to tokenizer model\n",
    "            max_token_limit: Maximum tokens allowed\n",
    "            \n",
    "        Returns:\n",
    "            List of chunked LangChain Document objects\n",
    "        \"\"\"\n",
    "        \n",
    "        if not docs:\n",
    "            print(\"⚠️  No documents provided for chunking\")\n",
    "            return []\n",
    "        \n",
    "        # Load tokenizer\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "            print(f\"✅ Loaded tokenizer: {tokenizer_model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading tokenizer: {e}\") \n",
    "        \n",
    "        # Validate parameters\n",
    "        if chunk_size >= max_token_limit:\n",
    "            chunk_size = max_token_limit - 500  # Safe buffer\n",
    "            print(f\"⚠️  Adjusted chunk_size to {chunk_size} for safety\")\n",
    "        \n",
    "        if chunk_overlap >= chunk_size:\n",
    "            chunk_overlap = chunk_size // 5  # 20% overlap\n",
    "            print(f\"⚠️  Adjusted chunk_overlap to {chunk_overlap}\")\n",
    "        \n",
    "        # Create tokenizer-aware text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "            tokenizer=tokenizer,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            strip_whitespace=True,\n",
    "            separators=[\n",
    "                \"\\n\\n\",      # Paragraph breaks\n",
    "                \"\\n\",        # Line breaks\n",
    "                \". \",        # Sentence endings\n",
    "                \"! \",        # Exclamation endings  \n",
    "                \"? \",        # Question endings\n",
    "                \"; \",        # Semicolon breaks\n",
    "                \", \",        # Comma breaks\n",
    "                \" \",         # Word breaks\n",
    "                \"\"           # Character level\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Split documents\n",
    "        print(f\"🔄 Chunking {len(docs)} documents...\")\n",
    "        chunked_docs = text_splitter.split_documents(docs)\n",
    "        \n",
    "        # Validate token counts and add metadata\n",
    "        validated_chunks = []\n",
    "        max_tokens_found = 0\n",
    "        \n",
    "        for i, chunk in enumerate(chunked_docs):\n",
    "            # Count actual tokens\n",
    "            token_count = len(tokenizer.encode(chunk.page_content))\n",
    "            max_tokens_found = max(max_tokens_found, token_count)\n",
    "            \n",
    "            # Add chunk metadata\n",
    "            if chunk.metadata is None:\n",
    "                chunk.metadata = {}\n",
    "                \n",
    "            chunk.metadata.update({\n",
    "                'chunk_id': i,\n",
    "                'token_count': token_count,\n",
    "                'char_count': len(chunk.page_content),\n",
    "                'chunk_method': 'tokenizer_based'\n",
    "            })\n",
    "            \n",
    "            # Skip if too large\n",
    "            if token_count > max_token_limit:\n",
    "                print(f\"⚠️  Skipping oversized chunk {i}: {token_count} tokens\")\n",
    "                continue\n",
    "                \n",
    "            validated_chunks.append(chunk)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"✅ Created {len(validated_chunks)} chunks\")\n",
    "        print(f\"📊 Max tokens in any chunk: {max_tokens_found}\")\n",
    "        \n",
    "        return validated_chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_vector_store(\n",
    "        chunked_docs: List[Document],\n",
    "        embedding_model: str = \"D:/model/BAAI-bge-m3\",\n",
    "        index_name: str = \"pdf_documents\",\n",
    "        persist_directory: str = \"./faiss_db\"\n",
    "    ) -> FAISS:\n",
    "        \"\"\"\n",
    "        ສ້າງ Vector Store ດ້ວຍ FAISS ຈາກ chunked documents\n",
    "        \n",
    "        Embedding_model: ແມ່ນ Model ທີ່ໃຊ້ໃນການເຮັດ Embedding ເພື່ອປ່ຽນຂໍ້ຄວາມເປັນ Vector\n",
    "        Index_name: ແມ່ນຊື່ຂອງ FAISS index ສາມາດສ້າງຕາມໃຈ\n",
    "        Persist_directory: ແມ່ນໂຟລເດີຢຸ້ບັນທຶກ FAISS index\n",
    "        \n",
    "        Args:\n",
    "            chunked_docs: List of chunked Document objects\n",
    "            embedding_model: Path to embedding model\n",
    "            index_name: Name for FAISS index\n",
    "            persist_directory: Directory to save FAISS index\n",
    "            \n",
    "        Returns:\n",
    "            FAISS vector store object\n",
    "        \"\"\"\n",
    "        \n",
    "        if not chunked_docs:\n",
    "            print(\"⚠️  No chunked documents provided\")\n",
    "            return None\n",
    "        \n",
    "        # Create embeddings\n",
    "        try:\n",
    "            print(f\"🔄 Loading embedding model: {embedding_model}\")\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=embedding_model,\n",
    "                model_kwargs={'device': 'cpu'},  # ປ່ຽນເປັນ 'cuda' ຖ້າມີ GPU\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "            print(f\"✅ Loaded embedding model successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading embedding model: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Create FAISS vector store\n",
    "        try:\n",
    "            # ສ້າງໂຟລເດີຖ້າຍັງບໍ່ມີ\n",
    "            os.makedirs(persist_directory, exist_ok=True)\n",
    "            \n",
    "            print(f\"🔄 Creating FAISS vector store with {len(chunked_docs)} documents...\")\n",
    "            \n",
    "            # ສ້າງ FAISS vector store\n",
    "            vector_store = FAISS.from_documents(\n",
    "                documents=chunked_docs,\n",
    "                embedding=embeddings\n",
    "            )\n",
    "            \n",
    "            # ບັນທຶກ FAISS index\n",
    "            faiss_path = os.path.join(persist_directory, index_name)\n",
    "            vector_store.save_local(faiss_path)\n",
    "            print(f\"💾 FAISS index saved to: {faiss_path}\")\n",
    "            \n",
    "            # ສະແດງສະຖິຕິ\n",
    "            print(f\"✅ Created FAISS vector store\")\n",
    "            print(f\"📊 Total vectors: {len(chunked_docs)}\")\n",
    "            print(f\"📚 Index name: {index_name}\")\n",
    "            \n",
    "            return vector_store\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating FAISS vector store: {e}\")\n",
    "            return None\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_existing_vector_store(\n",
    "        embedding_model: str = \"D:/model/BAAI-bge-m3\",\n",
    "        index_name: str = \"pdf_documents\", \n",
    "        persist_directory: str = \"./faiss_db\"\n",
    "    ) -> Optional[FAISS]:\n",
    "        \"\"\"\n",
    "        ໂຫຼດ Vector Store ທີ່ມີຢູ່ແລ້ວຈາກ FAISS\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Path to embedding model\n",
    "            index_name: Name of FAISS index\n",
    "            persist_directory: Directory where FAISS index is saved\n",
    "            \n",
    "        Returns:\n",
    "            FAISS vector store object or None\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # ກວດສອບວ່າມີໂຟລເດີຫຼືບໍ່\n",
    "            faiss_path = os.path.join(persist_directory, index_name)\n",
    "            if not os.path.exists(faiss_path):\n",
    "                print(f\"❌ FAISS index not found: {faiss_path}\")\n",
    "                return None\n",
    "            \n",
    "            # ກວດສອບວ່າມີໄຟລ໌ທີ່ຈຳເປັນຫຼືບໍ່\n",
    "            index_file = os.path.join(faiss_path, \"index.faiss\")\n",
    "            pkl_file = os.path.join(faiss_path, \"index.pkl\")\n",
    "            \n",
    "            if not os.path.exists(index_file) or not os.path.exists(pkl_file):\n",
    "                print(f\"❌ FAISS files not found in: {faiss_path}\")\n",
    "                return None\n",
    "            \n",
    "            # ໂຫຼດ embedding model\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=embedding_model,\n",
    "                model_kwargs={'device': 'cpu'},\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "            \n",
    "            # ໂຫຼດ FAISS vector store\n",
    "            vector_store = FAISS.load_local(\n",
    "                faiss_path, \n",
    "                embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            \n",
    "            # ກວດສອບວ່າມີຂໍ້ມູນຫຼືບໍ່\n",
    "            if hasattr(vector_store, 'index') and vector_store.index.ntotal > 0:\n",
    "                print(f\"✅ Loaded existing FAISS vector store: {index_name}\")\n",
    "                print(f\"📊 Total vectors: {vector_store.index.ntotal}\")\n",
    "                return vector_store\n",
    "            else:\n",
    "                print(f\"⚠️  FAISS index '{index_name}' is empty\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading FAISS vector store: {e}\")\n",
    "            return None\n",
    "        \n",
    "    @staticmethod\n",
    "    def search_similar_documents(\n",
    "        vector_store: FAISS,\n",
    "        query: str,\n",
    "        k: int = 5\n",
    "    ) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        ຄົ້ນຫາເອກະສານທີ່ຄ້າຍຄືກັນ\n",
    "        vector_store: ແມ່ນຂໍ້ມູເຮົາເຄີຍສ້າງ Vector Store ໃນ ./faiss_db\n",
    "        query: ຄຳຖາມທີ່ຕ້ອງການຄົ້ນຫາ\n",
    "        k: ຈຳນວນຜົນລັບທີ່ຕ້ອງການ\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples (document, score)\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"🔍 Searching for: {query}\")\n",
    "            \n",
    "            # ຄົ້ນຫາດ້ວຍ score\n",
    "            results = vector_store.similarity_search_with_score(\n",
    "                query=query,\n",
    "                k=k\n",
    "            )\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during search: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqRAGSystem:\n",
    "    \"\"\"\n",
    "    ລະບົບ RAG ປະສົມກັບ Groq LLM ເພື່ອຕອບຄຳຖາມອ້າງອີງຈາກເອກະສານ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, groq_api_key: str, model_name: str = \"openai/gpt-oss-120b\"):\n",
    "        \"\"\"\n",
    "        ເລີ່ມຕົ້ນ GroqRAGSystem\n",
    "        \n",
    "        Args:\n",
    "            groq_api_key: Groq API key (ຕ້ອງໄປສະໝັກທີ່ https://console.groq.com)\n",
    "            model_name: ຊື່ Model ທີ່ຈະໃຊ້ (ຍົກຕົວຢ່າງ: openai/gpt-oss-120b)\n",
    "        \"\"\"\n",
    "        self.client = Groq(api_key=groq_api_key)\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def create_context_from_documents(self, search_results: List[tuple]) -> str:\n",
    "        \"\"\"\n",
    "        ສ້າງ context ຈາກຜົນການຄົ້ນຫາເອກະສານ\n",
    "        \n",
    "        Args:\n",
    "            search_results: List of tuples (document, score) ຈາກ vector search\n",
    "            \n",
    "        Returns:\n",
    "            ຂໍ້ຄວາມ context ສຳລັບ LLM\n",
    "        \"\"\"\n",
    "        if not search_results:\n",
    "            return \"ບໍ່ພົບເອກະສານທີ່ກ່ຽວຂ້ອງ\"\n",
    "            \n",
    "        context_parts = []\n",
    "        for i, (doc, score) in enumerate(search_results):\n",
    "            # FAISS ໃຊ້ cosine distance, ຄ່າຕ່ຳໝາຍຄວາມວ່າຄ້າຍຄືກັນຫຼາຍ\n",
    "            similarity = 1 - score  # ປ່ຽນເປັນ similarity\n",
    "            source_info = f\"ແຫຼ່ງ: {doc.metadata.get('source_file', 'Unknown')} (ໜ້າ {doc.metadata.get('page', 'Unknown')})\"\n",
    "            content = doc.page_content.strip()\n",
    "            \n",
    "            context_parts.append(f\"ເອກະສານ {i+1} (ຄວາມຄ້າຍຄື: {similarity:.3f}):\\n{source_info}\\n{content}\\n\")\n",
    "            \n",
    "        return \"\\n---\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_answer(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        ສ້າງຄຳຕອບໂດຍໃຊ້ Groq LLM ພ້ອມ context ຈາກເອກະສານ\n",
    "        \n",
    "        Args:\n",
    "            query: ຄຳຖາມຂອງຜູ້ໃຊ້\n",
    "            context: Context ຈາກເອກະສານ\n",
    "            \n",
    "        Returns:\n",
    "            ຄຳຕອບຈາກ LLM\n",
    "        \"\"\"\n",
    "        \n",
    "        # ສ້າງ prompt ສຳລັບ RAG\n",
    "        prompt = f\"\"\"ທ່ານເປັນ AI Assistant ທີ່ຊ່ຽວຊານໃນການຕອບຄຳຖາມໂດຍອ້າງອີງຈາກເອກະສານທີ່ໃຫ້ມາ.\n",
    "\n",
    "ຄຳແນະນຳ:\n",
    "1. ຕອບຄຳຖາມໂດຍອ້າງອີງຈາກເອກະສານທີ່ໃຫ້ມາເທົ່ານັ້ນ\n",
    "2. ຖ້າບໍ່ພົບຄຳຕອບໃນເອກະສານ, ໃຫ້ບອກວ່າບໍ່ພົບຂໍ້ມູນທີ່ກ່ຽວຂ້ອງ\n",
    "3. ລະບຸແຫຼ່ງຂໍ້ມູນທີ່ໃຊ້ໃນການຕອບ\n",
    "4. ຕອບເປັນພາສາລາວ ແລະ ໃຫ້ຄຳຕອບທີ່ຊັດເຈນ, ລະອຽດ\n",
    "5. ຕອບໃຫ້ເປັນ Format markdown\n",
    "\n",
    "ເອກະສານອ້າງອີງ:\n",
    "{context}\n",
    "\n",
    "ຄຳຖາມ: {query}\n",
    "\n",
    "ຄຳຕອບ:\"\"\"\n",
    "\n",
    "        try:\n",
    "            # ສົ່ງ request ໄປ Groq\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                model=self.model_name,\n",
    "                temperature=0.1,  # ຄວາມສ້າງສັນຕ່ຳ ເພື່ອຄວາມແມ່ນຍຳ  ຂຶ້ນນຳ Model ເພາະຄ່າ temperature ແຕ່ລະເຈົ້າມັນຕ່າງກັນ\n",
    "                max_tokens=2000,  # ຈຳນວນ tokens ສູງສຸດ \n",
    "            )\n",
    "            \n",
    "            answer = chat_completion.choices[0].message.content\n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"❌ ເກີດຂໍ້ຜິດພາດໃນການສ້າງຄຳຕອບ: {str(e)}\"\n",
    "    \n",
    "    def query_documents(self, vector_store: FAISS, query: str, k: int = 5) -> dict:\n",
    "        \"\"\"\n",
    "        ຄຳຖາມແບບສົມບູນຈາກການຄົ້ນຫາເອກະສານຈົນເຖີງການສ້າງຄຳຕອບ\n",
    "        \n",
    "        Args:\n",
    "            vector_store: FAISS vector store\n",
    "            query: ຄຳຖາມຂອງຜູ້ໃຊ້\n",
    "            k: ຈຳນວນເອກະສານທີ່ຈະຄົ້ນຫາ\n",
    "            \n",
    "        Returns:\n",
    "            dict ທີ່ປະກອບດ້ວຍ answer, context, ແລະ sources\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n🤖 Processing query: {query}\")\n",
    "        \n",
    "        # 1. ຄົ້ນຫາເອກະສານທີ່ກ່ຽວຂ້ອງ\n",
    "        search_results = DocumentLoader.search_similar_documents(\n",
    "            vector_store=vector_store,\n",
    "            query=query,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        if not search_results:\n",
    "            return {\n",
    "                \"answer\": \"❌ ບໍ່ພົບເອກະສານທີ່ກ່ຽວຂ້ອງກັບຄຳຖາມຂອງທ່ານ\",\n",
    "                \"context\": \"\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # 2. ສ້າງ context ຈາກຜົນການຄົ້ນຫາ\n",
    "        context = self.create_context_from_documents(search_results)\n",
    "        \n",
    "        # 3. ສ້າງຄຳຕອບດ້ວຍ LLM\n",
    "        print(\"🧠 Generating answer with Groq LLM...\")\n",
    "        answer = self.generate_answer(query, context)\n",
    "        \n",
    "        # 4. ສ້າງລາຍຊື່ແຫຼ່ງຂໍ້ມູນ\n",
    "        sources = []\n",
    "        for doc, score in search_results:\n",
    "            similarity = 1 - score\n",
    "            sources.append({\n",
    "                \"source_file\": doc.metadata.get('source_file', 'Unknown'),\n",
    "                \"page\": doc.metadata.get('page', 'Unknown'),\n",
    "                \"similarity\": f\"{similarity:.3f}\",\n",
    "                \"content_preview\": doc.page_content\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"context\": context,\n",
    "            \"sources\": sources\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    ຟັງຊັ່ນຫຼັກສຳລັບການທົດສອບລະບົບ RAG ກັບ Groq ແລະ FAISS\n",
    "    \"\"\"\n",
    "    \n",
    "    # ການຕັ້ງຄ່າ\n",
    "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")  # ແທນຄ່າດ້ວຍ API key ຈິງ\n",
    "    \n",
    "    # ລາຍຊື່ໄຟລ໌ PDF (ຖ້າຕ້ອງການສ້າງ vector store ໃໝ່)\n",
    "    pdf_files = [ \n",
    "        \"C:/Users/Dell/Desktop/Finetuing vs RAG.pdf\"\n",
    "    ]\n",
    "    \n",
    "    # ກວດສອບວ່າມີ vector store ຢູ່ແລ້ວຫຼືບໍ່\n",
    "    \n",
    "    display(Markdown(\"## 🔍 ກວດສອບ FAISS Vector Store\")) \n",
    "    loaded_vectorstore = DocumentLoader.load_existing_vector_store(\n",
    "        embedding_model=\"D:/model/BAAI-bge-m3\", \n",
    "        index_name=\"pdf_documents\", \n",
    "        persist_directory=\"./faiss_db\"\n",
    "    )\n",
    "    \n",
    "    # ຖ້າບໍ່ມີ vector store, ສ້າງໃໝ່\n",
    "    if loaded_vectorstore is None:\n",
    "        display(Markdown(\"## 📚 Creating new FAISS vector store...\"))  \n",
    "        \n",
    "        # 1. ໂຫຼດເອກະສານ\n",
    "        documents = DocumentLoader.load_docs(pdf_files) \n",
    "        \n",
    "        if not documents:\n",
    "            print(\"❌ No documents found. Please check your PDF file paths.\")\n",
    "            return\n",
    "            \n",
    "        # 2. ເຮັດ chunking ຂໍ້ມູນ \n",
    "        display(Markdown(\"## ✂️ Chunking documents...\")) \n",
    "        # ເຮັດ chunking ຂໍ້ມູນ\n",
    "        # ໃຊ້ Model ຂອງ BAAI-bge-m3 ເພື່ອຮັບຄ່າການເຮັດ chunking ຂໍ້ມູນ ເຊີ່ງຜູ້ໃຊ້ແມ່ນສາມາດເລືອກໄດ້ຕາມໃຈເລີຍວ່າຈະ ໃຊ້ Model ຍັງໃນການເຮັດ Embedding ສາມາດໂຫລດຜ່ານ Hugginface ໄດ້ ໂດຍກຳນົດ path ເອງ ສາມາດ ເຂົ້າໄປໃນ Folder Download Model/download-model.ipynb ເພື່ອດາວໂຫລດ Model ຍັງ\n",
    "        # ກຳນັດຄ່າຕ່າງໆຂອງ chunking ໂດຍ Base on ຈາກເອກະສານ ຖ້າ ມີເອກະສານຫລາຍຫນ້າ ແນະນຳໃຫ້ລອງເພິ່ມຄ່າ chunk_size ແລະ chunk_overlap ເພື່ອຮັບຄ່າທີ່ດີກວ່າ\n",
    "        chunk_documents = DocumentLoader.chunk_documents_standard(\n",
    "            documents, \n",
    "            chunk_size=500, \n",
    "            chunk_overlap=50, \n",
    "            tokenizer_model=\"D:/model/BAAI-bge-m3\", \n",
    "            max_token_limit=1000\n",
    "        )\n",
    "        \n",
    "        if not chunk_documents:\n",
    "            print(\"❌ Failed to chunk documents.\")\n",
    "            return\n",
    "            \n",
    "        # 3. ສ້າງ vector store ດ້ວຍ FAISS\n",
    "        display(Markdown(\"## 🔄 Creating FAISS vector store...\")) \n",
    "        # ເຮັດ Embedding ຂໍ້ມູນ\n",
    "        # ກໍລະນີນີ້ຈະຖ້າດົນແນ່ ເນື່ອງຈາກວ່າ ຈະມີການເອົາ ເອກະສານທີ່ເຮົາ Chunking ມາແປງເປັນ Vector ເພື່ອບັນທືກໃນ FAISS ຖ້າຢາກໃຫ້ໄວ້ ໃຜມີ GPU ແນະນຳໃຫ້ໃຊ້ cuda ແທນ cpu\n",
    "        loaded_vectorstore = DocumentLoader.create_vector_store(chunk_documents)\n",
    "        \n",
    "        if loaded_vectorstore is None:\n",
    "            print(\"❌ Failed to create FAISS vector store.\")\n",
    "            return\n",
    "    \n",
    "    # ເລີ່ມຕົ້ນລະບົບ RAG ກັບ Groq \n",
    "    display(Markdown(\"## 🚀 Initializing Groq RAG System...\")) \n",
    "    \n",
    "    if GROQ_API_KEY == \"ໃສ່ Groq API Key ຂອງເຈົ້າທີ່ນີ້\":\n",
    "        print(\"❌ ກະລຸນາໃສ່ Groq API Key ຂອງເຈົ້າໃນຕົວແປ GROQ_API_KEY\")\n",
    "        print(\"💡 ສາມາດໄດ້ API key ຟຣີທີ່: https://console.groq.com\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        rag_system = GroqRAGSystem(\n",
    "            groq_api_key=GROQ_API_KEY,\n",
    "            model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\" \n",
    "        )\n",
    "        display(Markdown(\"## ✅ Groq RAG System initialized successfully !\"))  \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing Groq system: {e}\")\n",
    "        return\n",
    "    \n",
    "    # ທົດສອບລະບົບດ້ວຍຄຳຖາມຕົວຢ່າງ\n",
    "    test_queries = [\n",
    "        \"RAG ກັບ Fine-tuning ມີຄວາມແຕກຕ່າງກັນແນວໃດ ?\"\n",
    "    ]\n",
    "    \n",
    "    display(Markdown(\"## 🧪 ການທົດສອບລະບົບ RAG\"))\n",
    "    display(Markdown(\"ທົດສອບດ້ວຍຄຳຖາມຕົວຢ່າງ\"))\n",
    "    display(Markdown(\"---\"))\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n📝 ຄຳຖາມທີ່ {i}: {query}\")\n",
    "        print(\"-\" * 40)\n",
    "        display(Markdown(f\"### 📝 ຄຳຖາມທີ່ {i}: {query}\"))\n",
    "        display(Markdown(\"---\"))\n",
    "        \n",
    "        # ສົ່ງຄຳຖາມໄປລະບົບ RAG\n",
    "        result = rag_system.query_documents(\n",
    "            vector_store=loaded_vectorstore,\n",
    "            query=query,\n",
    "            k=5  # ຄົ້ນຫາ 5 ເອກະສານທີ່ກ່ຽວຂ້ອງ\n",
    "        )\n",
    "        \n",
    "        # ສະແດງຜົນລັບ\n",
    "        display(Markdown(\"#### 🤖 ຄຳຕອບ:\"))\n",
    "        display(Markdown(f\"\"\"\n",
    "            ```\n",
    "            {result['answer']}\n",
    "            ```\n",
    "        \"\"\"))\n",
    "        \n",
    "        if result['sources']:\n",
    "            display(Markdown(\"#### 📚 ແຫຼ່ງຂໍ້ມູນອ້າງອີງ:\"))\n",
    "                \n",
    "            sources_md = \"\"\n",
    "            for j, source in enumerate(result['sources'], 1):\n",
    "                    sources_md += f\"\"\"\n",
    "                **{j}.** `{source['source_file']}` (ໜ້າ {source['page']}) - ຄວາມຄ້າຍຄື: `{source['similarity']}`\n",
    "                > {source['content_preview']}...\n",
    "\n",
    "            \"\"\"\n",
    "            display(Markdown(sources_md))\n",
    "                    \n",
    "        display(Markdown(\"---\"))\n",
    "    \n",
    "    # ໂໝດ interactive ສຳລັບຜູ້ໃຊ້ສາມາດຖາມຄຳຖາມເອງ\n",
    "    display(Markdown(\"## 💬 ໂໝດ Interactive - ພິມຄຳຖາມຂອງທ່ານ (ພິມ 'quit' ເພື່ອອອກ\")) \n",
    "    display(Markdown(\"---\"))\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_query = input(\"\\n❓ ຄຳຖາມຂອງທ່ານ: \").strip()\n",
    "            \n",
    "            if user_query.lower() in ['quit', 'exit', 'ອອກ']:\n",
    "                print(\"👋 ຂອບໃຈທີ່ໃຊ້ລະບົບ RAG!\")\n",
    "                break\n",
    "                \n",
    "            if not user_query:\n",
    "                print(\"⚠️ ກະລຸນາໃສ່ຄຳຖາມ\")\n",
    "                continue\n",
    "            \n",
    "            display(Markdown(f\"### ❓ ຄຳຖາມ: `{user_query}`\"))\n",
    "            \n",
    "            # ສົ່ງຄຳຖາມໄປລະບົບ RAG\n",
    "            result = rag_system.query_documents(\n",
    "                vector_store=loaded_vectorstore,\n",
    "                query=user_query,\n",
    "                k=5\n",
    "            )\n",
    "            \n",
    "            # ສະແດງຜົນລັບ\n",
    "            display(Markdown(\"#### 🤖 ຄຳຕອບ:\"))\n",
    "            display(Markdown(f\"\"\"\n",
    "                ```\n",
    "                {result['answer']}\n",
    "                ```\n",
    "            \"\"\"))\n",
    "            \n",
    "            # ສະແດງແຫຼ່ງຂໍ້ມູນ (ແບບຫຍໍ້)\n",
    "            if result['sources']: \n",
    "                display(Markdown(\"#### 📚 ແຫຼ່ງຂໍ້ມູນອ້າງອີງ:\"))\n",
    "                for source in result['sources'][:3]:  # ສະແດງ 3 ແຫຼ່ງທຳອິດ\n",
    "                    display(Markdown(f\"#### • {source['source_file']} (ໜ້າ {source['page']})\")) \n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n👋 ຂອບໃຈທີ່ໃຊ້ລະບົບ RAG!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ເກີດຂໍ້ຜິດພາດ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 🔍 ກວດສອບ FAISS Vector Store"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded existing FAISS vector store: pdf_documents\n",
      "📊 Total vectors: 2\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## 🚀 Initializing Groq RAG System..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## ✅ Groq RAG System initialized successfully !"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## 🧪 ການທົດສອບລະບົບ RAG"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ທົດສອບດ້ວຍຄຳຖາມຕົວຢ່າງ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 ຄຳຖາມທີ່ 1: RAG ກັບ Fine-tuning ມີຄວາມແຕກຕ່າງກັນແນວໃດ ?\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### 📝 ຄຳຖາມທີ່ 1: RAG ກັບ Fine-tuning ມີຄວາມແຕກຕ່າງກັນແນວໃດ ?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Processing query: RAG ກັບ Fine-tuning ມີຄວາມແຕກຕ່າງກັນແນວໃດ ?\n",
      "🔍 Searching for: RAG ກັບ Fine-tuning ມີຄວາມແຕກຕ່າງກັນແນວໃດ ?\n",
      "🧠 Generating answer with Groq LLM...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### 🤖 ຄຳຕອບ:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            ```\n",
       "            ## ຄວາມແຕກຕ່າງລະຫວ່າງ RAG ແລະ Fine-tuning\n",
       "\n",
       "RAG (Retrieval-Augmented Generation) ແລະ Fine-tuning ແມ່ນສອງວິທີທີ່ໃຊ້ໃນການພັດທະນາແບບຈໍາລອງພາສາໃຫຍ່ (LLM) ສໍາລັບການນໍາໃຊ້ສະເພາະດ້ານ. ຄວາມແຕກຕ່າງຫຼັກລະຫວ່າງພວກມັນຢູ່ໃນວິທີການທີ່ພວກເຂົາຈັດການກັບຂໍ້ມູນແລະການປັບປຸງແບບຈໍາລອງ.\n",
       "\n",
       "### Fine-tuning\n",
       "\n",
       "*   **ການປະຕິບັດ**: ການເກັບກຳຂໍ້ມູນຈໍານວນຫຼວງຫຼາຍ, ການເລືອກແບບຈໍາລອງທີ່ເຫມາະສົມ, ການຝຶກອົບຮົມແບບຈໍາລອງ, ແລະການນໍາໄປໃຊ້.\n",
       "*   **ຂໍ້ດີ**: ການສ້າງຄໍາຕອບແບບ end-to-end.\n",
       "*   **ຂໍ້ເສຍ**: ຄ່າໃຊ້ຈ່າຍໃນການຝຶກອົບຮົມສູງ, ຄວາມຍາກລໍາໃນການປັບປຸງຄວາມຮູ້.\n",
       "\n",
       "### RAG\n",
       "\n",
       "*   **ການປະຕິບັດ**: ການເກັບກຳເອກະສານທີ່ກ່ຽວຂ້ອງ, ການສ້າງຖານຄວາມຮູ້, ການນໍາໃຊ້ແບບຈໍາລອງ, ແລະການພັດທະນາແອັບພລິເຄຊັນ RAG.\n",
       "*   **ຂໍ້ດີ**: ການປັບປຸງຄວາມຮູ້ແບບຍືດຫຍຸ່ນ, ການລວມຂໍ້ມູນຫຼ້າສຸດ, ຄວາມສາມາດໃນການຂະຫຍາຍ.\n",
       "*   **ຂໍ້ເສຍ**: ຄວາມຊັກຊ້າໃນການສ້າງຄໍາຕອບ.\n",
       "\n",
       "### ຄວາມແຕກຕ່າງຫຼັກ\n",
       "\n",
       "*   Fine-tuning ແມ່ນເຫມາະສົມກັບຂົງເຂດທີ່ຕ້ອງການຄວາມຖືກຕ້ອງແລະຄວາມສອດຄ່ອງຂອງຄວາມຮູ້ສູງ ແລະການປັບປຸງຄວາມຮູ້ແມ່ນຂ້ອນຂ້າງຊ້າ.\n",
       "*   RAG ແມ່ນເຫມາະສົມກັບຂົງເຂດທີ່ມີການປັບປຸງຄວາມຮູ້ຢ່າງໄວວາແລະຕ້ອງການການຕອບສະຫນອງໄວ.\n",
       "\n",
       "ອ້າງອີງຈາກເອກະສານ: Finetuing vs RAG.pdf (ໜ້າ0 ແລະ ໜ້າ1) [1](https://example.com/finetuing-vs-rag.pdf)\n",
       "            ```\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### 📚 ແຫຼ່ງຂໍ້ມູນອ້າງອີງ:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "                **1.** `Finetuing vs RAG.pdf` (ໜ້າ 0) - ຄວາມຄ້າຍຄື: `0.197`\n",
       "                > 1 Huawei Confidential\n",
       "How to Develop a Professional Domain Knowledge Q&A Assistant？\n",
       "Implementation\n",
       "Data Preparation: Collect a large amount of text data. These \n",
       "data need to be cleaned and annotated to ensure quality.\n",
       "Model Selection: Choose a suitable pre-trained model.\n",
       "Model Training: Input the prepared data into the model and \n",
       "adjust its parameters.\n",
       "Model Deployment: Deploy the trained model for inference.\n",
       "Advantages\n",
       "End-to-End Generation: The model can directly generate \n",
       "answers based on input questions. \n",
       "Disadvantages\n",
       "High Training Costs: Time-consuming and costly by \n",
       "requiring a large amount of data and computational resources.\n",
       "Difficulty in Knowledge Update: Once the model is trained, \n",
       "updating the knowledge requires heavy re -fine-tuning.\n",
       "Fine-tuning RAG\n",
       "Implementation\n",
       "Data Preparation: Gather a wide range of documents relevant \n",
       "to the professional domain and clean them.\n",
       "Knowledge Base Construction: Build a structured knowledge \n",
       "base that can be efficiently queried by the retrieval module.\n",
       "Model Deployment: Deploy a LLM as expected (deepseek).\n",
       "RAG Application Development: Develop a RAG application \n",
       "that integrates retrieval and generation to answer user queries.\n",
       "Advantages\n",
       "Flexible Knowledge Update: The knowledge base can be \n",
       "updated at any time without retraining the model.\n",
       "Integration of Latest Information: The system can generate \n",
       "more accurate and up -to-date answers.\n",
       "Scalability: It is easy to expand the content and scope of the \n",
       "knowledge base.\n",
       "Disadvantages\n",
       "Generation Delay: The added retrieval step may slow down \n",
       "the system's response time.\n",
       "The Fine-tuning approach is suitable for fields where knowledge accuracy and consistency are highly valued and knowledge updates\n",
       "are relatively slow, such as some traditional academic research areas. \n",
       "In contrast, the RAG approach is more suitable for fields where knowledge updates rapidly and quick responses are required, such as \n",
       "finance, healthcare, and technology....\n",
       "\n",
       "            \n",
       "                **2.** `Finetuing vs RAG.pdf` (ໜ້າ 1) - ຄວາມຄ້າຍຄື: `-0.168`\n",
       "                > 2 Huawei Confidential\n",
       "ModelEngine: AI Toolchain that Accelerates the Implementation of AI Applications\n",
       "40% NPU pooling usage\n",
       "60%+ faster multimodal data cleansing\n",
       "1.5x online concurrency\n",
       "1.6x offline throughput\n",
       "Lower costs Fast application rollout Good inference performance\n",
       "95% accuracy\n",
       "50% shorter development latency\n",
       "Operator \n",
       "ecosystem\n",
       "ModelEngine\n",
       "Full-process AI \n",
       "toolchain\n",
       "Application enablement\n",
       "High-precision RAG application \n",
       "development and optimization\n",
       "Open toolchain Open-source framework, supporting third-party operators\n",
       "Model enablement\n",
       "Lightweight model inference \n",
       "toolchain\n",
       "Data enablement\n",
       "Automatic data processing and \n",
       "knowledge generation\n",
       "API API\n",
       "Training and inference offload acceleration\n",
       "Cache offload acceleration | Ascend+Kunpeng\n",
       "heterogeneous computing\n",
       "Retrieval acceleration\n",
       "Converged retrieval of multimodal data | Vector \n",
       "retrieval acceleration\n",
       "Low-code toolchain\n",
       "Self-orchestrated data processing | Modular \n",
       "RAG\n",
       "NPU basic software Container platform Knowledge base storage\n",
       "Model ecosystem OpenMind HuggingFace Blue Whale Market LangChain LlamaIndex\n",
       "GPU processor | Ascend NPU processor\n",
       "Resource enablement\n",
       "AI task scheduling and XPU pooling...\n",
       "\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## 💬 ໂໝດ Interactive - ພິມຄຳຖາມຂອງທ່ານ (ພິມ 'quit' ເພື່ອອອກ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "⚠️ ກະລຸນາໃສ່ຄຳຖາມ\n",
      "\n",
      "\n",
      "👋 ຂອບໃຈທີ່ໃຊ້ລະບົບ RAG!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
