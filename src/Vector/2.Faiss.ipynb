{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import List, Optional\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from groq import Groq\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    @staticmethod\n",
    "    def load_docs(file_paths: List[str]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        เปเบซเบฅเบ PDF documents เปเบเบเปเบเป LangChain PyPDFLoader เปเบเบฒเบฐเบเบฑเบเปเบญเบฑเบเบเบฑเบเบกเบฑเบเปเบเบฒเบฐเบกเบฑเบเบชเปเบฒเบ Metadata เปเบซเป Auto \n",
    "\n",
    "        Metadata เบเบทเบเบฑเบ ? \n",
    "        Metadata เบเบทเบเปเปเบกเบนเบเปเบเบตเปเบกเปเบเบตเบกเบเปเบฝเบงเบเบฑเบเปเบญเบเบฐเบชเบฒเบ\n",
    "        เปเบเปเบฅเบฐ Document เบเบฐเบกเบต 2 เบชเปเบงเบเบซเบผเบฑเบ:\n",
    "        1. page_content: เปเบเบทเปเบญเปเบเบเปเปเบเบงเบฒเบกเบเบดเบเป\n",
    "        2. metadata: เบเปเปเบกเบนเบเบฅเบฒเบเบฅเบฐเบญเบฝเบเบเปเบฝเบงเบเบฑเบเปเบญเบเบฐเบชเบฒเบ เปเบเบทเปเบญเบเบปเบเบเบญเบเบงเปเบฒ เปเบงเบฅเบฒเปเบฎเบปเบฒ เบเบปเปเบเบซเบฒเบเปเปเบกเบนเบ เปเบซเบผเปเบเบเปเปเบกเบนเบเบเบฑเปเบเบกเบฒเบเบฒเบเปเบช\n",
    "        \n",
    "        Args:\n",
    "            file_paths (list): List of PDF file paths\n",
    "        \n",
    "        Returns:\n",
    "            List[Document]: List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        \n",
    "        all_docs = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Warning: File {file_path} not found. Skipping...\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                file_extension = os.path.splitext(file_path)[1].lower()\n",
    "                \n",
    "                # Check if file is PDF\n",
    "                if file_extension != '.pdf':\n",
    "                    print(f\"Warning: {file_path} is not a PDF file. Skipping...\")\n",
    "                    continue\n",
    "                \n",
    "                # Load PDF using LangChain PyPDFLoader\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                documents = loader.load()\n",
    "                \n",
    "                # Add enhanced metadata to all documents\n",
    "                for doc in documents:\n",
    "                    if doc.metadata is None:\n",
    "                        doc.metadata = {}\n",
    "                        \n",
    "                    doc.metadata.update({\n",
    "                        'source_file': os.path.basename(file_path),\n",
    "                        'file_type': file_extension,\n",
    "                        'file_path': file_path,\n",
    "                        'file_size': os.path.getsize(file_path) if os.path.exists(file_path) else 0,\n",
    "                    })\n",
    "                \n",
    "                all_docs.extend(documents)\n",
    "                print(f\"โ Processed PDF: {file_path} ({len(documents)} pages)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"โ Error processing {file_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"๐ Total PDF documents loaded: {len(all_docs)}\")\n",
    "        return all_docs\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_documents_standard(\n",
    "        docs: List[Document], \n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        tokenizer_model: str = \"D:/model/BAAI-bge-m3\",\n",
    "        max_token_limit: int = 8192\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        เปเบเป Lanchain เปเบเบเบฒเบเปเบฎเบฑเบ chunking เบเปเปเบกเบนเบ เปเบเบทเปเบญเบเบฒเบเบเบปเปเบเบซเบฒเบเปเปเบกเบนเบเบเปเบงเบ FAISS  \n",
    "\n",
    "        Chunk_size: เปเบกเปเบเบเบณเบเบงเบเบเปเปเบกเบนเบเบเบตเปเบเบฐเปเบฎเบฑเบ chunking เบเปเปเบซเบเปเบงเบ เปเบเบฒเบฐเบเบฑเบ เปเบฎเบปเบฒเบเปเปเบชเบฒเบกเบฒเบเปเบญเบปเบฒเปเบญเบเบฐเบชเบฒเบเบเบฑเปเบเปเบปเบเปเบซเป AI เบเบญเบเปเบเป เปเบเบทเปเบญเบเบเบฒเบเบเบฒเบเปเบญเบเบฐเบชเบฒเบเบกเบตเบซเบฅเบฒเบเบซเบเปเบฒ\n",
    "        Chunk_overlap: เปเบกเปเบเบเบณเบเบงเบเบเปเปเบกเบนเบเบเบตเปเบเบฐเปเบฎเบฑเบ chunking เบเปเปเบซเบเปเบงเบ เปเบเบฒเบฐเบเบฑเบ เปเบฎเบปเบฒเบเปเปเบชเบฒเบกเบฒเบเปเบญเบปเบฒเปเบญเบเบฐเบชเบฒเบเบเบฑเปเบเปเบปเบเปเบซเป AI เบเบญเบเปเบเป เปเบเบทเปเบญเบเบเบฒเบเบเบฒเบเปเบญเบเบฐเบชเบฒเบเบกเบตเบซเบฅเบฒเบเบซเบเปเบฒ\n",
    "        Tokenizer_model: เปเบกเปเบ Model เบเบตเปเปเบฎเบปเบฒเบเบฐเปเบเปเปเบเบเบฒเบเปเบฎเบฑเบ chunking เบเปเปเบกเบนเบ เปเบเบทเปเบญเบเบฒเบเบเบปเปเบเบซเบฒเบเปเปเบกเบนเบเบเปเบงเบ FAISS\n",
    "        Max_token_limit: เปเบกเปเบเบเบฒเบเปเบเปเบเบชเบฑเบเบชเปเบงเบเปเบซเปเปเบซเบกเบฒเบฐเบชเบปเบกเบเบฑเบ chunk_size\n",
    "        \n",
    "        Args:\n",
    "            docs: List of LangChain Document objects\n",
    "            chunk_size: Target size for each chunk in tokens\n",
    "            chunk_overlap: Number of overlapping tokens between chunks\n",
    "            tokenizer_model: Path to tokenizer model\n",
    "            max_token_limit: Maximum tokens allowed\n",
    "            \n",
    "        Returns:\n",
    "            List of chunked LangChain Document objects\n",
    "        \"\"\"\n",
    "        \n",
    "        if not docs:\n",
    "            print(\"โ๏ธ  No documents provided for chunking\")\n",
    "            return []\n",
    "        \n",
    "        # Load tokenizer\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "            print(f\"โ Loaded tokenizer: {tokenizer_model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"โ Error loading tokenizer: {e}\") \n",
    "        \n",
    "        # Validate parameters\n",
    "        if chunk_size >= max_token_limit:\n",
    "            chunk_size = max_token_limit - 500  # Safe buffer\n",
    "            print(f\"โ๏ธ  Adjusted chunk_size to {chunk_size} for safety\")\n",
    "        \n",
    "        if chunk_overlap >= chunk_size:\n",
    "            chunk_overlap = chunk_size // 5  # 20% overlap\n",
    "            print(f\"โ๏ธ  Adjusted chunk_overlap to {chunk_overlap}\")\n",
    "        \n",
    "        # Create tokenizer-aware text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "            tokenizer=tokenizer,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            strip_whitespace=True,\n",
    "            separators=[\n",
    "                \"\\n\\n\",      # Paragraph breaks\n",
    "                \"\\n\",        # Line breaks\n",
    "                \". \",        # Sentence endings\n",
    "                \"! \",        # Exclamation endings  \n",
    "                \"? \",        # Question endings\n",
    "                \"; \",        # Semicolon breaks\n",
    "                \", \",        # Comma breaks\n",
    "                \" \",         # Word breaks\n",
    "                \"\"           # Character level\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Split documents\n",
    "        print(f\"๐ Chunking {len(docs)} documents...\")\n",
    "        chunked_docs = text_splitter.split_documents(docs)\n",
    "        \n",
    "        # Validate token counts and add metadata\n",
    "        validated_chunks = []\n",
    "        max_tokens_found = 0\n",
    "        \n",
    "        for i, chunk in enumerate(chunked_docs):\n",
    "            # Count actual tokens\n",
    "            token_count = len(tokenizer.encode(chunk.page_content))\n",
    "            max_tokens_found = max(max_tokens_found, token_count)\n",
    "            \n",
    "            # Add chunk metadata\n",
    "            if chunk.metadata is None:\n",
    "                chunk.metadata = {}\n",
    "                \n",
    "            chunk.metadata.update({\n",
    "                'chunk_id': i,\n",
    "                'token_count': token_count,\n",
    "                'char_count': len(chunk.page_content),\n",
    "                'chunk_method': 'tokenizer_based'\n",
    "            })\n",
    "            \n",
    "            # Skip if too large\n",
    "            if token_count > max_token_limit:\n",
    "                print(f\"โ๏ธ  Skipping oversized chunk {i}: {token_count} tokens\")\n",
    "                continue\n",
    "                \n",
    "            validated_chunks.append(chunk)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"โ Created {len(validated_chunks)} chunks\")\n",
    "        print(f\"๐ Max tokens in any chunk: {max_tokens_found}\")\n",
    "        \n",
    "        return validated_chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_vector_store(\n",
    "        chunked_docs: List[Document],\n",
    "        embedding_model: str = \"D:/model/BAAI-bge-m3\",\n",
    "        index_name: str = \"pdf_documents\",\n",
    "        persist_directory: str = \"./faiss_db\"\n",
    "    ) -> FAISS:\n",
    "        \"\"\"\n",
    "        เบชเปเบฒเบ Vector Store เบเปเบงเบ FAISS เบเบฒเบ chunked documents\n",
    "        \n",
    "        Embedding_model: เปเบกเปเบ Model เบเบตเปเปเบเปเปเบเบเบฒเบเปเบฎเบฑเบ Embedding เปเบเบทเปเบญเบเปเบฝเบเบเปเปเบเบงเบฒเบกเปเบเบฑเบ Vector\n",
    "        Index_name: เปเบกเปเบเบเบทเปเบเบญเบ FAISS index เบชเบฒเบกเบฒเบเบชเปเบฒเบเบเบฒเบกเปเบ\n",
    "        Persist_directory: เปเบกเปเบเปเบเบฅเปเบเบตเบขเบธเปเบเบฑเบเบเบถเบ FAISS index\n",
    "        \n",
    "        Args:\n",
    "            chunked_docs: List of chunked Document objects\n",
    "            embedding_model: Path to embedding model\n",
    "            index_name: Name for FAISS index\n",
    "            persist_directory: Directory to save FAISS index\n",
    "            \n",
    "        Returns:\n",
    "            FAISS vector store object\n",
    "        \"\"\"\n",
    "        \n",
    "        if not chunked_docs:\n",
    "            print(\"โ๏ธ  No chunked documents provided\")\n",
    "            return None\n",
    "        \n",
    "        # Create embeddings\n",
    "        try:\n",
    "            print(f\"๐ Loading embedding model: {embedding_model}\")\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=embedding_model,\n",
    "                model_kwargs={'device': 'cpu'},  # เบเปเบฝเบเปเบเบฑเบ 'cuda' เบเปเบฒเบกเบต GPU\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "            print(f\"โ Loaded embedding model successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"โ Error loading embedding model: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Create FAISS vector store\n",
    "        try:\n",
    "            # เบชเปเบฒเบเปเบเบฅเปเบเบตเบเปเบฒเบเบฑเบเบเปเปเบกเบต\n",
    "            os.makedirs(persist_directory, exist_ok=True)\n",
    "            \n",
    "            print(f\"๐ Creating FAISS vector store with {len(chunked_docs)} documents...\")\n",
    "            \n",
    "            # เบชเปเบฒเบ FAISS vector store\n",
    "            vector_store = FAISS.from_documents(\n",
    "                documents=chunked_docs,\n",
    "                embedding=embeddings\n",
    "            )\n",
    "            \n",
    "            # เบเบฑเบเบเบถเบ FAISS index\n",
    "            faiss_path = os.path.join(persist_directory, index_name)\n",
    "            vector_store.save_local(faiss_path)\n",
    "            print(f\"๐พ FAISS index saved to: {faiss_path}\")\n",
    "            \n",
    "            # เบชเบฐเปเบเบเบชเบฐเบเบดเบเบด\n",
    "            print(f\"โ Created FAISS vector store\")\n",
    "            print(f\"๐ Total vectors: {len(chunked_docs)}\")\n",
    "            print(f\"๐ Index name: {index_name}\")\n",
    "            \n",
    "            return vector_store\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"โ Error creating FAISS vector store: {e}\")\n",
    "            return None\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_existing_vector_store(\n",
    "        embedding_model: str = \"D:/model/BAAI-bge-m3\",\n",
    "        index_name: str = \"pdf_documents\", \n",
    "        persist_directory: str = \"./faiss_db\"\n",
    "    ) -> Optional[FAISS]:\n",
    "        \"\"\"\n",
    "        เปเบซเบผเบ Vector Store เบเบตเปเบกเบตเบขเบนเปเปเบฅเปเบงเบเบฒเบ FAISS\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Path to embedding model\n",
    "            index_name: Name of FAISS index\n",
    "            persist_directory: Directory where FAISS index is saved\n",
    "            \n",
    "        Returns:\n",
    "            FAISS vector store object or None\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # เบเบงเบเบชเบญเบเบงเปเบฒเบกเบตเปเบเบฅเปเบเบตเบซเบผเบทเบเปเป\n",
    "            faiss_path = os.path.join(persist_directory, index_name)\n",
    "            if not os.path.exists(faiss_path):\n",
    "                print(f\"โ FAISS index not found: {faiss_path}\")\n",
    "                return None\n",
    "            \n",
    "            # เบเบงเบเบชเบญเบเบงเปเบฒเบกเบตเปเบเบฅเปเบเบตเปเบเบณเปเบเบฑเบเบซเบผเบทเบเปเป\n",
    "            index_file = os.path.join(faiss_path, \"index.faiss\")\n",
    "            pkl_file = os.path.join(faiss_path, \"index.pkl\")\n",
    "            \n",
    "            if not os.path.exists(index_file) or not os.path.exists(pkl_file):\n",
    "                print(f\"โ FAISS files not found in: {faiss_path}\")\n",
    "                return None\n",
    "            \n",
    "            # เปเบซเบผเบ embedding model\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=embedding_model,\n",
    "                model_kwargs={'device': 'cpu'},\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "            \n",
    "            # เปเบซเบผเบ FAISS vector store\n",
    "            vector_store = FAISS.load_local(\n",
    "                faiss_path, \n",
    "                embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            \n",
    "            # เบเบงเบเบชเบญเบเบงเปเบฒเบกเบตเบเปเปเบกเบนเบเบซเบผเบทเบเปเป\n",
    "            if hasattr(vector_store, 'index') and vector_store.index.ntotal > 0:\n",
    "                print(f\"โ Loaded existing FAISS vector store: {index_name}\")\n",
    "                print(f\"๐ Total vectors: {vector_store.index.ntotal}\")\n",
    "                return vector_store\n",
    "            else:\n",
    "                print(f\"โ๏ธ  FAISS index '{index_name}' is empty\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"โ Error loading FAISS vector store: {e}\")\n",
    "            return None\n",
    "        \n",
    "    @staticmethod\n",
    "    def search_similar_documents(\n",
    "        vector_store: FAISS,\n",
    "        query: str,\n",
    "        k: int = 5\n",
    "    ) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        เบเบปเปเบเบซเบฒเปเบญเบเบฐเบชเบฒเบเบเบตเปเบเปเบฒเบเบเบทเบเบฑเบ\n",
    "        vector_store: เปเบกเปเบเบเปเปเบกเบนเปเบฎเบปเบฒเปเบเบตเบเบชเปเบฒเบ Vector Store เปเบ ./faiss_db\n",
    "        query: เบเบณเบเบฒเบกเบเบตเปเบเปเบญเบเบเบฒเบเบเบปเปเบเบซเบฒ\n",
    "        k: เบเบณเบเบงเบเบเบปเบเบฅเบฑเบเบเบตเปเบเปเบญเบเบเบฒเบ\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples (document, score)\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"๐ Searching for: {query}\")\n",
    "            \n",
    "            # เบเบปเปเบเบซเบฒเบเปเบงเบ score\n",
    "            results = vector_store.similarity_search_with_score(\n",
    "                query=query,\n",
    "                k=k\n",
    "            )\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"โ Error during search: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqRAGSystem:\n",
    "    \"\"\"\n",
    "    เบฅเบฐเบเบปเบ RAG เบเบฐเบชเบปเบกเบเบฑเบ Groq LLM เปเบเบทเปเบญเบเบญเบเบเบณเบเบฒเบกเบญเปเบฒเบเบญเบตเบเบเบฒเบเปเบญเบเบฐเบชเบฒเบ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, groq_api_key: str, model_name: str = \"openai/gpt-oss-120b\"):\n",
    "        \"\"\"\n",
    "        เปเบฅเบตเปเบกเบเบปเปเบ GroqRAGSystem\n",
    "        \n",
    "        Args:\n",
    "            groq_api_key: Groq API key (เบเปเบญเบเปเบเบชเบฐเปเบฑเบเบเบตเป https://console.groq.com)\n",
    "            model_name: เบเบทเป Model เบเบตเปเบเบฐเปเบเป (เบเบปเบเบเบปเบงเบขเปเบฒเบ: openai/gpt-oss-120b)\n",
    "        \"\"\"\n",
    "        self.client = Groq(api_key=groq_api_key)\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def create_context_from_documents(self, search_results: List[tuple]) -> str:\n",
    "        \"\"\"\n",
    "        เบชเปเบฒเบ context เบเบฒเบเบเบปเบเบเบฒเบเบเบปเปเบเบซเบฒเปเบญเบเบฐเบชเบฒเบ\n",
    "        \n",
    "        Args:\n",
    "            search_results: List of tuples (document, score) เบเบฒเบ vector search\n",
    "            \n",
    "        Returns:\n",
    "            เบเปเปเบเบงเบฒเบก context เบชเบณเบฅเบฑเบ LLM\n",
    "        \"\"\"\n",
    "        if not search_results:\n",
    "            return \"เบเปเปเบเบปเบเปเบญเบเบฐเบชเบฒเบเบเบตเปเบเปเบฝเบงเบเปเบญเบ\"\n",
    "            \n",
    "        context_parts = []\n",
    "        for i, (doc, score) in enumerate(search_results):\n",
    "            # FAISS เปเบเป cosine distance, เบเปเบฒเบเปเบณเปเบฒเบเบเบงเบฒเบกเบงเปเบฒเบเปเบฒเบเบเบทเบเบฑเบเบซเบผเบฒเบ\n",
    "            similarity = 1 - score  # เบเปเบฝเบเปเบเบฑเบ similarity\n",
    "            source_info = f\"เปเบซเบผเปเบ: {doc.metadata.get('source_file', 'Unknown')} (เปเปเบฒ {doc.metadata.get('page', 'Unknown')})\"\n",
    "            content = doc.page_content.strip()\n",
    "            \n",
    "            context_parts.append(f\"เปเบญเบเบฐเบชเบฒเบ {i+1} (เบเบงเบฒเบกเบเปเบฒเบเบเบท: {similarity:.3f}):\\n{source_info}\\n{content}\\n\")\n",
    "            \n",
    "        return \"\\n---\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_answer(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        เบชเปเบฒเบเบเบณเบเบญเบเปเบเบเปเบเป Groq LLM เบเปเบญเบก context เบเบฒเบเปเบญเบเบฐเบชเบฒเบ\n",
    "        \n",
    "        Args:\n",
    "            query: เบเบณเบเบฒเบกเบเบญเบเบเบนเปเปเบเป\n",
    "            context: Context เบเบฒเบเปเบญเบเบฐเบชเบฒเบ\n",
    "            \n",
    "        Returns:\n",
    "            เบเบณเบเบญเบเบเบฒเบ LLM\n",
    "        \"\"\"\n",
    "        \n",
    "        # เบชเปเบฒเบ prompt เบชเบณเบฅเบฑเบ RAG\n",
    "        prompt = f\"\"\"เบเปเบฒเบเปเบเบฑเบ AI Assistant เบเบตเปเบเปเบฝเบงเบเบฒเบเปเบเบเบฒเบเบเบญเบเบเบณเบเบฒเบกเปเบเบเบญเปเบฒเบเบญเบตเบเบเบฒเบเปเบญเบเบฐเบชเบฒเบเบเบตเปเปเบซเปเบกเบฒ.\n",
    "\n",
    "เบเบณเปเบเบฐเบเบณ:\n",
    "1. เบเบญเบเบเบณเบเบฒเบกเปเบเบเบญเปเบฒเบเบญเบตเบเบเบฒเบเปเบญเบเบฐเบชเบฒเบเบเบตเปเปเบซเปเบกเบฒเปเบเบปเปเบฒเบเบฑเปเบ\n",
    "2. เบเปเบฒเบเปเปเบเบปเบเบเบณเบเบญเบเปเบเปเบญเบเบฐเบชเบฒเบ, เปเบซเปเบเบญเบเบงเปเบฒเบเปเปเบเบปเบเบเปเปเบกเบนเบเบเบตเปเบเปเบฝเบงเบเปเบญเบ\n",
    "3. เบฅเบฐเบเบธเปเบซเบผเปเบเบเปเปเบกเบนเบเบเบตเปเปเบเปเปเบเบเบฒเบเบเบญเบ\n",
    "4. เบเบญเบเปเบเบฑเบเบเบฒเบชเบฒเบฅเบฒเบง เปเบฅเบฐ เปเบซเปเบเบณเบเบญเบเบเบตเปเบเบฑเบเปเบเบ, เบฅเบฐเบญเบฝเบ\n",
    "5. เบเบญเบเปเบซเปเปเบเบฑเบ Format markdown\n",
    "\n",
    "เปเบญเบเบฐเบชเบฒเบเบญเปเบฒเบเบญเบตเบ:\n",
    "{context}\n",
    "\n",
    "เบเบณเบเบฒเบก: {query}\n",
    "\n",
    "เบเบณเบเบญเบ:\"\"\"\n",
    "\n",
    "        try:\n",
    "            # เบชเบปเปเบ request เปเบ Groq\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                model=self.model_name,\n",
    "                temperature=0.1,  # เบเบงเบฒเบกเบชเปเบฒเบเบชเบฑเบเบเปเบณ เปเบเบทเปเบญเบเบงเบฒเบกเปเบกเปเบเบเบณ  เบเบถเปเบเบเบณ Model เปเบเบฒเบฐเบเปเบฒ temperature เปเบเปเบฅเบฐเปเบเบปเปเบฒเบกเบฑเบเบเปเบฒเบเบเบฑเบ\n",
    "                max_tokens=2000,  # เบเบณเบเบงเบ tokens เบชเบนเบเบชเบธเบ \n",
    "            )\n",
    "            \n",
    "            answer = chat_completion.choices[0].message.content\n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"โ เปเบเบตเบเบเปเปเบเบดเบเบเบฒเบเปเบเบเบฒเบเบชเปเบฒเบเบเบณเบเบญเบ: {str(e)}\"\n",
    "    \n",
    "    def query_documents(self, vector_store: FAISS, query: str, k: int = 5) -> dict:\n",
    "        \"\"\"\n",
    "        เบเบณเบเบฒเบกเปเบเบเบชเบปเบกเบเบนเบเบเบฒเบเบเบฒเบเบเบปเปเบเบซเบฒเปเบญเบเบฐเบชเบฒเบเบเบปเบเปเบเบตเบเบเบฒเบเบชเปเบฒเบเบเบณเบเบญเบ\n",
    "        \n",
    "        Args:\n",
    "            vector_store: FAISS vector store\n",
    "            query: เบเบณเบเบฒเบกเบเบญเบเบเบนเปเปเบเป\n",
    "            k: เบเบณเบเบงเบเปเบญเบเบฐเบชเบฒเบเบเบตเปเบเบฐเบเบปเปเบเบซเบฒ\n",
    "            \n",
    "        Returns:\n",
    "            dict เบเบตเปเบเบฐเบเบญเบเบเปเบงเบ answer, context, เปเบฅเบฐ sources\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n๐ค Processing query: {query}\")\n",
    "        \n",
    "        # 1. เบเบปเปเบเบซเบฒเปเบญเบเบฐเบชเบฒเบเบเบตเปเบเปเบฝเบงเบเปเบญเบ\n",
    "        search_results = DocumentLoader.search_similar_documents(\n",
    "            vector_store=vector_store,\n",
    "            query=query,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        if not search_results:\n",
    "            return {\n",
    "                \"answer\": \"โ เบเปเปเบเบปเบเปเบญเบเบฐเบชเบฒเบเบเบตเปเบเปเบฝเบงเบเปเบญเบเบเบฑเบเบเบณเบเบฒเบกเบเบญเบเบเปเบฒเบ\",\n",
    "                \"context\": \"\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # 2. เบชเปเบฒเบ context เบเบฒเบเบเบปเบเบเบฒเบเบเบปเปเบเบซเบฒ\n",
    "        context = self.create_context_from_documents(search_results)\n",
    "        \n",
    "        # 3. เบชเปเบฒเบเบเบณเบเบญเบเบเปเบงเบ LLM\n",
    "        print(\"๐ง Generating answer with Groq LLM...\")\n",
    "        answer = self.generate_answer(query, context)\n",
    "        \n",
    "        # 4. เบชเปเบฒเบเบฅเบฒเบเบเบทเปเปเบซเบผเปเบเบเปเปเบกเบนเบ\n",
    "        sources = []\n",
    "        for doc, score in search_results:\n",
    "            similarity = 1 - score\n",
    "            sources.append({\n",
    "                \"source_file\": doc.metadata.get('source_file', 'Unknown'),\n",
    "                \"page\": doc.metadata.get('page', 'Unknown'),\n",
    "                \"similarity\": f\"{similarity:.3f}\",\n",
    "                \"content_preview\": doc.page_content\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"context\": context,\n",
    "            \"sources\": sources\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    เบเบฑเบเบเบฑเปเบเบซเบผเบฑเบเบชเบณเบฅเบฑเบเบเบฒเบเบเบปเบเบชเบญเบเบฅเบฐเบเบปเบ RAG เบเบฑเบ Groq เปเบฅเบฐ FAISS\n",
    "    \"\"\"\n",
    "    \n",
    "    # เบเบฒเบเบเบฑเปเบเบเปเบฒ\n",
    "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")  # เปเบเบเบเปเบฒเบเปเบงเบ API key เบเบดเบ\n",
    "    \n",
    "    # เบฅเบฒเบเบเบทเปเปเบเบฅเป PDF (เบเปเบฒเบเปเบญเบเบเบฒเบเบชเปเบฒเบ vector store เปเปเป)\n",
    "    pdf_files = [ \n",
    "        \"C:/Users/Dell/Desktop/Finetuing vs RAG.pdf\"\n",
    "    ]\n",
    "    \n",
    "    # เบเบงเบเบชเบญเบเบงเปเบฒเบกเบต vector store เบขเบนเปเปเบฅเปเบงเบซเบผเบทเบเปเป\n",
    "    \n",
    "    display(Markdown(\"## ๐ เบเบงเบเบชเบญเบ FAISS Vector Store\")) \n",
    "    loaded_vectorstore = DocumentLoader.load_existing_vector_store(\n",
    "        embedding_model=\"D:/model/BAAI-bge-m3\", \n",
    "        index_name=\"pdf_documents\", \n",
    "        persist_directory=\"./faiss_db\"\n",
    "    )\n",
    "    \n",
    "    # เบเปเบฒเบเปเปเบกเบต vector store, เบชเปเบฒเบเปเปเป\n",
    "    if loaded_vectorstore is None:\n",
    "        display(Markdown(\"## ๐ Creating new FAISS vector store...\"))  \n",
    "        \n",
    "        # 1. เปเบซเบผเบเปเบญเบเบฐเบชเบฒเบ\n",
    "        documents = DocumentLoader.load_docs(pdf_files) \n",
    "        \n",
    "        if not documents:\n",
    "            print(\"โ No documents found. Please check your PDF file paths.\")\n",
    "            return\n",
    "            \n",
    "        # 2. เปเบฎเบฑเบ chunking เบเปเปเบกเบนเบ \n",
    "        display(Markdown(\"## โ๏ธ Chunking documents...\")) \n",
    "        # เปเบฎเบฑเบ chunking เบเปเปเบกเบนเบ\n",
    "        # เปเบเป Model เบเบญเบ BAAI-bge-m3 เปเบเบทเปเบญเบฎเบฑเบเบเปเบฒเบเบฒเบเปเบฎเบฑเบ chunking เบเปเปเบกเบนเบ เปเบเบตเปเบเบเบนเปเปเบเปเปเบกเปเบเบชเบฒเบกเบฒเบเปเบฅเบทเบญเบเปเบเปเบเบฒเบกเปเบเปเบฅเบตเบเบงเปเบฒเบเบฐ เปเบเป Model เบเบฑเบเปเบเบเบฒเบเปเบฎเบฑเบ Embedding เบชเบฒเบกเบฒเบเปเบซเบฅเบเบเปเบฒเบ Hugginface เปเบเป เปเบเบเบเบณเบเบปเบ path เปเบญเบ เบชเบฒเบกเบฒเบ เปเบเบปเปเบฒเปเบเปเบ Folder Download Model/download-model.ipynb เปเบเบทเปเบญเบเบฒเบงเปเบซเบฅเบ Model เบเบฑเบ\n",
    "        # เบเบณเบเบฑเบเบเปเบฒเบเปเบฒเบเปเบเบญเบ chunking เปเบเบ Base on เบเบฒเบเปเบญเบเบฐเบชเบฒเบ เบเปเบฒ เบกเบตเปเบญเบเบฐเบชเบฒเบเบซเบฅเบฒเบเบซเบเปเบฒ เปเบเบฐเบเบณเปเบซเปเบฅเบญเบเปเบเบดเปเบกเบเปเบฒ chunk_size เปเบฅเบฐ chunk_overlap เปเบเบทเปเบญเบฎเบฑเบเบเปเบฒเบเบตเปเบเบตเบเบงเปเบฒ\n",
    "        chunk_documents = DocumentLoader.chunk_documents_standard(\n",
    "            documents, \n",
    "            chunk_size=500, \n",
    "            chunk_overlap=50, \n",
    "            tokenizer_model=\"D:/model/BAAI-bge-m3\", \n",
    "            max_token_limit=1000\n",
    "        )\n",
    "        \n",
    "        if not chunk_documents:\n",
    "            print(\"โ Failed to chunk documents.\")\n",
    "            return\n",
    "            \n",
    "        # 3. เบชเปเบฒเบ vector store เบเปเบงเบ FAISS\n",
    "        display(Markdown(\"## ๐ Creating FAISS vector store...\")) \n",
    "        # เปเบฎเบฑเบ Embedding เบเปเปเบกเบนเบ\n",
    "        # เบเปเบฅเบฐเบเบตเบเบตเปเบเบฐเบเปเบฒเบเบปเบเปเบเป เปเบเบทเปเบญเบเบเบฒเบเบงเปเบฒ เบเบฐเบกเบตเบเบฒเบเปเบญเบปเบฒ เปเบญเบเบฐเบชเบฒเบเบเบตเปเปเบฎเบปเบฒ Chunking เบกเบฒเปเบเบเปเบเบฑเบ Vector เปเบเบทเปเบญเบเบฑเบเบเบทเบเปเบ FAISS เบเปเบฒเบขเบฒเบเปเบซเปเปเบงเป เปเบเบกเบต GPU เปเบเบฐเบเบณเปเบซเปเปเบเป cuda เปเบเบ cpu\n",
    "        loaded_vectorstore = DocumentLoader.create_vector_store(chunk_documents)\n",
    "        \n",
    "        if loaded_vectorstore is None:\n",
    "            print(\"โ Failed to create FAISS vector store.\")\n",
    "            return\n",
    "    \n",
    "    # เปเบฅเบตเปเบกเบเบปเปเบเบฅเบฐเบเบปเบ RAG เบเบฑเบ Groq \n",
    "    display(Markdown(\"## ๐ Initializing Groq RAG System...\")) \n",
    "    \n",
    "    if GROQ_API_KEY == \"เปเบชเป Groq API Key เบเบญเบเปเบเบปเปเบฒเบเบตเปเบเบตเป\":\n",
    "        print(\"โ เบเบฐเบฅเบธเบเบฒเปเบชเป Groq API Key เบเบญเบเปเบเบปเปเบฒเปเบเบเบปเบงเปเบ GROQ_API_KEY\")\n",
    "        print(\"๐ก เบชเบฒเบกเบฒเบเปเบเป API key เบเบฃเบตเบเบตเป: https://console.groq.com\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        rag_system = GroqRAGSystem(\n",
    "            groq_api_key=GROQ_API_KEY,\n",
    "            model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\" \n",
    "        )\n",
    "        display(Markdown(\"## โ Groq RAG System initialized successfully !\"))  \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"โ Error initializing Groq system: {e}\")\n",
    "        return\n",
    "    \n",
    "    # เบเบปเบเบชเบญเบเบฅเบฐเบเบปเบเบเปเบงเบเบเบณเบเบฒเบกเบเบปเบงเบขเปเบฒเบ\n",
    "    test_queries = [\n",
    "        \"RAG เบเบฑเบ Fine-tuning เบกเบตเบเบงเบฒเบกเปเบเบเบเปเบฒเบเบเบฑเบเปเบเบงเปเบ ?\"\n",
    "    ]\n",
    "    \n",
    "    display(Markdown(\"## ๐งช เบเบฒเบเบเบปเบเบชเบญเบเบฅเบฐเบเบปเบ RAG\"))\n",
    "    display(Markdown(\"เบเบปเบเบชเบญเบเบเปเบงเบเบเบณเบเบฒเบกเบเบปเบงเบขเปเบฒเบ\"))\n",
    "    display(Markdown(\"---\"))\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n๐ เบเบณเบเบฒเบกเบเบตเป {i}: {query}\")\n",
    "        print(\"-\" * 40)\n",
    "        display(Markdown(f\"### ๐ เบเบณเบเบฒเบกเบเบตเป {i}: {query}\"))\n",
    "        display(Markdown(\"---\"))\n",
    "        \n",
    "        # เบชเบปเปเบเบเบณเบเบฒเบกเปเบเบฅเบฐเบเบปเบ RAG\n",
    "        result = rag_system.query_documents(\n",
    "            vector_store=loaded_vectorstore,\n",
    "            query=query,\n",
    "            k=5  # เบเบปเปเบเบซเบฒ 5 เปเบญเบเบฐเบชเบฒเบเบเบตเปเบเปเบฝเบงเบเปเบญเบ\n",
    "        )\n",
    "        \n",
    "        # เบชเบฐเปเบเบเบเบปเบเบฅเบฑเบ\n",
    "        display(Markdown(\"#### ๐ค เบเบณเบเบญเบ:\"))\n",
    "        display(Markdown(f\"\"\"\n",
    "            ```\n",
    "            {result['answer']}\n",
    "            ```\n",
    "        \"\"\"))\n",
    "        \n",
    "        if result['sources']:\n",
    "            display(Markdown(\"#### ๐ เปเบซเบผเปเบเบเปเปเบกเบนเบเบญเปเบฒเบเบญเบตเบ:\"))\n",
    "                \n",
    "            sources_md = \"\"\n",
    "            for j, source in enumerate(result['sources'], 1):\n",
    "                    sources_md += f\"\"\"\n",
    "                **{j}.** `{source['source_file']}` (เปเปเบฒ {source['page']}) - เบเบงเบฒเบกเบเปเบฒเบเบเบท: `{source['similarity']}`\n",
    "                > {source['content_preview']}...\n",
    "\n",
    "            \"\"\"\n",
    "            display(Markdown(sources_md))\n",
    "                    \n",
    "        display(Markdown(\"---\"))\n",
    "    \n",
    "    # เปเปเบ interactive เบชเบณเบฅเบฑเบเบเบนเปเปเบเปเบชเบฒเบกเบฒเบเบเบฒเบกเบเบณเบเบฒเบกเปเบญเบ\n",
    "    display(Markdown(\"## ๐ฌ เปเปเบ Interactive - เบเบดเบกเบเบณเบเบฒเบกเบเบญเบเบเปเบฒเบ (เบเบดเบก 'quit' เปเบเบทเปเบญเบญเบญเบ\")) \n",
    "    display(Markdown(\"---\"))\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_query = input(\"\\nโ เบเบณเบเบฒเบกเบเบญเบเบเปเบฒเบ: \").strip()\n",
    "            \n",
    "            if user_query.lower() in ['quit', 'exit', 'เบญเบญเบ']:\n",
    "                print(\"๐ เบเบญเบเปเบเบเบตเปเปเบเปเบฅเบฐเบเบปเบ RAG!\")\n",
    "                break\n",
    "                \n",
    "            if not user_query:\n",
    "                print(\"โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\")\n",
    "                continue\n",
    "            \n",
    "            display(Markdown(f\"### โ เบเบณเบเบฒเบก: `{user_query}`\"))\n",
    "            \n",
    "            # เบชเบปเปเบเบเบณเบเบฒเบกเปเบเบฅเบฐเบเบปเบ RAG\n",
    "            result = rag_system.query_documents(\n",
    "                vector_store=loaded_vectorstore,\n",
    "                query=user_query,\n",
    "                k=5\n",
    "            )\n",
    "            \n",
    "            # เบชเบฐเปเบเบเบเบปเบเบฅเบฑเบ\n",
    "            display(Markdown(\"#### ๐ค เบเบณเบเบญเบ:\"))\n",
    "            display(Markdown(f\"\"\"\n",
    "                ```\n",
    "                {result['answer']}\n",
    "                ```\n",
    "            \"\"\"))\n",
    "            \n",
    "            # เบชเบฐเปเบเบเปเบซเบผเปเบเบเปเปเบกเบนเบ (เปเบเบเบซเบเปเป)\n",
    "            if result['sources']: \n",
    "                display(Markdown(\"#### ๐ เปเบซเบผเปเบเบเปเปเบกเบนเบเบญเปเบฒเบเบญเบตเบ:\"))\n",
    "                for source in result['sources'][:3]:  # เบชเบฐเปเบเบ 3 เปเบซเบผเปเบเบเบณเบญเบดเบ\n",
    "                    display(Markdown(f\"#### โข {source['source_file']} (เปเปเบฒ {source['page']})\")) \n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n๐ เบเบญเบเปเบเบเบตเปเปเบเปเบฅเบฐเบเบปเบ RAG!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"โ เปเบเบตเบเบเปเปเบเบดเบเบเบฒเบ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ๐ เบเบงเบเบชเบญเบ FAISS Vector Store"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โ Loaded existing FAISS vector store: pdf_documents\n",
      "๐ Total vectors: 2\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## ๐ Initializing Groq RAG System..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## โ Groq RAG System initialized successfully !"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## ๐งช เบเบฒเบเบเบปเบเบชเบญเบเบฅเบฐเบเบปเบ RAG"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "เบเบปเบเบชเบญเบเบเปเบงเบเบเบณเบเบฒเบกเบเบปเบงเบขเปเบฒเบ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "๐ เบเบณเบเบฒเบกเบเบตเป 1: RAG เบเบฑเบ Fine-tuning เบกเบตเบเบงเบฒเบกเปเบเบเบเปเบฒเบเบเบฑเบเปเบเบงเปเบ ?\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ๐ เบเบณเบเบฒเบกเบเบตเป 1: RAG เบเบฑเบ Fine-tuning เบกเบตเบเบงเบฒเบกเปเบเบเบเปเบฒเบเบเบฑเบเปเบเบงเปเบ ?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "๐ค Processing query: RAG เบเบฑเบ Fine-tuning เบกเบตเบเบงเบฒเบกเปเบเบเบเปเบฒเบเบเบฑเบเปเบเบงเปเบ ?\n",
      "๐ Searching for: RAG เบเบฑเบ Fine-tuning เบกเบตเบเบงเบฒเบกเปเบเบเบเปเบฒเบเบเบฑเบเปเบเบงเปเบ ?\n",
      "๐ง Generating answer with Groq LLM...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### ๐ค เบเบณเบเบญเบ:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            ```\n",
       "            ## เบเบงเบฒเบกเปเบเบเบเปเบฒเบเบฅเบฐเบซเบงเปเบฒเบ RAG เปเบฅเบฐ Fine-tuning\n",
       "\n",
       "RAG (Retrieval-Augmented Generation) เปเบฅเบฐ Fine-tuning เปเบกเปเบเบชเบญเบเบงเบดเบเบตเบเบตเปเปเบเปเปเบเบเบฒเบเบเบฑเบเบเบฐเบเบฒเปเบเบเบเปเบฒเบฅเบญเบเบเบฒเบชเบฒเปเบซเบเป (LLM) เบชเปเบฒเบฅเบฑเบเบเบฒเบเบเปเบฒเปเบเปเบชเบฐเปเบเบฒเบฐเบเปเบฒเบ. เบเบงเบฒเบกเปเบเบเบเปเบฒเบเบซเบผเบฑเบเบฅเบฐเบซเบงเปเบฒเบเบเบงเบเบกเบฑเบเบขเบนเปเปเบเบงเบดเบเบตเบเบฒเบเบเบตเปเบเบงเบเปเบเบปเบฒเบเบฑเบเบเบฒเบเบเบฑเบเบเปเปเบกเบนเบเปเบฅเบฐเบเบฒเบเบเบฑเบเบเบธเบเปเบเบเบเปเบฒเบฅเบญเบ.\n",
       "\n",
       "### Fine-tuning\n",
       "\n",
       "*   **เบเบฒเบเบเบฐเบเบดเบเบฑเบ**: เบเบฒเบเปเบเบฑเบเบเบณเบเปเปเบกเบนเบเบเปเบฒเบเบงเบเบซเบผเบงเบเบซเบผเบฒเบ, เบเบฒเบเปเบฅเบทเบญเบเปเบเบเบเปเบฒเบฅเบญเบเบเบตเปเปเบซเบกเบฒเบฐเบชเบปเบก, เบเบฒเบเบเบถเบเบญเบปเบเบฎเบปเบกเปเบเบเบเปเบฒเบฅเบญเบ, เปเบฅเบฐเบเบฒเบเบเปเบฒเปเบเปเบเป.\n",
       "*   **เบเปเปเบเบต**: เบเบฒเบเบชเปเบฒเบเบเปเบฒเบเบญเบเปเบเบ end-to-end.\n",
       "*   **เบเปเปเปเบชเบ**: เบเปเบฒเปเบเปเบเปเบฒเบเปเบเบเบฒเบเบเบถเบเบญเบปเบเบฎเบปเบกเบชเบนเบ, เบเบงเบฒเบกเบเบฒเบเบฅเปเบฒเปเบเบเบฒเบเบเบฑเบเบเบธเบเบเบงเบฒเบกเบฎเบนเป.\n",
       "\n",
       "### RAG\n",
       "\n",
       "*   **เบเบฒเบเบเบฐเบเบดเบเบฑเบ**: เบเบฒเบเปเบเบฑเบเบเบณเปเบญเบเบฐเบชเบฒเบเบเบตเปเบเปเบฝเบงเบเปเบญเบ, เบเบฒเบเบชเปเบฒเบเบเบฒเบเบเบงเบฒเบกเบฎเบนเป, เบเบฒเบเบเปเบฒเปเบเปเปเบเบเบเปเบฒเบฅเบญเบ, เปเบฅเบฐเบเบฒเบเบเบฑเบเบเบฐเบเบฒเปเบญเบฑเบเบเบฅเบดเปเบเบเบฑเบ RAG.\n",
       "*   **เบเปเปเบเบต**: เบเบฒเบเบเบฑเบเบเบธเบเบเบงเบฒเบกเบฎเบนเปเปเบเบเบเบทเบเบซเบเบธเปเบ, เบเบฒเบเบฅเบงเบกเบเปเปเบกเบนเบเบซเบผเปเบฒเบชเบธเบ, เบเบงเบฒเบกเบชเบฒเบกเบฒเบเปเบเบเบฒเบเบเบฐเบซเบเบฒเบ.\n",
       "*   **เบเปเปเปเบชเบ**: เบเบงเบฒเบกเบเบฑเบเบเปเบฒเปเบเบเบฒเบเบชเปเบฒเบเบเปเบฒเบเบญเบ.\n",
       "\n",
       "### เบเบงเบฒเบกเปเบเบเบเปเบฒเบเบซเบผเบฑเบ\n",
       "\n",
       "*   Fine-tuning เปเบกเปเบเปเบซเบกเบฒเบฐเบชเบปเบกเบเบฑเบเบเบปเบเปเบเบเบเบตเปเบเปเบญเบเบเบฒเบเบเบงเบฒเบกเบเบทเบเบเปเบญเบเปเบฅเบฐเบเบงเบฒเบกเบชเบญเบเบเปเบญเบเบเบญเบเบเบงเบฒเบกเบฎเบนเปเบชเบนเบ เปเบฅเบฐเบเบฒเบเบเบฑเบเบเบธเบเบเบงเบฒเบกเบฎเบนเปเปเบกเปเบเบเปเบญเบเบเปเบฒเบเบเปเบฒ.\n",
       "*   RAG เปเบกเปเบเปเบซเบกเบฒเบฐเบชเบปเบกเบเบฑเบเบเบปเบเปเบเบเบเบตเปเบกเบตเบเบฒเบเบเบฑเบเบเบธเบเบเบงเบฒเบกเบฎเบนเปเบขเปเบฒเบเปเบงเบงเบฒเปเบฅเบฐเบเปเบญเบเบเบฒเบเบเบฒเบเบเบญเบเบชเบฐเบซเบเบญเบเปเบง.\n",
       "\n",
       "เบญเปเบฒเบเบญเบตเบเบเบฒเบเปเบญเบเบฐเบชเบฒเบ: Finetuing vs RAG.pdf (เปเปเบฒ0 เปเบฅเบฐ เปเปเบฒ1) [1](https://example.com/finetuing-vs-rag.pdf)\n",
       "            ```\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### ๐ เปเบซเบผเปเบเบเปเปเบกเบนเบเบญเปเบฒเบเบญเบตเบ:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "                **1.** `Finetuing vs RAG.pdf` (เปเปเบฒ 0) - เบเบงเบฒเบกเบเปเบฒเบเบเบท: `0.197`\n",
       "                > 1 Huawei Confidential\n",
       "How to Develop a Professional Domain Knowledge Q&A Assistant๏ผ\n",
       "Implementation\n",
       "Data Preparation: Collect a large amount of text data. These \n",
       "data need to be cleaned and annotated to ensure quality.\n",
       "Model Selection: Choose a suitable pre-trained model.\n",
       "Model Training: Input the prepared data into the model and \n",
       "adjust its parameters.\n",
       "Model Deployment: Deploy the trained model for inference.\n",
       "Advantages\n",
       "End-to-End Generation: The model can directly generate \n",
       "answers based on input questions. \n",
       "Disadvantages\n",
       "High Training Costs: Time-consuming and costly by \n",
       "requiring a large amount of data and computational resources.\n",
       "Difficulty in Knowledge Update: Once the model is trained, \n",
       "updating the knowledge requires heavy re -fine-tuning.\n",
       "Fine-tuning RAG\n",
       "Implementation\n",
       "Data Preparation: Gather a wide range of documents relevant \n",
       "to the professional domain and clean them.\n",
       "Knowledge Base Construction: Build a structured knowledge \n",
       "base that can be efficiently queried by the retrieval module.\n",
       "Model Deployment: Deploy a LLM as expected (deepseek).\n",
       "RAG Application Development: Develop a RAG application \n",
       "that integrates retrieval and generation to answer user queries.\n",
       "Advantages\n",
       "Flexible Knowledge Update: The knowledge base can be \n",
       "updated at any time without retraining the model.\n",
       "Integration of Latest Information: The system can generate \n",
       "more accurate and up -to-date answers.\n",
       "Scalability: It is easy to expand the content and scope of the \n",
       "knowledge base.\n",
       "Disadvantages\n",
       "Generation Delay: The added retrieval step may slow down \n",
       "the system's response time.\n",
       "The Fine-tuning approach is suitable for fields where knowledge accuracy and consistency are highly valued and knowledge updates\n",
       "are relatively slow, such as some traditional academic research areas. \n",
       "In contrast, the RAG approach is more suitable for fields where knowledge updates rapidly and quick responses are required, such as \n",
       "finance, healthcare, and technology....\n",
       "\n",
       "            \n",
       "                **2.** `Finetuing vs RAG.pdf` (เปเปเบฒ 1) - เบเบงเบฒเบกเบเปเบฒเบเบเบท: `-0.168`\n",
       "                > 2 Huawei Confidential\n",
       "ModelEngine: AI Toolchain that Accelerates the Implementation of AI Applications\n",
       "40% NPU pooling usage\n",
       "60%+ faster multimodal data cleansing\n",
       "1.5x online concurrency\n",
       "1.6x offline throughput\n",
       "Lower costs Fast application rollout Good inference performance\n",
       "95% accuracy\n",
       "50% shorter development latency\n",
       "Operator \n",
       "ecosystem\n",
       "ModelEngine\n",
       "Full-process AI \n",
       "toolchain\n",
       "Application enablement\n",
       "High-precision RAG application \n",
       "development and optimization\n",
       "Open toolchain Open-source framework, supporting third-party operators\n",
       "Model enablement\n",
       "Lightweight model inference \n",
       "toolchain\n",
       "Data enablement\n",
       "Automatic data processing and \n",
       "knowledge generation\n",
       "API API\n",
       "Training and inference offload acceleration\n",
       "Cache offload acceleration | Ascend+Kunpeng\n",
       "heterogeneous computing\n",
       "Retrieval acceleration\n",
       "Converged retrieval of multimodal data | Vector \n",
       "retrieval acceleration\n",
       "Low-code toolchain\n",
       "Self-orchestrated data processing | Modular \n",
       "RAG\n",
       "NPU basic software Container platform Knowledge base storage\n",
       "Model ecosystem OpenMind HuggingFace Blue Whale Market LangChain LlamaIndex\n",
       "GPU processor | Ascend NPU processor\n",
       "Resource enablement\n",
       "AI task scheduling and XPU pooling...\n",
       "\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## ๐ฌ เปเปเบ Interactive - เบเบดเบกเบเบณเบเบฒเบกเบเบญเบเบเปเบฒเบ (เบเบดเบก 'quit' เปเบเบทเปเบญเบญเบญเบ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\n",
      "\n",
      "\n",
      "๐ เบเบญเบเปเบเบเบตเปเปเบเปเบฅเบฐเบเบปเบ RAG!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
