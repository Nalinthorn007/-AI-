{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "from groq import Groq\n",
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    @staticmethod\n",
    "    def load_docs(file_paths: List[str]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        เปเบซเบฅเบ PDF documents เปเบเบเปเบเป LangChain PyPDFLoader เปเบเบฒเบฐเบเบฑเบเปเบญเบฑเบเบเบฑเบเบกเบฑเบเปเบเบฒเบฐเบกเบฑเบเบชเปเบฒเบ Metadata เปเบซเป Auto \n",
    "\n",
    "        Metadata เบเบทเบเบฑเบ ? \n",
    "        Metadata เบเบทเบเปเปเบกเบนเบเปเบเบตเปเบกเปเบเบตเบกเบเปเบฝเบงเบเบฑเบเปเบญเบเบฐเบชเบฒเบ\n",
    "        เปเบเปเบฅเบฐ Document เบเบฐเบกเบต 2 เบชเปเบงเบเบซเบผเบฑเบ:\n",
    "        1. page_content: เปเบเบทเปเบญเปเบเบเปเปเบเบงเบฒเบกเบเบดเบเป\n",
    "        2. metadata: เบเปเปเบกเบนเบเบฅเบฒเบเบฅเบฐเบญเบฝเบเบเปเบฝเบงเบเบฑเบเปเบญเบเบฐเบชเบฒเบ เปเบเบทเปเบญเบเบปเบเบเบญเบเบงเปเบฒ เปเบงเบฅเบฒเปเบฎเบปเบฒ เบเบปเปเบเบซเบฒเบเปเปเบกเบนเบ เปเบซเบผเปเบเบเปเปเบกเบนเบเบเบฑเปเบเบกเบฒเบเบฒเบเปเบช\n",
    "        \n",
    "        Args:\n",
    "            file_paths (list): List of PDF file paths\n",
    "        \n",
    "        Returns:\n",
    "            List[Document]: List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        \n",
    "        all_docs = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Warning: File {file_path} not found. Skipping...\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                file_extension = os.path.splitext(file_path)[1].lower()\n",
    "                \n",
    "                # Check if file is PDF\n",
    "                if file_extension != '.pdf':\n",
    "                    print(f\"Warning: {file_path} is not a PDF file. Skipping...\")\n",
    "                    continue\n",
    "                \n",
    "                # Load PDF using LangChain PyPDFLoader\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                documents = loader.load()\n",
    "                \n",
    "                # Add enhanced metadata to all documents\n",
    "                for doc in documents:\n",
    "                    if doc.metadata is None:\n",
    "                        doc.metadata = {}\n",
    "                        \n",
    "                    doc.metadata.update({\n",
    "                        'source_file': os.path.basename(file_path),\n",
    "                        'file_type': file_extension,\n",
    "                        'file_path': file_path,\n",
    "                        'file_size': os.path.getsize(file_path) if os.path.exists(file_path) else 0,\n",
    "                    })\n",
    "                \n",
    "                all_docs.extend(documents)\n",
    "                print(f\"โ Processed PDF: {file_path} ({len(documents)} pages)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"โ Error processing {file_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"๐ Total PDF documents loaded: {len(all_docs)}\")\n",
    "        return all_docs\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_documents_standard(\n",
    "        docs: List[Document], \n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        tokenizer_model: str = \"D:/model/BAAI-bge-m3\",\n",
    "        max_token_limit: int = 8192\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        เปเบเป Lanchain เปเบเบเบฒเบเปเบฎเบฑเบ chunking เบเปเปเบกเบนเบ เปเบเบทเปเบญเบเบฒเบเบเบปเปเบเบซเบฒเบเปเปเบกเบนเบเบเปเบงเบ ChromaDB  \n",
    "\n",
    "        Chunk_size: เปเบกเปเบเบเบณเบเบงเบเบเปเปเบกเบนเบเบเบตเปเบเบฐเปเบฎเบฑเบ chunking เบเปเปเบซเบเปเบงเบ เปเบเบฒเบฐเบเบฑเบ เปเบฎเบปเบฒเบเปเปเบชเบฒเบกเบฒเบเปเบญเบปเบฒเปเบญเบเบฐเบชเบฒเบเบเบฑเปเบเปเบปเบเปเบซเป AI เบเบญเบเปเบเป เปเบเบทเปเบญเบเบเบฒเบเบเบฒเบเปเบญเบเบฐเบชเบฒเบเบกเบตเบซเบฅเบฒเบเบซเบเปเบฒ\n",
    "        Chunk_overlap: เปเบกเปเบเบเบณเบเบงเบเบเปเปเบกเบนเบเบเบตเปเบเบฐเปเบฎเบฑเบ chunking เบเปเปเบซเบเปเบงเบ เปเบเบฒเบฐเบเบฑเบ เปเบฎเบปเบฒเบเปเปเบชเบฒเบกเบฒเบเปเบญเบปเบฒเปเบญเบเบฐเบชเบฒเบเบเบฑเปเบเปเบปเบเปเบซเป AI เบเบญเบเปเบเป เปเบเบทเปเบญเบเบเบฒเบเบเบฒเบเปเบญเบเบฐเบชเบฒเบเบกเบตเบซเบฅเบฒเบเบซเบเปเบฒ\n",
    "        Tokenizer_model: เปเบกเปเบ Model เบเบตเปเปเบฎเบปเบฒเบเบฐเปเบเปเปเบเบเบฒเบเปเบฎเบฑเบ chunking เบเปเปเบกเบนเบ เปเบเบทเปเบญเบเบฒเบเบเบปเปเบเบซเบฒเบเปเปเบกเบนเบเบเปเบงเบ ChromaDB\n",
    "        Max_token_limit: เปเบกเปเบเบเบฒเบเปเบเปเบเบชเบฑเบเบชเปเบงเบเปเบซเปเปเบซเบกเบฒเบฐเบชเบปเบกเบเบฑเบ chunk_size\n",
    "        \n",
    "        Args:\n",
    "            docs: List of LangChain Document objects\n",
    "            chunk_size: Target size for each chunk in tokens\n",
    "            chunk_overlap: Number of overlapping tokens between chunks\n",
    "            tokenizer_model: Path to tokenizer model\n",
    "            max_token_limit: Maximum tokens allowed\n",
    "            \n",
    "        Returns:\n",
    "            List of chunked LangChain Document objects\n",
    "        \"\"\"\n",
    "        \n",
    "        if not docs:\n",
    "            print(\"โ๏ธ  No documents provided for chunking\")\n",
    "            return []\n",
    "        \n",
    "        # Load tokenizer\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "            print(f\"โ Loaded tokenizer: {tokenizer_model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"โ Error loading tokenizer: {e}\") \n",
    "        \n",
    "        # Validate parameters\n",
    "        if chunk_size >= max_token_limit:\n",
    "            chunk_size = max_token_limit - 500  # Safe buffer\n",
    "            print(f\"โ๏ธ  Adjusted chunk_size to {chunk_size} for safety\")\n",
    "        \n",
    "        if chunk_overlap >= chunk_size:\n",
    "            chunk_overlap = chunk_size // 5  # 20% overlap\n",
    "            print(f\"โ๏ธ  Adjusted chunk_overlap to {chunk_overlap}\")\n",
    "        \n",
    "        # Create tokenizer-aware text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "            tokenizer=tokenizer,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            strip_whitespace=True,\n",
    "            separators=[\n",
    "                \"\\n\\n\",      # Paragraph breaks\n",
    "                \"\\n\",        # Line breaks\n",
    "                \". \",        # Sentence endings\n",
    "                \"! \",        # Exclamation endings  \n",
    "                \"? \",        # Question endings\n",
    "                \"; \",        # Semicolon breaks\n",
    "                \", \",        # Comma breaks\n",
    "                \" \",         # Word breaks\n",
    "                \"\"           # Character level\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Split documents\n",
    "        print(f\"๐ Chunking {len(docs)} documents...\")\n",
    "        chunked_docs = text_splitter.split_documents(docs)\n",
    "        \n",
    "        # Validate token counts and add metadata\n",
    "        validated_chunks = []\n",
    "        max_tokens_found = 0\n",
    "        \n",
    "        for i, chunk in enumerate(chunked_docs):\n",
    "            # Count actual tokens\n",
    "            token_count = len(tokenizer.encode(chunk.page_content))\n",
    "            max_tokens_found = max(max_tokens_found, token_count)\n",
    "            \n",
    "            # Add chunk metadata\n",
    "            if chunk.metadata is None:\n",
    "                chunk.metadata = {}\n",
    "                \n",
    "            chunk.metadata.update({\n",
    "                'chunk_id': i,\n",
    "                'token_count': token_count,\n",
    "                'char_count': len(chunk.page_content),\n",
    "                'chunk_method': 'tokenizer_based'\n",
    "            })\n",
    "            \n",
    "            # Skip if too large\n",
    "            if token_count > max_token_limit:\n",
    "                print(f\"โ๏ธ  Skipping oversized chunk {i}: {token_count} tokens\")\n",
    "                continue\n",
    "                \n",
    "            validated_chunks.append(chunk)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"โ Created {len(validated_chunks)} chunks\")\n",
    "        print(f\"๐ Max tokens in any chunk: {max_tokens_found}\")\n",
    "        \n",
    "        return validated_chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_vector_store(\n",
    "        chunked_docs: List[Document],\n",
    "        embedding_model: str = \"D:/model/BAAI-bge-m3\",\n",
    "        collection_name: str = \"pdf_documents\",\n",
    "        persist_directory: str = \"./chroma_db\",\n",
    "        batch_size: int = 50\n",
    "    ) -> Chroma:\n",
    "        \"\"\"\n",
    "        เบชเปเบฒเบ Vector Store เบเปเบงเบ ChromaDB เบเบฒเบ chunked documents\n",
    "        \n",
    "        Embedding_model: เปเบกเปเบ Model เบเบตเปเปเบเปเปเบเบเบฒเบเปเบฎเบฑเบ Embedding เปเบเบทเปเบญเบเปเบฝเบเบเปเปเบเบงเบฒเบกเปเบเบฑเบ Vector\n",
    "        Collection_name: เปเบกเปเบเบเบทเปเบเบญเบ Collection เปเบ ChromaDB เบชเบฒเบกเบฒเบเบชเปเบฒเบเบเบฒเบกเปเบ เบเบตเปเบเปเบญเบเบเบฒเบ เปเบเบฐเบเบณเปเบซเปเบชเปเบฒเบเปเบเบฑเบ Folder เบเบญเบเปเบเบกเบฑเบ เปเบฅเบฐ point เปเบ Folder เบเบฑเปเบ เปเบเบฒเบฐเบงเปเบฒ ChromaDB เปเบงเบฅเบฒเบกเบฑเบเบเบฑเบเบเบทเบเบกเบฑเบเบเบฐเบเบฑเบเบเบทเบ unique key เปเบเบตเปเบเบกเบฑเบเบเบฐเปเบฎเบฑเบเปเบซเปเปเบฎเบปเบฒเบเบณเปเบเบเบขเบฒเบ\n",
    "        Persist_directory: เปเบกเปเบเปเบเบฅเปเบเบตเบขเบธเปเบเบฑเบเบเบถเบ ChromaDB\n",
    "        Batch_size: เปเบกเปเบเบเบณเบเบงเบ chunks เบเบตเปเปเบฎเบฑเบ embedding เบเปเปเบเบฑเปเบ เปเบเบทเปเบญเบเปเบญเบเบเบฑเบ memory overflow\n",
    "        \n",
    "        Args:\n",
    "            chunked_docs: List of chunked Document objects\n",
    "            embedding_model: Path to embedding model\n",
    "            collection_name: Name for ChromaDB collection\n",
    "            persist_directory: Directory to save ChromaDB\n",
    "            batch_size: Number of documents to process at once\n",
    "            \n",
    "        Returns:\n",
    "            Chroma vector store object\n",
    "        \"\"\"\n",
    "        \n",
    "        if not chunked_docs:\n",
    "            print(\"โ๏ธ  No chunked documents provided\")\n",
    "            return None\n",
    "        \n",
    "        # Create embeddings\n",
    "        try:\n",
    "            print(f\"๐ Loading embedding model: {embedding_model}\")\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=embedding_model,\n",
    "                model_kwargs={'device': 'cpu'},  # เบเปเบฝเบเปเบเบฑเบ 'cuda' เบเปเบฒเบกเบต GPU\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "            print(f\"โ Loaded embedding model successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"โ Error loading embedding model: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Create ChromaDB client and collection\n",
    "        try:\n",
    "            # เบชเปเบฒเบเปเบเบฅเปเบเบตเบเปเบฒเบเบฑเบเบเปเปเบกเบต\n",
    "            os.makedirs(persist_directory, exist_ok=True)\n",
    "            \n",
    "            print(f\"๐ Creating ChromaDB collection: {collection_name}\")\n",
    "            \n",
    "            # เบฅเบทเบ collection เปเบเบปเปเบฒเบเปเบฒเบกเบต (เบเปเบญเบเบเบฑเบเบเปเปเบเบดเบเบเบฒเบ)\n",
    "            try:\n",
    "                client = chromadb.PersistentClient(path=persist_directory)\n",
    "                try:\n",
    "                    client.delete_collection(collection_name)\n",
    "                    print(f\"๐๏ธ  Deleted existing collection: {collection_name}\")\n",
    "                except:\n",
    "                    pass  # Collection เบเปเปเบกเบตเบขเบนเปเปเบฅเปเบง\n",
    "            except Exception as e:\n",
    "                print(f\"โ๏ธ  Warning during cleanup: {e}\")\n",
    "            \n",
    "            # เบชเปเบฒเบ vector store เปเบเบ batch\n",
    "            print(f\"๐ Processing {len(chunked_docs)} documents in batches of {batch_size}\")\n",
    "            \n",
    "            vector_store = None\n",
    "            total_processed = 0\n",
    "            \n",
    "            for i in range(0, len(chunked_docs), batch_size):\n",
    "                batch = chunked_docs[i:i + batch_size]\n",
    "                batch_num = (i // batch_size) + 1\n",
    "                total_batches = (len(chunked_docs) + batch_size - 1) // batch_size\n",
    "                \n",
    "                print(f\"๐ฆ Processing batch {batch_num}/{total_batches} ({len(batch)} documents)\")\n",
    "                \n",
    "                try:\n",
    "                    if vector_store is None:\n",
    "                        # เบชเปเบฒเบ vector store เบเบณเบญเบดเบ\n",
    "                        vector_store = Chroma.from_documents(\n",
    "                            documents=batch,\n",
    "                            embedding=embeddings,\n",
    "                            collection_name=collection_name,\n",
    "                            persist_directory=persist_directory\n",
    "                        )\n",
    "                    else:\n",
    "                        # เปเบเบตเปเบก documents เปเปเปเปเบเบปเปเบฒเปเบ\n",
    "                        vector_store.add_documents(batch)\n",
    "                    \n",
    "                    total_processed += len(batch)\n",
    "                    print(f\"โ Batch {batch_num} completed. Total processed: {total_processed}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"โ Error processing batch {batch_num}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # เบเบฑเบเบเบถเบเบเบฒเบเบเปเบฝเบเปเบเบ\n",
    "            if vector_store:\n",
    "                vector_store.persist()\n",
    "                print(f\"๐พ Vector store saved to: {persist_directory}\")\n",
    "                \n",
    "                collection_count = vector_store._collection.count()\n",
    "                print(f\"๐ Total vectors in collection: {collection_count}\")\n",
    "                print(f\"๐ Collection name: {collection_name}\")\n",
    "                \n",
    "                return vector_store\n",
    "            else:\n",
    "                print(\"โ Failed to create vector store\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"โ Error creating vector store: {e}\")\n",
    "            return None\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_existing_vector_store(\n",
    "        embedding_model: str = \"D:/model/BAAI-bge-m3\",\n",
    "        collection_name: str = \"pdf_documents\", \n",
    "        persist_directory: str = \"./chroma_db\"\n",
    "    ) -> Optional[Chroma]:\n",
    "        \"\"\"\n",
    "        เปเบซเบผเบ Vector Store เบเบตเปเบกเบตเบขเบนเปเปเบฅเปเบงเบเบฒเบ ChromaDB\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Path to embedding model\n",
    "            collection_name: Name of ChromaDB collection\n",
    "            persist_directory: Directory where ChromaDB is saved\n",
    "            \n",
    "        Returns:\n",
    "            Chroma vector store object or None\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # เบเบงเบเบชเบญเบเบงเปเบฒเบกเบตเปเบเบฅเปเบเบตเบซเบผเบทเบเปเป\n",
    "            if not os.path.exists(persist_directory):\n",
    "                print(f\"โ Directory not found: {persist_directory}\")\n",
    "                return None\n",
    "            \n",
    "            # เปเบซเบผเบ embedding model\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=embedding_model,\n",
    "                model_kwargs={'device': 'cpu'},\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "            \n",
    "            # เปเบซเบผเบ vector store\n",
    "            vector_store = Chroma(\n",
    "                collection_name=collection_name,\n",
    "                embedding_function=embeddings,\n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "            \n",
    "            # เบเบงเบเบชเบญเบเบงเปเบฒเบกเบตเบเปเปเบกเบนเบเบซเบผเบทเบเปเป\n",
    "            collection_count = vector_store._collection.count()\n",
    "            if collection_count > 0:\n",
    "                print(f\"โ Loaded existing vector store: {collection_name}\")\n",
    "                print(f\"๐ Total vectors: {collection_count}\")\n",
    "                return vector_store\n",
    "            else:\n",
    "                print(f\"โ๏ธ  Collection '{collection_name}' is empty\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"โ Error loading vector store: {e}\")\n",
    "            return None\n",
    "        \n",
    "    @staticmethod\n",
    "    def search_similar_documents(\n",
    "        vector_store: Chroma,\n",
    "        query: str,\n",
    "        k: int = 5\n",
    "    ) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        เบเบปเปเบเบซเบฒเปเบญเบเบฐเบชเบฒเบเบเบตเปเบเปเบฒเบเบเบทเบเบฑเบ\n",
    "        vector_store: เปเบกเปเบเบเปเปเบกเบนเปเบฎเบปเบฒเปเบเบตเบเบชเปเบฒเบ Vector Store เปเบ ./chroma_db\n",
    "        query: เบเบณเบเบฒเบกเบเบตเปเบเปเบญเบเบเบฒเบเบเบปเปเบเบซเบฒ\n",
    "        k: เบเบณเบเบงเบเบเบปเบเบฅเบฑเบเบเบตเปเบเปเบญเบเบเบฒเบ\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples (document, score)\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"๐ Searching for: {query}\")\n",
    "            \n",
    "            # เบเบปเปเบเบซเบฒเบเปเบงเบ score\n",
    "            results = vector_store.similarity_search_with_score(\n",
    "                query=query,\n",
    "                k=k\n",
    "            )\n",
    "            \n",
    "            # เบชเบฐเปเบเบเบเบปเบเบฅเบฑเบ\n",
    "            # for i, (doc, score) in enumerate(results):\n",
    "            #     similarity = 1 - score  # เบเปเบฝเบ distance เปเบเบฑเบ similarity\n",
    "            #     print(f\"\\n๐ Result {i+1} (Similarity: {similarity:.3f}):\")\n",
    "            #     print(f\"   ๐ Source: {doc.metadata.get('source_file', 'Unknown')}\")\n",
    "            #     print(f\"   ๐ Page: {doc.metadata.get('page', 'Unknown')}\")\n",
    "            #     print(f\"   ๐ Chunk: {doc.metadata.get('chunk_id', 'Unknown')}\")\n",
    "            #     print(f\"   ๐ Content preview: {doc.page_content[:100]}...\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"โ Error during search: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqRAGSystem:\n",
    "    \"\"\"\n",
    "    เบฅเบฐเบเบปเบ RAG เบเบฐเบชเบปเบกเบเบฑเบ Groq LLM เปเบเบทเปเบญเบเบญเบเบเบณเบเบฒเบกเบญเปเบฒเบเบญเบตเบเบเบฒเบเปเบญเบเบฐเบชเบฒเบ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, groq_api_key: str, model_name: str = \"openai/gpt-oss-120b\"):\n",
    "        \"\"\"\n",
    "        เปเบฅเบตเปเบกเบเบปเปเบ GroqRAGSystem\n",
    "        \n",
    "        Args:\n",
    "            groq_api_key: Groq API key (เบเปเบญเบเปเบเบชเบฐเปเบฑเบเบเบตเป https://console.groq.com)\n",
    "            model_name: เบเบทเป Model เบเบตเปเบเบฐเปเบเป (เบเบปเบเบเบปเบงเบขเปเบฒเบ: openai/gpt-oss-120b)\n",
    "        \"\"\"\n",
    "        self.client = Groq(api_key=groq_api_key)\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def create_context_from_documents(self, search_results: List[tuple]) -> str:\n",
    "        \"\"\"\n",
    "        เบชเปเบฒเบ context เบเบฒเบเบเบปเบเบเบฒเบเบเบปเปเบเบซเบฒเปเบญเบเบฐเบชเบฒเบ\n",
    "        \n",
    "        Args:\n",
    "            search_results: List of tuples (document, score) เบเบฒเบ vector search\n",
    "            \n",
    "        Returns:\n",
    "            เบเปเปเบเบงเบฒเบก context เบชเบณเบฅเบฑเบ LLM\n",
    "        \"\"\"\n",
    "        if not search_results:\n",
    "            return \"เบเปเปเบเบปเบเปเบญเบเบฐเบชเบฒเบเบเบตเปเบเปเบฝเบงเบเปเบญเบ\"\n",
    "            \n",
    "        context_parts = []\n",
    "        for i, (doc, score) in enumerate(search_results):\n",
    "            similarity = 1 - score\n",
    "            source_info = f\"เปเบซเบผเปเบ: {doc.metadata.get('source_file', 'Unknown')} (เปเปเบฒ {doc.metadata.get('page', 'Unknown')})\"\n",
    "            content = doc.page_content.strip()\n",
    "            \n",
    "            context_parts.append(f\"เปเบญเบเบฐเบชเบฒเบ {i+1} (เบเบงเบฒเบกเบเปเบฒเบเบเบท: {similarity:.3f}):\\n{source_info}\\n{content}\\n\")\n",
    "            \n",
    "        return \"\\n---\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_answer(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        เบชเปเบฒเบเบเบณเบเบญเบเปเบเบเปเบเป Groq LLM เบเปเบญเบก context เบเบฒเบเปเบญเบเบฐเบชเบฒเบ\n",
    "        \n",
    "        Args:\n",
    "            query: เบเบณเบเบฒเบกเบเบญเบเบเบนเปเปเบเป\n",
    "            context: Context เบเบฒเบเปเบญเบเบฐเบชเบฒเบ\n",
    "            \n",
    "        Returns:\n",
    "            เบเบณเบเบญเบเบเบฒเบ LLM\n",
    "        \"\"\"\n",
    "        \n",
    "        # เบชเปเบฒเบ prompt เบชเบณเบฅเบฑเบ RAG\n",
    "        prompt = f\"\"\"เบเปเบฒเบเปเบเบฑเบ AI Assistant เบเบตเปเบเปเบฝเบงเบเบฒเบเปเบเบเบฒเบเบเบญเบเบเบณเบเบฒเบกเปเบเบเบญเปเบฒเบเบญเบตเบเบเบฒเบเปเบญเบเบฐเบชเบฒเบเบเบตเปเปเบซเปเบกเบฒ.\n",
    "\n",
    "เบเบณเปเบเบฐเบเบณ:\n",
    "1. เบเบญเบเบเบณเบเบฒเบกเปเบเบเบญเปเบฒเบเบญเบตเบเบเบฒเบเปเบญเบเบฐเบชเบฒเบเบเบตเปเปเบซเปเบกเบฒเปเบเบปเปเบฒเบเบฑเปเบ\n",
    "2. เบเปเบฒเบเปเปเบเบปเบเบเบณเบเบญเบเปเบเปเบญเบเบฐเบชเบฒเบ, เปเบซเปเบเบญเบเบงเปเบฒเบเปเปเบเบปเบเบเปเปเบกเบนเบเบเบตเปเบเปเบฝเบงเบเปเบญเบ\n",
    "3. เบฅเบฐเบเบธเปเบซเบผเปเบเบเปเปเบกเบนเบเบเบตเปเปเบเปเปเบเบเบฒเบเบเบญเบ\n",
    "4. เบเบญเบเปเบเบฑเบเบเบฒเบชเบฒเบฅเบฒเบง เปเบฅเบฐ เปเบซเปเบเบณเบเบญเบเบเบตเปเบเบฑเบเปเบเบ, เบฅเบฐเบญเบฝเบ\n",
    "5. เบเบญเบเปเบซเปเปเบเบฑเบ Format markdown\n",
    "\n",
    "เปเบญเบเบฐเบชเบฒเบเบญเปเบฒเบเบญเบตเบ:\n",
    "{context}\n",
    "\n",
    "เบเบณเบเบฒเบก: {query}\n",
    "\n",
    "เบเบณเบเบญเบ:\"\"\"\n",
    "\n",
    "        try:\n",
    "            # เบชเบปเปเบ request เปเบ Groq\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                model=self.model_name,\n",
    "                temperature=0.1,  # เบเบงเบฒเบกเบชเปเบฒเบเบชเบฑเบเบเปเบณ เปเบเบทเปเบญเบเบงเบฒเบกเปเบกเปเบเบเบณ  เบเบถเปเบเบเบณ Model เปเบเบฒเบฐเบเปเบฒ temperature เปเบเปเบฅเบฐเปเบเบปเปเบฒเบกเบฑเบเบเปเบฒเบเบเบฑเบ\n",
    "                max_tokens=2000,  # เบเบณเบเบงเบ tokens เบชเบนเบเบชเบธเบ \n",
    "            )\n",
    "            \n",
    "            answer = chat_completion.choices[0].message.content\n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"โ เปเบเบตเบเบเปเปเบเบดเบเบเบฒเบเปเบเบเบฒเบเบชเปเบฒเบเบเบณเบเบญเบ: {str(e)}\"\n",
    "    \n",
    "    def query_documents(self, vector_store: Chroma, query: str, k: int = 5) -> dict:\n",
    "        \"\"\"\n",
    "        เบเบณเบเบฒเบกเปเบเบเบชเบปเบกเบเบนเบเบเบฒเบเบเบฒเบเบเบปเปเบเบซเบฒเปเบญเบเบฐเบชเบฒเบเบเบปเบเปเบเบตเบเบเบฒเบเบชเปเบฒเบเบเบณเบเบญเบ\n",
    "        \n",
    "        Args:\n",
    "            vector_store: ChromaDB vector store\n",
    "            query: เบเบณเบเบฒเบกเบเบญเบเบเบนเปเปเบเป\n",
    "            k: เบเบณเบเบงเบเปเบญเบเบฐเบชเบฒเบเบเบตเปเบเบฐเบเบปเปเบเบซเบฒ\n",
    "            \n",
    "        Returns:\n",
    "            dict เบเบตเปเบเบฐเบเบญเบเบเปเบงเบ answer, context, เปเบฅเบฐ sources\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n๐ค Processing query: {query}\")\n",
    "        \n",
    "        # 1. เบเบปเปเบเบซเบฒเปเบญเบเบฐเบชเบฒเบเบเบตเปเบเปเบฝเบงเบเปเบญเบ\n",
    "        search_results = DocumentLoader.search_similar_documents(\n",
    "            vector_store=vector_store,\n",
    "            query=query,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        if not search_results:\n",
    "            return {\n",
    "                \"answer\": \"โ เบเปเปเบเบปเบเปเบญเบเบฐเบชเบฒเบเบเบตเปเบเปเบฝเบงเบเปเบญเบเบเบฑเบเบเบณเบเบฒเบกเบเบญเบเบเปเบฒเบ\",\n",
    "                \"context\": \"\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # 2. เบชเปเบฒเบ context เบเบฒเบเบเบปเบเบเบฒเบเบเบปเปเบเบซเบฒ\n",
    "        context = self.create_context_from_documents(search_results)\n",
    "        \n",
    "        # 3. เบชเปเบฒเบเบเบณเบเบญเบเบเปเบงเบ LLM\n",
    "        print(\"๐ง Generating answer with Groq LLM...\")\n",
    "        answer = self.generate_answer(query, context)\n",
    "        \n",
    "        # 4. เบชเปเบฒเบเบฅเบฒเบเบเบทเปเปเบซเบผเปเบเบเปเปเบกเบนเบ\n",
    "        sources = []\n",
    "        for doc, score in search_results:\n",
    "            similarity = 1 - score\n",
    "            sources.append({\n",
    "                \"source_file\": doc.metadata.get('source_file', 'Unknown'),\n",
    "                \"page\": doc.metadata.get('page', 'Unknown'),\n",
    "                \"similarity\": f\"{similarity:.3f}\",\n",
    "                \"content_preview\": doc.page_content\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"context\": context,\n",
    "            \"sources\": sources\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    เบเบฑเบเบเบฑเปเบเบซเบผเบฑเบเบชเบณเบฅเบฑเบเบเบฒเบเบเบปเบเบชเบญเบเบฅเบฐเบเบปเบ RAG เบเบฑเบ Groq\n",
    "    \"\"\"\n",
    "    \n",
    "    # เบเบฒเบเบเบฑเปเบเบเปเบฒ\n",
    "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")  # เปเบเบเบเปเบฒเบเปเบงเบ API key เบเบดเบ\n",
    "    \n",
    "    # เบฅเบฒเบเบเบทเปเปเบเบฅเป PDF (เบเปเบฒเบเปเบญเบเบเบฒเบเบชเปเบฒเบ vector store เปเปเป)\n",
    "    pdf_files = [ \n",
    "        \"C:/Users/Dell/Desktop/FINAL_2024.pdf\"\n",
    "    ]\n",
    "    \n",
    "    # เบเบงเบเบชเบญเบเบงเปเบฒเบกเบต vector store เบขเบนเปเปเบฅเปเบงเบซเบผเบทเบเปเป\n",
    "    \n",
    "    display(Markdown(\"## ๐ เบเบงเบเบชเบญเบ Vector Store\")) \n",
    "    loaded_vectorstore = DocumentLoader.load_existing_vector_store(\n",
    "        embedding_model=\"D:/model/BAAI-bge-m3\", \n",
    "        collection_name=\"pdf_documents\", \n",
    "        persist_directory=\"./chroma_db\"\n",
    "    )\n",
    "    \n",
    "    # เบเปเบฒเบเปเปเบกเบต vector store, เบชเปเบฒเบเปเปเป\n",
    "    if loaded_vectorstore is None:\n",
    "        display(Markdown(\"## ๐ Creating new vector store...\"))  \n",
    "        \n",
    "        # 1. เปเบซเบผเบเปเบญเบเบฐเบชเบฒเบ\n",
    "        documents = DocumentLoader.load_docs(pdf_files) \n",
    "        \n",
    "        if not documents:\n",
    "            print(\"โ No documents found. Please check your PDF file paths.\")\n",
    "            return\n",
    "            \n",
    "        # 2. เปเบฎเบฑเบ chunking เบเปเปเบกเบนเบ \n",
    "        display(Markdown(\"## โ๏ธ Chunking documents...\")) \n",
    "        # เปเบฎเบฑเบ chunking เบเปเปเบกเบนเบ\n",
    "        # เปเบเป Model เบเบญเบ BAAI-bge-m3 เปเบเบทเปเบญเบฎเบฑเบเบเปเบฒเบเบฒเบเปเบฎเบฑเบ chunking เบเปเปเบกเบนเบ เปเบเบตเปเบเบเบนเปเปเบเปเปเบกเปเบเบชเบฒเบกเบฒเบเปเบฅเบทเบญเบเปเบเปเบเบฒเบกเปเบเปเบฅเบตเบเบงเปเบฒเบเบฐ เปเบเป Model เบเบฑเบเปเบเบเบฒเบเปเบฎเบฑเบ Embedding เบชเบฒเบกเบฒเบเปเบซเบฅเบเบเปเบฒเบ Hugginface เปเบเป เปเบเบเบเบณเบเบปเบ path เปเบญเบ เบชเบฒเบกเบฒเบ เปเบเบปเปเบฒเปเบเปเบ Folder Download Model/download-model.ipynb เปเบเบทเปเบญเบเบฒเบงเปเบซเบฅเบ Model เบเบฑเบ\n",
    "        # เบเบณเบเบฑเบเบเปเบฒเบเปเบฒเบเปเบเบญเบ chunking เปเบเบ Base on เบเบฒเบเปเบญเบเบฐเบชเบฒเบ เบเปเบฒ เบกเบตเปเบญเบเบฐเบชเบฒเบเบซเบฅเบฒเบเบซเบเปเบฒ เปเบเบฐเบเบณเปเบซเปเบฅเบญเบเปเบเบดเปเบกเบเปเบฒ chunk_size เปเบฅเบฐ chunk_overlap เปเบเบทเปเบญเบฎเบฑเบเบเปเบฒเบเบตเปเบเบตเบเบงเปเบฒ\n",
    "        chunk_documents = DocumentLoader.chunk_documents_standard(\n",
    "            documents, \n",
    "            chunk_size=4000, \n",
    "            chunk_overlap=400, \n",
    "            tokenizer_model=\"D:/model/BAAI-bge-m3\", \n",
    "            max_token_limit=8000\n",
    "        )\n",
    "        \n",
    "        if not chunk_documents:\n",
    "            print(\"โ Failed to chunk documents.\")\n",
    "            return\n",
    "            \n",
    "        # 3. เบชเปเบฒเบ vector store \n",
    "        display(Markdown(\"## ๐ Creating vector store...\")) \n",
    "        # เปเบฎเบฑเบ Embedding เบเปเปเบกเบนเบ\n",
    "        # เบเปเบฅเบฐเบเบตเบเบตเปเบเบฐเบเปเบฒเบเบปเบเปเบเป เปเบเบทเปเบญเบเบเบฒเบเบงเปเบฒ เบเบฐเบกเบตเบเบฒเบเปเบญเบปเบฒ เปเบญเบเบฐเบชเบฒเบเบเบตเปเปเบฎเบปเบฒ Chunking เบกเบฒเปเบเบเปเบเบฑเบ Vector เปเบเบทเปเบญเบเบฑเบเบเบทเบเปเบ ChromaDB เบเปเบฒเบขเบฒเบเปเบซเปเปเบงเป เปเบเบกเบต GPU เปเบเบฐเบเบณเปเบซเปเปเบเป cuda เปเบเบ cpu\n",
    "        loaded_vectorstore = DocumentLoader.create_vector_store(chunk_documents)\n",
    "        \n",
    "        if loaded_vectorstore is None:\n",
    "            print(\"โ Failed to create vector store.\")\n",
    "            return\n",
    "    \n",
    "    # เปเบฅเบตเปเบกเบเบปเปเบเบฅเบฐเบเบปเบ RAG เบเบฑเบ Groq \n",
    "    display(Markdown(\"## ๐ Initializing Groq RAG System...\")) \n",
    "    \n",
    "    if GROQ_API_KEY == \"เปเบชเป Groq API Key เบเบญเบเปเบเบปเปเบฒเบเบตเปเบเบตเป\":\n",
    "        print(\"โ เบเบฐเบฅเบธเบเบฒเปเบชเป Groq API Key เบเบญเบเปเบเบปเปเบฒเปเบเบเบปเบงเปเบ GROQ_API_KEY\")\n",
    "        print(\"๐ก เบชเบฒเบกเบฒเบเปเบเป API key เบเบฃเบตเบเบตเป: https://console.groq.com\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        rag_system = GroqRAGSystem(\n",
    "            groq_api_key=GROQ_API_KEY,\n",
    "            model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\" \n",
    "        )\n",
    "        display(Markdown(\"## โ Groq RAG System initialized successfully !\"))  \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"โ Error initializing Groq system: {e}\")\n",
    "        return\n",
    "    \n",
    "    # เบเบปเบเบชเบญเบเบฅเบฐเบเบปเบเบเปเบงเบเบเบณเบเบฒเบกเบเบปเบงเบขเปเบฒเบ\n",
    "    test_queries = [\n",
    "        \"RAG เบเบฑเบ Fine-tuning เบกเบตเบเบงเบฒเบกเปเบเบเบเปเบฒเบเบเบฑเบเปเบเบงเปเบ ?\"\n",
    "    ]\n",
    "    \n",
    "    display(Markdown(\"## ๐งช เบเบฒเบเบเบปเบเบชเบญเบเบฅเบฐเบเบปเบ RAG\"))\n",
    "    display(Markdown(\"เบเบปเบเบชเบญเบเบเปเบงเบเบเบณเบเบฒเบกเบเบปเบงเบขเปเบฒเบ 4 เบเบณเบเบฒเบก\"))\n",
    "    display(Markdown(\"---\"))\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n๐ เบเบณเบเบฒเบกเบเบตเป {i}: {query}\")\n",
    "        print(\"-\" * 40)\n",
    "        display(Markdown(f\"### ๐ เบเบณเบเบฒเบกเบเบตเป {i}: {query}\"))\n",
    "        display(Markdown(\"---\"))\n",
    "        \n",
    "        # เบชเบปเปเบเบเบณเบเบฒเบกเปเบเบฅเบฐเบเบปเบ RAG\n",
    "        result = rag_system.query_documents(\n",
    "            vector_store=loaded_vectorstore,\n",
    "            query=query,\n",
    "            k=5  # เบเบปเปเบเบซเบฒ 5 เปเบญเบเบฐเบชเบฒเบเบเบตเปเบเปเบฝเบงเบเปเบญเบ\n",
    "        )\n",
    "        \n",
    "        # เบชเบฐเปเบเบเบเบปเบเบฅเบฑเบ\n",
    "        display(Markdown(\"#### ๐ค เบเบณเบเบญเบ:\"))\n",
    "        display(Markdown(f\"\"\"\n",
    "            ```\n",
    "            {result['answer']}\n",
    "            ```\n",
    "        \"\"\"))\n",
    "        \n",
    "        if result['sources']:\n",
    "            display(Markdown(\"#### ๐ เปเบซเบผเปเบเบเปเปเบกเบนเบเบญเปเบฒเบเบญเบตเบ:\"))\n",
    "                \n",
    "            sources_md = \"\"\n",
    "            for j, source in enumerate(result['sources'], 1):\n",
    "                    sources_md += f\"\"\"\n",
    "                **{j}.** `{source['source_file']}` (เปเปเบฒ {source['page']}) - เบเบงเบฒเบกเบเปเบฒเบเบเบท: `{source['similarity']}`\n",
    "                > {source['content_preview']}...\n",
    "\n",
    "            \"\"\"\n",
    "            display(Markdown(sources_md))\n",
    "                    \n",
    "        display(Markdown(\"---\"))\n",
    "    \n",
    "    # เปเปเบ interactive เบชเบณเบฅเบฑเบเบเบนเปเปเบเปเบชเบฒเบกเบฒเบเบเบฒเบกเบเบณเบเบฒเบกเปเบญเบ\n",
    "    display(Markdown(\"## ๐ฌ เปเปเบ Interactive - เบเบดเบกเบเบณเบเบฒเบกเบเบญเบเบเปเบฒเบ (เบเบดเบก 'quit' เปเบเบทเปเบญเบญเบญเบ\")) \n",
    "    display(Markdown(\"---\"))\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_query = input(\"\\nโ เบเบณเบเบฒเบกเบเบญเบเบเปเบฒเบ: \").strip()\n",
    "            \n",
    "            if user_query.lower() in ['quit', 'exit', 'เบญเบญเบ']:\n",
    "                print(\"๐ เบเบญเบเปเบเบเบตเปเปเบเปเบฅเบฐเบเบปเบ RAG!\")\n",
    "                break\n",
    "                \n",
    "            if not user_query:\n",
    "                print(\"โ๏ธ เบเบฐเบฅเบธเบเบฒเปเบชเปเบเบณเบเบฒเบก\")\n",
    "                continue\n",
    "            \n",
    "            display(Markdown(f\"### โ เบเบณเบเบฒเบก: `{user_query}`\"))\n",
    "            \n",
    "            # เบชเบปเปเบเบเบณเบเบฒเบกเปเบเบฅเบฐเบเบปเบ RAG\n",
    "            result = rag_system.query_documents(\n",
    "                vector_store=loaded_vectorstore,\n",
    "                query=user_query,\n",
    "                k=5\n",
    "            )\n",
    "            \n",
    "            # เบชเบฐเปเบเบเบเบปเบเบฅเบฑเบ\n",
    "            display(Markdown(\"#### ๐ค เบเบณเบเบญเบ:\"))\n",
    "            display(Markdown(f\"\"\"\n",
    "                ```\n",
    "                {result['answer']}\n",
    "                ```\n",
    "            \"\"\"))\n",
    "            \n",
    "            # เบชเบฐเปเบเบเปเบซเบผเปเบเบเปเปเบกเบนเบ (เปเบเบเบซเบเปเป)\n",
    "            if result['sources']: \n",
    "                display(Markdown(\"#### ๐ เปเบซเบผเปเบเบเปเปเบกเบนเบเบญเปเบฒเบเบญเบตเบ:\"))\n",
    "                for source in result['sources'][:3]:  # เบชเบฐเปเบเบ 3 เปเบซเบผเปเบเบเบณเบญเบดเบ\n",
    "                    display(Markdown(f\"#### โข {source['source_file']} (เปเปเบฒ {source['page']})\")) \n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n๐ เบเบญเบเปเบเบเบตเปเปเบเปเบฅเบฐเบเบปเบ RAG!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"โ เปเบเบตเบเบเปเปเบเบดเบเบเบฒเบ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ๐ เบเบงเบเบชเบญเบ Vector Store"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โ Directory not found: ./chroma_db\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## ๐ Creating new vector store..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โ Processed PDF: C:/Users/Dell/Desktop/FINAL_2024.pdf (95 pages)\n",
      "๐ Total PDF documents loaded: 95\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## โ๏ธ Chunking documents..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โ Loaded tokenizer: D:/model/BAAI-bge-m3\n",
      "๐ Chunking 95 documents...\n",
      "โ Created 95 chunks\n",
      "๐ Max tokens in any chunk: 1415\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## ๐ Creating vector store..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๐ Loading embedding model: D:/model/BAAI-bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_53468\\3089190159.py:198: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โ Loaded embedding model successfully\n",
      "๐ Creating ChromaDB collection: pdf_documents\n",
      "๐ Processing 95 documents in batches of 50\n",
      "๐ฆ Processing batch 1/2 (50 documents)\n",
      "โ Batch 1 completed. Total processed: 50\n",
      "๐ฆ Processing batch 2/2 (45 documents)\n",
      "โ Batch 2 completed. Total processed: 95\n",
      "๐พ Vector store saved to: ./chroma_db\n",
      "๐ Total vectors in collection: 95\n",
      "๐ Collection name: pdf_documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_53468\\3089190159.py:261: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_store.persist()\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## ๐ Initializing Groq RAG System..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## โ Groq RAG System initialized successfully !"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## ๐งช เบเบฒเบเบเบปเบเบชเบญเบเบฅเบฐเบเบปเบ RAG"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "เบเบปเบเบชเบญเบเบเปเบงเบเบเบณเบเบฒเบกเบเบปเบงเบขเปเบฒเบ 4 เบเบณเบเบฒเบก"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "๐ เบเบณเบเบฒเบกเบเบตเป 1: RAG เบเบฑเบ Fine-tuning เบกเบตเบเบงเบฒเบกเปเบเบเบเปเบฒเบเบเบฑเบเปเบเบงเปเบ ?\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ๐ เบเบณเบเบฒเบกเบเบตเป 1: RAG เบเบฑเบ Fine-tuning เบกเบตเบเบงเบฒเบกเปเบเบเบเปเบฒเบเบเบฑเบเปเบเบงเปเบ ?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "๐ค Processing query: RAG เบเบฑเบ Fine-tuning เบกเบตเบเบงเบฒเบกเปเบเบเบเปเบฒเบเบเบฑเบเปเบเบงเปเบ ?\n",
      "๐ Searching for: RAG เบเบฑเบ Fine-tuning เบกเบตเบเบงเบฒเบกเปเบเบเบเปเบฒเบเบเบฑเบเปเบเบงเปเบ ?\n",
      "๐ง Generating answer with Groq LLM...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### ๐ค เบเบณเบเบญเบ:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            ```\n",
       "            โ เปเบเบตเบเบเปเปเบเบดเบเบเบฒเบเปเบเบเบฒเบเบชเปเบฒเบเบเบณเบเบญเบ: Error code: 413 - {'error': {'message': 'Request too large for model `meta-llama/llama-4-maverick-17b-128e-instruct` in organization `org_01jd9a4r3efedt6aj3yk87fgb0` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7899, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
       "            ```\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### ๐ เปเบซเบผเปเบเบเปเปเบกเบนเบเบญเปเบฒเบเบญเบตเบ:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "                **1.** `FINAL_2024.pdf` (เปเปเบฒ 10) - เบเบงเบฒเบกเบเปเบฒเบเบเบท: `-0.230`\n",
       "                > เบเบทเปเบกเบชเบฑเบเบฅเบงเบกเบเบฐเบฅเบดเบเบเบฐเบเบฑเบเบเบฑเบเปเบปเบเบเบญเบ เบเบเบเบฅ_2020_Update.2 เป๏ฟฝเบฒเบเบต 7 \n",
       "     \n",
       "  \n",
       "   \n",
       " \n",
       " \n",
       "My QR \n",
       "๏ง เปเบ๏ฟฝเบช๏ฟฝเบฒเบฅเบฑเบเบช๏ฟฝเบฒเบ QRเบเบญเบเปเบฅเบเบเบฑเบเบเบต  เปเบเบเบเบฒเบเบชเบปเปเบเปเบฅเบเบเบฑเบเบเบต \n",
       "เปเบเบเบฒเบเปเบญเบเปเบเบดเบเปเบซ๏ฟฝเบเบฑเบ. เปเบเบเบชเบฐเปเบเบเบ๏ฟฝเบฒเบ OnePay เบ๏ฟฝ\n",
       "เบเบฐเบชเบฐเปเบเบเบเบฑเบเบเบตเบเบฒเบเบเบฒเบเปเบซ๏ฟฝเบ๏ฟฝเบฒเบ เปเบเบตเปเบกเบเบงเบฒเบกเบชเบฐเบเบงเบ \n",
       "เปเบฅเบฐ เบง๏ฟฝเบญเบเปเบงเบเบง๏ฟฝเบฒ). \n",
       " \n",
       "          เบญเบญเบเบเบฑเบเบเบดเบ \n",
       "๏ง เปเบเบฑเบเบเบฑเบเบเบฑเบเบญเบญเบเบเบฑเบเบเบดเบ Chip Card) เบเบฑเบ BCEL One \n",
       "เปเบเบเบญเบญเบเบฅเบฒเบเปเบเบเบ๏ฟฝเปเบ๏ฟฝเบญเบเปเบเบปเปเบฒเบกเบฒ Counter. \n",
       " \n",
       "              OneX \n",
       "๏ง เปเบเบฑเบเบเบฑเบเบเบฑเบ เปเบเบฅเบเบเบญเบกเบเบฒเบเบ๏ฟฝเบฒเบญเบดเปเบฅเบฑเบเปเบเบผเบเบตเบ เบเบตเปเบกเบตเบเบฒเบเบเบทเป -\n",
       "เบเบฒเบเบชเบดเบเบ๏ฟฝเบฒ เปเบฅเบฐ เบเบฒเบเบ๏ฟฝเบฅเบดเบเบฒเบ เปเบฅเบฐเบชเบฒเบกเบฒเบเบเบปเบเบชเบปเปเบเบเบฒเบ\n",
       "เบซเบผเบฒเบเบ๏ฟฝเบฅเบดเบชเบฑเบเปเบ๏ฟฝ. \n",
       " \n",
       "เบชเบฑเปเบเบเบญเบเบเบทเปเบเบฑเบเบเบฐเบเบฑเบ \n",
       "๏ง เปเบเบฑเบเบเบฑเบเบเบฑเบเปเบเบเบฒเบเบเบญเบเปเบฅเบฐเบเบทเปเบเบฑเบเบเบฐเบเบฑเบเบเบญเบเบฅเบฑเบเบเบฐเบเบฒเบ \n",
       "เบเบฒเบกเบเบตเปเบกเบตเบเบฒเบเปเบ๏ฟฝเบเบเบฒเบเบเบฐเบเบฒเบเบเบฒเบเปเบเปเบ๏ฟฝเบฅเบฐเปเบฅเบเบฐ เปเบเบ)\n",
       "เบเบฒเบเบเบทเปเปเบก๏ฟฝเบเปเบกเบทเปเบญเบฎเบญเบเปเบงเบฅเบฒเบ๏ฟฝเบฒเบเบปเบ เบเบฐเบกเบตเบเบฒเบเบ๏ฟฝเบฒเบเบเบญเบ\n",
       "เปเบ๏ฟฝเบ เปเบฅเบฐเปเบเบดเบเบฅเบปเบเบเบถเบเปเบซ๏ฟฝเบเบฒเบกเบ๏ฟฝเบฒเบเบงเบ%เบเบฐเบเบฒเบเบเบฒเบ . \n",
       " \n",
       "               เบเบฐเบซเบผเบฒเบเปเบฅเบเบ๏ฟฝเบฝเบ \n",
       "๏ง เปเบเบฑเบเบเบฑเบเบเบฑเบเบฅเบฐเบเบปเบเปเบฅเบเบ๏ฟฝเบฝเบเปเบเบดเบเบเบฒเบฅเบฐเบซเบง๏ฟฝเบฒเบเบฅเบนเบ๏ฟฝเบฒ เปเบฅเบฐ เบฅเบนเบ\n",
       "เบ๏ฟฝเบฒเบเบน๏ฟฝเปเบ๏ฟฝเบ๏ฟฝเบฅเบดเบเบฒเบเบชเบฒเบกเบฒเบเบ๏ฟฝเบฒเบเบปเบเบญเบฑเบเบเบฒเปเบฅเบเบ๏ฟฝเบฝเบ เปเบฅเบฐ  ,\n",
       "เปเบฅเบทเบญเบเบงเบปเบเปเบเบดเบเบเบปเบเบเบทเปเบเบเบฒเบเบเบญเบเปเบเบเบเบตเปเบฅเบฐเบเบปเบเปเบ๏ฟฝเบ๏ฟฝเบฒเบเบปเบเปเบง๏ฟฝ  \n",
       "๏ง เบ๏ฟฝเบฒเบซเบฒเบเบง๏ฟฝเบฒเบเบฒเบเปเบฅเบเบ๏ฟฝเบฝเบเปเบเบดเบเบเบฒเบเบฑเบเบเบน๏ฟฝเบเบฑเบเปเบ๏ฟฝ เบซเบผเบท เบฅเบฐเบเบปเบเบเบฑเบ\n",
       "เบเบน๏ฟฝเปเบซ๏ฟฝเบเบฒเบกเบ๏ฟฝเบฒเบชเบฑเปเบเบเบน๏ฟฝเปเบ๏ฟฝเบ๏ฟฝเบฅเบดเบเบฒเบ เบฅเบฐเบเบปเบเบเบฐเบเบฑเบเปเบเบดเบเบเบฑเบเบเบต. \n",
       " TAP เบเบฑเปเบเบ๏ฟฝเบฒ \n",
       " เบ๏ฟฝเบฝเบเบฅเบฐเบซเบฑเบเบ๏ฟฝเบฒเบ ๏ง เบ๏ฟฝเบฝเบเบฅเบฐเบซเบฑเบเบ๏ฟฝเบฒเบเบเบตเปเปเบ๏ฟฝเปเบเบปเปเบฒเบชเบน๏ฟฝเบฅเบฐเบเบปเบ. \n",
       "เบ๏ฟฝเบฝเบ 3 เบ๏ฟฝเบฒเบเบฒเบกเบฅเบฑเบ \n",
       "๏ง เปเบเบฑเบเบเบฑเบเบเบฑเบเปเบซ๏ฟฝเบฅเบนเบเบเบตเปเบ๏ฟฝเบญเบเบเบฒเบเบ๏ฟฝเบฝเบ3 เบ๏ฟฝเบฒเบเบฒเบกเบฅเบฑเบ. \n",
       " เบเบฑเปเบเบ๏ฟฝเบฒเบฅเบฒเบเบเบตเปเบงเบกเบท \n",
       "๏ง เปเบเบฑเบเบเบฒเบเบเบฑเปเบเบ๏ฟฝเบฒเปเบเบตเปเบกเบฅเบฒเบเบเบดเปเบงเบกเบท เบชเบฒเบกเบฒเบเบชเบฐเปเบเบเบฅเบฒเบเบเบดเปเบกเบกเบท\n",
       "เปเบเบเบเบฒเบเบเบดเบกเบฅเบฐเบซเบฑเบเบ๏ฟฝเบฒเบเบ๏ฟฝเบฒเบเป เปเบเบทเปเบญ เปเบเบตเบ/เบเบดเบ เบเบฒเบเปเบเบปเปเบฒเบชเบน๏ฟฝ\n",
       "เบฅเบฐเบเบปเบ เปเบฅเบฐ เบเบฒเบเปเบเบตเปเบกเปเบฅเบเบเบฑเบเบเบตเปเบเบทเปเบญเปเบญเบเปเบเบดเบเบ๏ฟฝเบงเบเบฅเบฒเบเบเบดเปเบง\n",
       "เบกเบท. \n",
       " เบฅเบฑเบญเบ/เบเบปเบเบฅเบฑเบญเบเบเบฑเบ \n",
       "๏ง เบชเบฒเบกเบฒเบเปเบ๏ฟฝ เปเบเบตเบ/เบเบดเบ เบเบฒเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบเบฑเบ เปเบเบ๏ฟฝเบฅเบฐเบเบตเบเบฑเบเปเบชเบ, เบเบฑเบ\n",
       "เบเบทเบเบฅเบฑเบ เบซเบผเบท เบ๏ฟฝเบญเบเบเบฒเบเบฅเบฐเบ๏ฟฝเบเบเบฑเบเปเบง๏ฟฝเบเบปเปเบงเบเบฒเบง. เบ๏ฟฝเบฒเบเบฑเบเบเบทเบเบฅเบฑเบญเบ \n",
       "เบเบฐเบ๏ฟฝเปเบชเบฒเบกเบฒเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบเบฑเบเปเบเบทเปเบญเบเบญเบเปเบเบดเบ เปเบฅเบฐ เบ๏ฟฝเบฒเบฅเบฐเบ๏ฟฝเบฒเบ๏ฟฝเบฅเบดเบเบฒเบ\n",
       "เบ๏ฟฝเบฒเบเปเบ๏ฟฝเบฒเบ BCEL One เปเบ๏ฟฝ....\n",
       "\n",
       "            \n",
       "                **2.** `FINAL_2024.pdf` (เปเปเบฒ 81) - เบเบงเบฒเบกเบเปเบฒเบเบเบท: `-0.236`\n",
       "                > เบเบทเปเบกเบชเบฑเบเบฅเบงเบกเบเบฐเบฅเบดเบเบเบฐเบเบฑเบเบเบฑเบเปเบปเบเบเบญเบ เบเบเบเบฅ_2020_Update.2 เป๏ฟฝเบฒเบเบต 78 \n",
       "๏ง เปเบ๏ฟฝเบฎเบฑเบเบเบฐเปเบเบเบชเบฐเบชเบปเบกเปเบกเบฅ๏ฟฝเปเบเบตเปเบกเบเบทเปเบก 3% เบเบฒเบเบ๏ฟฝเบฒเบเบงเบเบเบฐเปเบเบเบเบตเปเปเบ๏ฟฝเปเบเบ๏ฟฝเบฝเบงเบเบดเบเบเบฑเปเบ เบซเบผเบท เบเบฑเปเบเบเบฑเปเบเบเบญเบเบเบธเบเป\n",
       "เปเบชเบฑเปเบเบเบฒเบเบเบดเบ เบเบฑเบเปเบชเบฑเปเบเบเบฒเบเบเบฒเบเปเบ เปเบฅเบฐ เบ๏ฟฝเบฒเบเบเบฐเปเบเบ. \n",
       "เบช๏ฟฝเบฒเบเบฑเบ  :เบ๏ฟฝเบฅเบฐเบเบตเบเบตเปเบ๏ฟฝเบฒเบเบกเบตเบเบฑเบเบเบฑเบเบชเบฐเบกเบฒเบเบดเบเบเบญเบเบเบฒเบเบเบดเบเบฅเบฒเบง เปเบฅเบฐ เบเบฑเบเบฎ๏ฟฝเบงเบกเบเบดเปเบชเบ Co- brand, เบ๏ฟฝเบฒเบเบชเบฒเบกเบฒเบ\n",
       "เปเบฅเบทเบญเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบชเบดเบเบเบดเบเบดเปเบชเบเบเบฒเบเบเบฑเบเปเบเบเบฑเบเปเบถเปเบเปเบเบปเปเบฒเบเบฑเปเบ  .เบ๏ฟฝเปเบชเบฒเบกเบฒเบเบชเบฐเปเปเบตเบเบฑเบเบชเบญเบเบเบฑเบ เปเบเบทเปเบญเบฎเบฑเบเบชเบดเบเบเบดเบเบดเปเบชเบเปเบเบฑเบ 2 \n",
       "เปเบเบปเปเบฒ.  \n",
       "เบเบฑเบ Co- brand \n",
       "Gold Sky \n",
       "๏ง เปเบ๏ฟฝเปเบเบฑเบเบเบฑเบเบชเบฐเบกเบฒเบเบดเบเบชเบฐเบชเบปเบกเปเบกเบฅ๏ฟฝเปเบเบทเปเบญเปเบฅเบเบเบญเบเบชเบปเบกเบกเบฐเบเบฒเบเบธเบเบเบฒเบเบเบฒเบเบเบดเบเบฅเบฒเบง, \n",
       "๏ง เปเบ๏ฟฝเบฎเบฑเบเบช๏ฟฝเบงเบเบซ๏ฟฝเบ 30%  เบช๏ฟฝเบฒเบฅเบฑเบเบซ๏ฟฝเบญเบเบเบฑเบเบฎเบฑเบเบฎเบญเบเบเบดเปเบชเบเบเบฒเบเบเบฒเบเบเบดเบเบฅเบฒเบง เบชเบฐเปเบเบฒเบฐเบขเบน๏ฟฝเบชเบฐ เปเบฒเบกเบเบดเบเบเบฒเบเบเบฒเบเบเบตเป\n",
       "เบเบฒเบเบเบดเบเบฅเบฒเบงเบกเบต เปเบฅเบฐ เปเบ๏ฟฝเบเบน๏ฟฝเบเบทเบเบฑเบเบ๏ฟฝเบญเบเปเบ๏ฟฝเบเบเบฐเบเบฑเบเบเบฒเบเบเบฒเบเบเบดเบเบฅเบฒเบง เบเบตเปเบเบธเบ Check- in เบ๏ฟฝเบฒเบ๏ฟฝเบฒเบเบ๏ฟฝเบญเบเบเบฒเบเบ๏ฟฝเบฒ\n",
       "เปเบ๏ฟฝเบซ๏ฟฝเบญเบเบเบฑเบเบฎเบฑเบเบฎเบญเบเบเบดเปเบชเบเบเบฒเบเบเบฒเบเบเบดเบเบฅเบฒเบง). \n",
       "๏ง เปเบ๏ฟฝเบชเบฐเบชเบปเบกเปเบกเบฅ๏ฟฝ เปเบเบทเปเบญเปเบฅเบเบเบทเปเบเบตเปเปเบฎเบทเบญเบเบดเบเบเบฑเบเบเบฒเบเบเบดเบเบฅเบฒเบง เปเบฅเบฐ เบเบปเบเบฅเบฐเบเบฑเบเบเบฒเบเบเบตเปเบ๏ฟฝเปเบฒเบกเบฐเบเบฒ เปเบเบฑเบเบเบตเปเบ๏ฟฝเบญเบเบเบฑเปเบ\n",
       "เบเบธเบฅเบฐเบเบดเบ เปเบ๏ฟฝเบ๏ฟฝเบญเบเปเบ๏ฟฝเบเปเบซ๏ฟฝเบเบฐเบเบฑเบเบเบฒเบ เบเบฒเบเบเบดเบเบฅเบฒเบงเบฎเบฑเบเบฎเบน๏ฟฝ 24 เบเบปเปเบงเปเบกเบเบ๏ฟฝเบญเบเบเบฒเบเปเบเบตเบเบเบฒเบ). \n",
       "๏ง เปเบ๏ฟฝเบชเบฐเบชเบปเบกเปเบกเบฅ๏ฟฝ เปเบเบทเปเบญเปเบฅเบเบเบทเปเบเบตเปเปเบฎเบทเบญเบเบดเบเบเบฑเบเบเบฒเบเบเบดเบเปเบซ๏ฟฝเปเบ๏ฟฝเบเบปเบเปเบญเบ, เบเบญเบเบเบปเบง เบซเบผเบท เปเบน๏ฟฝเปเบเบทเปเบญเบ. \n",
       "๏ง เปเบ๏ฟฝเบฎเบฑเบเบเบฐเปเบเบเบชเบฐเบชเบปเบกเปเบกเบฅ๏ฟฝเปเบเบตเปเบกเบเบทเปเบก 2% เบเบฒเบเบ๏ฟฝเบฒเบเบงเบเบเบฐเปเบเบเบเบตเปเปเบ๏ฟฝเปเบเบ๏ฟฝเบฝเบงเบเบดเบเบเบฑเปเบ เบซเบผเบท เบเบฑเปเบเบเบฑเปเบเบเบญเบเบเบธเบเป\n",
       "เปเบชเบฑเปเบเบเบฒเบเบเบดเบ เบเบฑเบเปเบชเบฑเปเบเบเบฒเบเบเบฒเบเปเบ เปเบฅเบฐ เบ๏ฟฝเบฒเบเบเบฐเปเบเบ. \n",
       "๏ง เปเบ๏ฟฝเบฎเบฑเบเบชเบดเบเบญเบฑเบเบเบฑเบเบเบต 2 เปเบเบเบฒเบเปเบ๏ฟฝเบ๏ฟฝเบญเบเบ๏ฟฝเบญเบ เบ๏ฟฝเบฅเบฐเบเบตเบฅ๏ฟฝเบ๏ฟฝเบฒ Stand by list \n",
       "๏ง เปเบ๏ฟฝเบฎเบฑเบเบช๏ฟฝเบงเบเบซ๏ฟฝเบเปเบกเบทเปเบญเปเบเบปเปเบฒเบเบฑเบเบเบตเปเปเบฎเบเปเบฎเบกเปเบกเบทเบญเบเบเบญเบ เปเบเบงเบ เบซเบผเบงเบเบเบฐเบเบฒเบ เบญเบตเบเบเบฒเบกเปเบเบผเปเบกเบเบฑเปเบเบเบตเปเปเบฎเบเปเบฎเบก\n",
       "เบญเบญเบเปเบ๏ฟฝเบฅเบฐเปเบฅเบเบฐ \n",
       "๏ง เบชเบฒเบกเบฒเบเปเบ๏ฟฝเบเบเบตเปเปเบฎเบทเบญเบเบดเบเบเบตเปเบ๏ฟฝเบญเบเบ๏ฟฝเบฅเบดเบเบฒเบเปเบ๏ฟฝเบเบเบตเปเบเบญเบเบเบฑเปเบเบเบธเบฅเบฐเบเบดเบเปเบ๏ฟฝ Business Check- in Counter) \n",
       "๏ง เปเบ๏ฟฝเบฎเบฑเบเบเบฐเปเบเบเบฒเบเบ๏ฟฝเปเบฒเปเบฑเบเปเบเบตเบ 8 Kg, เบชเบดเบเบเบตเปเบ๏ฟฝเปเบชเบฒเบกเบฒเบเปเบ๏ฟฝเปเบ๏ฟฝเบเบฑเบเบชเบฒเบเบเบฒเบเบเบดเบเบญเบทเปเบ Code Share Flight) \n",
       "๏ง เปเบ๏ฟฝเบฎเบฑเบเบชเบดเบเบเบดเปเบชเบเบญเบฑเบเบเบฑเบเบเบตเป 2 เปเบเบเบฒเบเบเบถเปเบเปเบฎเบทเบญเบเบดเบเบ๏ฟฝเบญเบเปเบงเบฅเบฒ Boarding \n",
       "๏ง เบชเบฒเบกเบฒเบเปเบเบปเปเบฒเปเบ๏ฟฝเบ๏ฟฝเบฅเบดเบเบฒเบเบเบตเปเบ๏ฟฝเบญเบเบเบญเบ-เบเบฒเบเบเบตเปเบเบฒเบเบเบดเบเบฅเบฒเบงเปเบเบเบ๏ฟฝเปเบ๏ฟฝเบญเบเบเบปเบเบเบฑเบเบเบดเบง \n",
       "๏ง เบช๏ฟฝเบฒเบเบฑเบ:  เบ๏ฟฝเบฅเบฐเบเบตเบเบตเปเบ๏ฟฝเบฒเบเบกเบตเบเบฑเบเบเบฑเบเบชเบฐเบกเบฒเบเบดเบเบเบญเบเบเบฒเบเบเบดเบเบฅเบฒเบง เปเบฅเบฐ เบเบฑเบเบฎ๏ฟฝเบงเบกเบเบดเปเบชเบ Co- brand, เบ๏ฟฝเบฒเบ\n",
       "เบชเบฒเบกเบฒเบเปเบฅเบทเบญเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบชเบดเบเบเบดเบเบดเปเบชเบเบเบฒเบเบเบฑเบเปเบเบเบฑเบเปเบถเปเบเปเบเบปเปเบฒเบเบฑเปเบ. เบ๏ฟฝเปเบชเบฒเบกเบฒเบเบชเบฐเปเปเบตเบเบฑเบเบชเบญเบเบเบฑเบ เปเบเบทเปเบญเบฎเบฑเบเบชเบดเบเบเบด\n",
       "เบเบดเปเบชเบเปเบเบฑเบ 2 เปเบเบปเปเบฒ.   \n",
       " \n",
       " \n",
       "เบเบฑเบ Co- brand \n",
       "Silver Cloud \n",
       "๏ง เปเบ๏ฟฝเปเบเบฑเบเบเบฑเบเบชเบฐเบกเบฒเบเบดเบเบชเบฐเบชเบปเบกเปเบกเบฅ๏ฟฝเปเบเบทเปเบญเปเบฅเบเบเบญเบเบชเบปเบกเบกเบฐเบเบฒเบเบธเบเบเบฒเบเบเบฒเบเบเบดเบเบฅเบฒเบง \n",
       "๏ง เปเบ๏ฟฝเบฎเบฑเบเบช๏ฟฝเบงเบเบซ๏ฟฝเบ 20%  เบช๏ฟฝเบฒเบฅเบฑเบเบซ๏ฟฝเบญเบเบเบฑเบเบฎเบฑเบเบฎเบญเบเบเบดเปเบชเบเบเบฒเบเบเบฒเบเบเบดเบเบฅเบฒเบง เบชเบฐเปเบเบฒเบฐเบขเบน๏ฟฝเบชเบฐเปเบฒเบกเบเบดเบเปเบฒเบ\n",
       "เบเบฒเบเบเบฒเบเบเบตเปเบเบฒเบเบเบดเบเบฅเบฒเบงเบกเบต เปเบฅเบฐ เปเบ๏ฟฝเบเบน๏ฟฝเบเบทเบเบฑเบเบ๏ฟฝเบญเบเปเบ๏ฟฝเบเบเบฐเบเบฑเบเบเบฒเบเบเบฒเบเบเบดเบเบฅเบฒเบง เบเบตเปเบเบธเบ Check- in เบ๏ฟฝเบฒเบ๏ฟฝเบฒเบ\n",
       "เบ๏ฟฝเบญเบเบเบฒเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบซ๏ฟฝเบญเบเบเบฑเบเบฎเบฑเบเบฎเบญเบเบเบดเปเบชเบเบเบฒเบเบเบฒเบเบเบดเบเบฅเบฒเบง). \n",
       "๏ง เปเบ๏ฟฝเบชเบฐเบชเบปเบกเปเบกเบฅ๏ฟฝ เปเบเบทเปเบญเปเบฅเบเบเบทเปเบเบตเปเปเบฎเบทเบญเบเบดเบเบเบฑเบเบเบฒเบเบเบดเบเปเบซ๏ฟฝเปเบ๏ฟฝเบเบปเบเปเบญเบ, เบเบญเบเบเบปเบง, เบซเบผเบท เปเบน๏ฟฝเปเบเบทเปเบญเบ. \n",
       "๏ง เปเบ๏ฟฝเบฎเบฑเบเบชเบดเบเบญเบฑเบเบเบฑเบเบเบต 3 เปเบเบเบฒเบเปเบ๏ฟฝเบ๏ฟฝเบญเบเบ๏ฟฝเบญเบ เบ๏ฟฝเบฅเบฐเบเบตเบฅ๏ฟฝเบ๏ฟฝเบฒ Stand by list \n",
       "๏ง เปเบ๏ฟฝเบฎเบฑเบเบเบฐเปเบเบเบชเบฐเบชเบปเบกเปเบกเบฅ๏ฟฝเปเบเบตเปเบกเบเบทเปเบก 1% เบเบฒเบเบ๏ฟฝเบฒเบเบงเบเบเบฐเปเบเบเบเบตเปเปเบ๏ฟฝเปเบเบ๏ฟฝเบฝเบงเบเบดเบเบเบฑเปเบ เบซเบผเบท เบเบฑเปเบเบเบฑเปเบเบเบญเบเบเบธเบเป\n",
       "เปเบชเบฑเปเบเบเบฒเบเบเบดเบ เบเบฑเบเปเบชเบฑเปเบเบเบฒเบเบเบฒเบเปเบ เปเบฅเบฐ เบ๏ฟฝเบฒเบเบเบฐเปเบเบ. \n",
       "๏ง เปเบ๏ฟฝเบฎเบฑเบเบช๏ฟฝเบงเบเบซ๏ฟฝเบเปเบกเบทเปเบญเปเบเบปเปเบฒเบเบฑเบเบเบตเปเปเบฎเบเปเบฎเบกเปเบกเบทเบญเบเบเบญเบ เปเบเบงเบ เบซเบผเบงเบเบเบฐเบเบฒเบ เบญเบตเบเบเบฒเบกเปเบเบผเปเบกเบเบฑเปเบเบเบตเปเปเบฎเบเปเบฎเบก\n",
       "เบญเบญเบเปเบ๏ฟฝเบฅเบฐเปเบฅเบเบฐ \n",
       "๏ง เปเบ๏ฟฝเบฎเบฑเบเบเบฐเปเบเบเบฒเบเบ๏ฟฝเปเบฒเปเบฑเบเปเบเบตเบ 5 Kg, เบชเบดเบเบเบตเปเบ๏ฟฝเปเบชเบฒเบกเบฒเบเปเบ๏ฟฝเปเบ๏ฟฝเบเบฑเบเบชเบฒเบเบเบฒเบเบเบดเบเบญเบทเปเบ Code Share Flight) \n",
       "๏ง เบช๏ฟฝเบฒเบเบฑเบ:  เบ๏ฟฝเบฅเบฐเบเบตเบเบตเปเบ๏ฟฝเบฒเบเบกเบตเบเบฑเบเบเบฑเบเบชเบฐเบกเบฒเบเบดเบเบเบญเบเบเบฒเบเบเบดเบเบฅเบฒเบง เปเบฅเบฐ เบเบฑเบเบฎ๏ฟฝเบงเบกเบเบดเปเบชเบ Co- brand, เบ๏ฟฝเบฒเบ\n",
       "เบชเบฒเบกเบฒเบเปเบฅเบทเบญเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบชเบดเบเบเบดเบเบดเปเบชเบเบเบฒเบเบเบฑเบเปเบเบเบฑเบเปเบถเปเบเปเบเบปเปเบฒเบเบฑเปเบ. เบ๏ฟฝเปเบชเบฒเบกเบฒเบเบชเบฐเปเปเบตเบเบฑเบเบชเบญเบเบเบฑเบ เปเบเบทเปเบญเบฎเบฑเบเบชเบดเบเบเบด\n",
       "เบเบดเปเบชเบเปเบเบฑเบ 2 เปเบเบปเปเบฒ. \n",
       "เบชเบดเบเบเบดเบเบฐเปเบซเบเบ 1. เบเบฐเบเบฑเบเปเบเบ๏ฟฝเบญเบเบ๏ฟฝเบฝเบงเปเบเปเบงเบฅเบฒเปเบเบตเบเบเบฒเบเปเบเบ๏ฟฝเบฒเบเบเบฐเปเบเบ Oversea Travel Insurance) *เบช๏ฟฝเบฒเบฅเบฑเบเบเบฑเบ    \n",
       "Co- Brand Platinum เปเบฅเบฐ เบเบฑเบ Co- Brand Gold เบกเบตเบเบฐเบเบฑเบเปเบ)...\n",
       "\n",
       "            \n",
       "                **3.** `FINAL_2024.pdf` (เปเปเบฒ 52) - เบเบงเบฒเบกเบเปเบฒเบเบเบท: `-0.264`\n",
       "                > เบเบทเปเบกเบชเบฑเบเบฅเบงเบกเบเบฐเบฅเบดเบเบเบฐเบเบฑเบเบเบฑเบเปเบปเบเบเบญเบ เบเบเบเบฅ_2020_Update.2 เป๏ฟฝเบฒเบเบต 49 \n",
       "๏ง เบเบฐเบเบฒเบเบฒเบเบเบฐเบ๏ฟฝเปเปเบซ๏ฟฝเบ๏ฟฝเบฅเบดเบเบฒเบเบชเบปเปเบเปเบเบดเบ  MoneyGram เปเบเบ๏ฟฝเบฅเบฐเบเบตเบเบตเปเบ๏ฟฝเปเบกเบนเบเบเบน๏ฟฝเบฎเบฑเบเปเบเบดเบเบขเบน๏ฟฝเปเบเบฅเบฒเบเบเบทเปเบ๏ฟฝเบญเบเบซ๏ฟฝเบฒเบก \n",
       "เปเบฅเบฐ เบฅเบฐเบเบปเบ AgentConnect เบ๏ฟฝเปเบญเบฐเบเบธเบเบฒเบ. \n",
       "๏ง เบ๏ฟฝเบงเบเปเบงเบฅเบฒเบฎเบฑเบเปเบเบดเบเปเบญเบ เบเบฐเบเบถเปเบเบขเบน๏ฟฝเบเบฑเบเปเบงเบฅเบฒเบเบฒเบเปเบซ๏ฟฝเบ๏ฟฝเบฅเบดเบเบฒเบเบเบญเบเบเบปเบงเปเบเบ MoneyGram  เบเบตเปเบเบฒเบเบเบฒเบ. \n",
       " \n",
       "5.5 เบเบฐเบฅเบดเบเบเบฐเบเบฑเบเปเบญเบเปเบเบดเบเบ๏ฟฝเบงเบ เบฅเบฐเบซเบง๏ฟฝเบฒเบเบเบฐเปเบเบ SpeedSend \n",
       " \n",
       "เบเบฐเบฅเบดเบเบเบฐเบเบฑเบเปเบญเบเปเบเบดเบเบ๏ฟฝเบงเบ เบฅเบฐเบซเบง๏ฟฝเบฒเบเบเบฐเปเบเบ SpeedSend \n",
       "เบเบธเบเบฅเบฑเบเบชเบฐเบเบฐ \n",
       "เปเบเบฑเบเบเบฐเบฅเบดเบเบเบฐเบเบฑเบเปเบญเบเปเบเบดเบเบ๏ฟฝเบงเบเบฅเบฐเบซเบง๏ฟฝเบฒเบเบเบฐเปเบเบ เบซเบผเบท เปเบญเบตเปเบเบง๏ฟฝเบฒ SpeedSend เบเบตเปเบชเบฒเบกเบฒเบเบเบญเบเบชเบฐเปเบญเบ \n",
       "เปเบฅเบฐ เปเบเบฑเบเบเบฒเบเปเบฅเบทเบญเบเปเบซ๏ฟฝเปเบ๏ฟฝเบเบธ๏ฟฝเบกเบฅเบนเบเบ๏ฟฝเบฒเบเบน๏ฟฝเบเบตเปเบกเบตเบเบงเบฒเบกเบ๏ฟฝเบญเบเบเบฒเบเปเบเบเบฒเบเปเบญเบเปเบเบดเบเปเบเบ๏ฟฝเบฒเบเบเบฐเปเบเบ เปเบเบฑเปเบ:  \n",
       "๏ง เปเบฎเบเบเบฒเบเบ๏ฟฝเบฒเบเบเบฐเปเบเบเบกเบฒเปเบฎเบฑเบเบงเบฝเบเปเบ เบชเบเบ เบฅเบฒเบง เปเบญเบเปเบเบดเบเบเบฑเบเบ๏ฟฝเบฒเบเปเบซ๏ฟฝเบเบญเบเบเบปเบงเบขเบน๏ฟฝเบ๏ฟฝเบฒเบเบเบฐเปเบเบ เบซเบผเบท เบเบฑเบเบเบฒ\n",
       "เปเบฎเบเบเบฒเบเบฅเบฒเบงเบเบตเปเปเบเปเบฎเบฑเบเบงเบฝเบเบขเบน๏ฟฝเบ๏ฟฝเบฒเบเบเบฐเปเบเบเปเบญเบเปเบเบดเบเบเบฑเบเบ๏ฟฝเบฒเบเปเบซ๏ฟฝเบเบญเบเบเบปเบงเบขเบน๏ฟฝเบฅเบฒเบง, \n",
       "๏ง เปเบเบฑเบเบญเบตเบเปเบถเปเบเบ๏ฟฝเบญเบเบเบฒเบเบเบตเปเบญ๏ฟฝเบฒเบเบงเบเบเบงเบฒเบกเบชเบฐเบเบงเบเปเบซ๏ฟฝเปเบ๏ฟฝเบ๏ฟฝเปเปเบก๏ฟฝเบเบน๏ฟฝเบเบปเบเบเบญเบ เบชเบฒเบกเบฒเบเบชเบปเปเบเปเบเบดเบเปเบซ๏ฟฝเบเบญเบเบเบปเบง เบซเบผเบท \n",
       "เบฅเบนเบเบซเบผเบฒเบเบเบตเปเบชเบถเบเบชเบฒเบ๏ฟฝเปเบขเบน๏ฟฝเบ๏ฟฝเบฒเบเบเบฐเปเบเบ.  \n",
       "เบเบปเบเบเบฐเปเบซเบเบเบเบตเป\n",
       "เปเบ๏ฟฝเบฎเบฑเบ \n",
       "๏ง เปเบเบฑเบเบเบฒเบเบซ๏ฟฝเบเบ๏ฟฝเบญเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบ๏ฟฝเบฒเบเบเบตเปเบ๏ฟฝเบญเบเบ๏ฟฝเบฒเบเปเบซ๏ฟฝเบกเบทเบเบฒเบ เบซเบผเบท เบเบฐเบซเบผเบฒเบเบกเบทเบ เบเบตเปเปเบญเบปเบฒเบเบฝเบเบเบน๏ฟฝเปเบญเบเปเบเบดเบ. \n",
       "๏ง เบชเบฐเบเบงเบเบชเบฐเบเบฒเบเบ๏ฟฝเบฒเบเปเบญเบเบฐเบชเบฒเบ เปเบฅเบฐ เบเบฑเปเบเบเบญเบเปเบเบเบฒเบเปเบญเบเปเบเบดเบ เปเบฅเบฐ เบฎเบฑเบเปเบเบดเบ เบ๏ฟฝเบญเบกเบ๏ฟฝเบงเบเบ๏ฟฝเบฒเบ๏ฟฝเบฒเบเบฝเบก\n",
       "เบเบฒเบเบ๏ฟฝเบฅเบดเบเบฒเบ เบเบตเปเปเบเบเปเปเบฒเบฐ เปเบฅเบฐ เบชเบฒเบกเบฒเบเบ๏ฟฝเบฒเบเปเบ๏ฟฝ.  \n",
       "๏ง เบชเบฒเบกเบฒเบเปเบญเบ-เบฎเบฑเบเปเบเบดเบเปเบ๏ฟฝเบเบฑเบเบเบฐเปเบเบเบเบฐเบก๏ฟฝเบฒ. \n",
       "๏ง เบเบเบเบฅ เบเบดเบเบเบตเบ๏ฟฝเบงเบเบ๏ฟฝเบฒเบเปเบเบเบฒเบเปเบฅเบเบ๏ฟฝเบฝเบเปเบเบดเบเบเบฒเบ๏ฟฝเบฒเบเบเบฐเปเบเบเปเบเปเบ๏ฟฝเบฅเบฐเปเบฅเบเบฐเปเบเบเบฒเบเบชเบปเปเบ-เบฎเบฑเบ. \n",
       "เปเบเบทเปเบญเบเปเบเบเบฒเบเปเบ๏ฟฝ\n",
       "เบ๏ฟฝเบฅเบดเบเบฒเบ \n",
       "๏ง เบช๏ฟฝเบฒเบฅเบฑเบเบเบธเบเบเบปเบเบเบตเปเปเบเบฑเบเบเบปเบเบ๏ฟฝเบฒเบเบเบฐเปเบเบ: เบ๏ฟฝเบญเบเบกเบตเปเบฑเบเบชเบทเปเบเบตเบเบเบฒเบเบ๏ฟฝเบญเบกเบเบฑเบเบงเบตเบ๏ฟฝเบฒเปเบเบปเปเบฒเบเบฐเปเบเบเบฅเบฒเบงเบเบฑเบเบ๏ฟฝเปเปเบปเบ\n",
       "เบ๏ฟฝเบฒเบเบปเบ. \n",
       "๏ง เบช๏ฟฝเบฒเบฅเบฑเบเบเบธเบเบเบปเบเบเบตเปเปเบเบฑเบเบเบปเบเบฅเบฒเบง เปเบฅเบฐ เบเบฒเบงเบ๏ฟฝเบฒเบเบ๏ฟฝเบฒเบง: เบ๏ฟฝเบญเบเบกเบตเบเบฑเบเบเบฐเบ๏ฟฝเบฒเบเบปเบง, เปเบฑเบเบชเบทเบ๏ฟฝเบฒเบเปเบเบ Passport) \n",
       "เบซเบผเบท เบช๏ฟฝเบฒเบกเบฐเปเบเบเบปเบง เบเบตเปเบกเบตเบญเบฒเบเบธเบเบฒเบเบ๏ฟฝเบฒเปเบ๏ฟฝเปเบ๏ฟฝเบเบฒเบกเบเบปเบเปเบฒเบ),  \n",
       " ๏ง เบชเบปเปเบ-เบฎเบฑเบเปเบเบดเบเบขเบน๏ฟฝ เบเบเบเบฅ เปเบเบฑเบเบชเบฐเบเบธเบเปเบเบฅเบฒเบชเบฐเบซเบฐเบฅเบฑเบ เปเบเบปเปเบฒเบเบฑเปเบ เบเบฒเบ เบชเบปเปเบ-เบฎเบฑเบเปเบเบดเบเปเบก๏ฟฝเบเบเบถเปเบเบเบฑเบเบเบฐเปเบเบเบฒเบ, \n",
       "เบฅเบฐเบเบฝเบเบเบญเบเบเบฐเปเบเบเบเบฑเปเบเป)  \n",
       "เบ๏ฟฝเปเบกเบนเบเปเบเบตเปเบกเบเบทเปเบก ๏ง เบ๏ฟฝเปเบกเบนเบเปเบเบตเปเบกเปเบเบตเบกเบเบฐเบฅเบธเบเบฒเปเบเบปเปเบฒเบเบตเป www.cimb.com.my/ en/ personal/ day- to- day-\n",
       "banking/ remittance/ speedsend.html เปเบ :021 263510 \n",
       "5.6 เบ๏ฟฝเบฅเบดเบเบฒเบเปเบญเบเปเบเบดเบเบญเบญเบ Outward Remittance เบ๏ฟฝเบฒเบเบฅเบฐเบเบปเบ Border Trade E-Bank) \n",
       "เบ๏ฟฝเบฅเบดเบเบฒเบเปเบญเบเปเบเบดเบเบญเบญเบ -  Outward Remittance (เบ๏ฟฝเบฒเบเบฅเบฐเบเบปเบ Border Trade E-Bank) \n",
       "เบเบธเบเบฅเบฑเบเบชเบฐเบเบฐ \n",
       "๏ง เบเบเบเบฅ เบชเบฒเบกเบฒเบเปเบซ๏ฟฝเบ๏ฟฝเบฅเบดเบเบฒเบเบ๏ฟฝเบฒเบเบเบตเปเบ๏ฟฝเบญเบเบเบฒเบเปเบญเบเปเบเบดเบ เบชเบฐเบเบธเบเปเบเบดเบเบขเบงเบ เปเบ เบช.เบ เบเบตเบ เปเบเบทเปเบทเบญเบ๏ฟฝเบฒเบฅเบฐเบ๏ฟฝเบฒเบชเบดเบเบ๏ฟฝเบฒ , \n",
       "เบ๏ฟฝเบฒเบ๏ฟฝเบฅเบดเบเบฒเบ, เบ๏ฟฝเบฒเบฎเบฝเบ เปเบฅเบฐ เบญเบทเปเบเป เบ๏ฟฝเบงเบเบเบงเบฒเบกเบเบญเบเปเบ, เบง๏ฟฝเบญเบเปเบง เปเบเบเบ๏ฟฝเบฒเบเบฅเบฐเบเบปเบ Border Trade E-bank \n",
       "เปเบเบดเปเบเปเบเบฑเบเบฅเบฐเบเบปเบเบชเบฐเปเบเบฒเบฐ เบเบฒเบเปเบญเบเบชเบฐเบเบธเบเปเบเบดเบเบขเบงเบเปเบ เบชเบ เบเบตเบ. \n",
       "เบเบธเบเบเบฐเบชเบปเบ เบเบฒเบ\n",
       "เปเบญเบเปเบเบดเบเปเบเบ๏ฟฝเบฒเบ\n",
       "เบเบฐเปเบเบ \n",
       "๏ง เบชเบฒเบกเบฒเบเบ๏ฟฝเบฒเบฅเบฐเบ๏ฟฝเบฒเบชเบดเบเบ๏ฟฝเบฒ เปเบฅเบฐ เบ๏ฟฝเบฅเบดเบเบฒเบเบ๏ฟฝเบฒเบเป. \n",
       "๏ง เบช๏ฟฝเบฒเบฅเบฑเบเบฅเบนเบเบ๏ฟฝเบฒเบช๏ฟฝเบงเบเบเบธเบเบเบปเบเบ๏ฟฝเบฒเบเบชเบฒเบกเบฒเบเปเบญเบเปเบเบดเบเปเบเบ๏ฟฝเบฒเบเบเบฐเปเบเบเปเบเบทเปเบญเบ๏ฟฝเบฒเบฅเบฐ เบ๏ฟฝเบฒเบเบดเปเบเบเบปเบง,  เบเบฒเบเบชเบถเบเบชเบฒ, \n",
       "เบข๏ฟฝเบฝเบกเบขเบฒเบกเบ๏ฟฝเบฒเบเบเบฐเปเบเบ เบเบฒเบกเบฅเบฐเบเบฝเบเบเบฒเบเบเบตเปเบเบฐเบเบฒเบเปเบซ๏ฟฝเบ เบชเบเบเบฅเบฒเบง เบงเบฒเบเบญเบญเบเปเบ๏ฟฝเบฅเบฐเปเบฅเบเบฐ. \n",
       "เบเบปเบเบเบฐเปเบซเบเบ \n",
       "เบเบตเปเปเบ๏ฟฝเบฎเบฑเบ \n",
       "๏ง  เบชเบฒเบกเบฒเบเปเบญเบเปเบเบดเบเปเบเบเบธเบเบเบฐเบเบฒเบเบฒเบเบเบตเปเบขเบน๏ฟฝเปเบ เบช.เบ เบเบตเบ เบ๏ฟฝเบงเบเบเบงเบฒเบกเบงเบญ๏ฟฝเบเปเบง เปเบฅเบฐ เบเบญเบเปเบ. \n",
       "๏ง เบชเบฒเบกเบฒเบเปเบญเบเบญเบญเบเปเบ๏ฟฝเบชเบฐเปเบเบฒเบฐเบชเบฐเบเบธเบเปเบเบดเบเบขเบงเบ CNY)...\n",
       "\n",
       "            \n",
       "                **4.** `FINAL_2024.pdf` (เปเปเบฒ 69) - เบเบงเบฒเบกเบเปเบฒเบเบเบท: `-0.281`\n",
       "                > เบเบทเปเบกเบชเบฑเบเบฅเบงเบกเบเบฐเบฅเบดเบเบเบฐเบเบฑเบเบเบฑเบเปเบปเบเบเบญเบ เบเบเบเบฅ_2020_Update.2 เป๏ฟฝเบฒเบเบต 66 \n",
       "เปเบฒเบเปเบซเบ เบญเบฑเบเบเบฒเบเบญเบเปเบ๏ฟฝเบ เปเบฅเบฐ เบ๏ฟฝเบฒเบ๏ฟฝเบฒเบเบฝเบก เบญเบฒเบเบเบฐเบ๏ฟฝเบฝเบเบเบฒเบกเบเบฒเบเบเบฐเบเบฒเบเปเบ๏ฟฝเปเบเปเบ๏ฟฝเบฅเบฐเปเบฅเบเบฐ \n",
       " \n",
       "9.6 เปเบฑเบเบชเบทเบ๏ฟฝเปเบฒเบเบฐเบเบฑเบเบเบฒเบเบ๏ฟฝเบฒเบเปเบเบดเบ )Payment Guarantees) \n",
       "เปเบฑเบเบชเบทเบ๏ฟฝเปเบฒเบเบฐเบเบฑเบเบเบฒเบเบ๏ฟฝเบฒเบเปเบเบดเบ (Payment Guarantees) \n",
       "เบเบธเบเบฅเบฑเบเบชเบฐเบเบฐ ๏ง เปเบเบทเปเบญเบ๏ฟฝเปเบฒเบเบฐเบเบฑเบเบเบฑเบเบเบฐเบเบฒเบเบ๏ฟฝเบฒเบเปเบเบดเบเบเบญเบเบเบน๏ฟฝเบชเบฐเปเปเบต เบเบน๏ฟฝเบเบทเป). เบ๏ฟฝเบฅเบฐเบเบตเบเบน๏ฟฝเบชเบฐเปเปเบตเบ๏ฟฝเปเบ๏ฟฝเบฒเบฅเบฐเบชเบดเบเบ๏ฟฝเบฒ, เบเบน๏ฟฝเบฎเบฑเบเบเบปเบ\n",
       "เบเบฐเปเบซเบเบ เบเบน๏ฟฝเบเบฒเบ) เบเบดเปเบเบฎเบฝเบเบฎ๏ฟฝเบญเบเปเบซ๏ฟฝเบเบฐเบเบฒเบเบฒเบเบ๏ฟฝเบฒเบฅเบฐเบ๏ฟฝเบฒเบชเบดเบเบ๏ฟฝเบฒเปเบเบ. \n",
       "เบเบธเบเบชเบปเบกเบเบฑเบเบเบญเบ\n",
       "เบเบน๏ฟฝเบชเบฐเปเปเบต \n",
       "๏ง เบกเบตเบเบฑเบเบเบตเบ๏ฟฝเบฒ เบเบเบเบฅ \n",
       "๏ง เปเบเบเบฐเบเบฝเบเบงเบดเบชเบฒเบซเบฐเบเบดเบ  \n",
       "เปเบญเบเบฐเบชเบฒเบ\n",
       "เบเบฐเบเบญเบเบซเบผเบฑเบ \n",
       "๏ง เบชเบฑเบเบเบฒ \n",
       "๏ง เปเบญเบเบฐเบชเบฒเบเบญเบทเปเบเปเบเบฒเบกเปเบเบเบเบญเบก เบเบญเบ เบเบเบเบฅ \n",
       "เบงเบปเบเปเบเบดเบเปเบฑเบเบชเบทเบ๏ฟฝเปเบฒ\n",
       "เบเบฐเบเบฑเบ \n",
       "๏ง เบเบทเปเบเบเบฑเบเปเบ๏ฟฝเบฅเบฐเปเบเบเบเบฒเบ \n",
       "เบญเบฑเบเบเบฒเบเบญเบเปเบ๏ฟฝเบ \n",
       ")เบ๏ฟฝเปเบเบต  \n",
       "๏ง เบ๏ฟฝเปเบกเบต \n",
       "เบ๏ฟฝเบฒเบ๏ฟฝเบฒเบเบฝเบก ๏ง เบญเบตเบเบเบฒเบกเบเบฒเบเบเบฐเบเบฒเบเปเบ๏ฟฝเปเบเปเบ๏ฟฝเบฅเบฐเปเบฅเบเบฐ \n",
       "เปเบฅเบเบฐเปเบงเบฅเบฒ ๏ง เบญเบตเบเบเบฒเบกเปเบ๏ฟฝเบฅเบฐเปเบเบเบเบฒเบ \n",
       "เบซเบผเบฑเบเบเบฑเบเบ๏ฟฝเปเบฒเบเบฐเบเบฑเบ ๏ง เบเบฑเบเบเบตเปเบเบดเบเบเบฒเบเบเบฐเบขเบฑเบ, เบเบฑเบเบเบตเปเบเบดเบเบเบฒเบเบเบฐเปเบชเบฅเบฒเบเบงเบฑเบ, เบเบฑเบเบเบตเปเบเบดเบเบเบฒเบเบกเบตเบ๏ฟฝเบฒเบเบปเบ, เบงเบปเบเปเบเบดเบเบชเบดเบเปเบเบทเปเบญ, \n",
       "เบเบฐเบเบฒเบเบฒเบเบญเบทเปเบ. \n",
       "เปเบฒเบเปเบซเบ เบญเบฑเบเบเบฒเบเบญเบเปเบ๏ฟฝเบ เปเบฅเบฐ เบ๏ฟฝเบฒเบ๏ฟฝเบฒเบเบฝเบก เบญเบฒเบเบเบฐเบ๏ฟฝเบฝเบเบเบฒเบกเบเบฒเบเบเบฐเบเบฒเบเปเบ๏ฟฝเปเบเปเบ๏ฟฝเบฅเบฐเปเบฅเบเบฐ \n",
       " \n",
       "9.7 เปเบฑเบเบชเบทเบ๏ฟฝเปเบฒเบเบฐเบเบฑเบเปเบเบทเปเบญเปเบญเบปเบฒเบชเบดเบเบ๏ฟฝเบฒเบญเบญเบเบเบฒเบเบ๏ฟฝเบฒเปเบฎเบทเบญ )Trade Finance) \n",
       "เปเบฑเบเบชเบทเบ๏ฟฝเปเบฒเบเบฐเบเบฑเบเปเบเบทเปเบญเปเบญเบปเบฒเบชเบดเบเบ๏ฟฝเบฒเบญเบญเบเบเบฒเบเบ๏ฟฝเบฒเปเบฎเบทเบญ (Shipping Guarantee) \n",
       "เบเบธเบเบฅเบฑเบเบชเบฐเบเบฐ ๏ง เปเบเบทเปเบญเปเบเบทเปเบญเบ๏ฟฝเปเบฒเบเบฐเบเบฑเบเบเบฒเบเปเบ๏ฟฝเปเบเบเปเบญเบเบฐเบชเบฒเบเบเบปเบเบชเบปเปเบเบเบปเปเบเบชเบฐเบเบฑเบ Original Bill of Lading)  \n",
       "๏ง เปเบเบทเปเบญเบฎเบฑเบเบชเบดเบเบ๏ฟฝเบฒเบญเบญเบเบเบฒเบเบ๏ฟฝเบฒเปเบฎเบทเบญ เบเบฑเบเบเบตเปเบ๏ฟฝเปเปเบเบทเปเบญเปเบเบฑเบเบเบฒเบเบซ๏ฟฝเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบ๏ฟฝเบฒเบเปเบเบเบฒเบเปเบเบฑเบเบฎเบฑเบเบชเบฒเบชเบดเบเบ๏ฟฝเบฒเบขเบน๏ฟฝ\n",
       "เบ๏ฟฝเบฒเปเบฎเบทเบญ. \n",
       "เบเบธเบเบชเบปเบกเบเบฑเบเบเบญเบ\n",
       "เบเบน๏ฟฝเบชเบฐเปเปเบต \n",
       "๏ง เบกเบตเบเบฑเบเบเบตเบ๏ฟฝเบฒ เบเบเบเบฅ \n",
       "๏ง เปเบเบเบฐเบเบฝเบเบงเบดเบชเบฒเบซเบฐเบเบดเบ  \n",
       "เปเบญเบเบฐเบชเบฒเบ\n",
       "เบเบฐเบเบญเบเบซเบผเบฑเบ \n",
       "๏ง เปเบญเบเบฐเบชเบฒเบเบเบปเบเบชเบปเปเบ Bill of Lading)  \n",
       "๏ง เปเบญเบเบฐเบชเบฒเบเบญเบทเปเบเปเบเบฒเบกเปเบเบเบเบญเบก เบเบญเบ เบเบเบเบฅ \n",
       "เบงเบปเบเปเบเบดเบเปเบฑเบเบชเบทเบ๏ฟฝเปเบฒ\n",
       "เบเบฐเบเบฑเบ \n",
       "๏ง เบเบทเปเบเบเบฑเบเบกเบนเบเบ๏ฟฝเบฒเบชเบดเบเบ๏ฟฝเบฒ \n",
       "เบญเบฑเบเบเบฒเบเบญเบเปเบ๏ฟฝเบ \n",
       ")เบ๏ฟฝเปเบเบต  \n",
       "๏ง เบ๏ฟฝเปเบกเบต \n",
       "เบ๏ฟฝเบฒเบ๏ฟฝเบฒเบเบฝเบก ๏ง เบญเบตเบเบเบฒเบกเบเบฒเบเบเบฐเบเบฒเบเปเบ๏ฟฝเปเบเปเบ๏ฟฝเบฅเบฐเปเบฅเบเบฐ \n",
       "เปเบฅเบเบฐเปเบงเบฅเบฒ ๏ง เบญเบตเบเบเบฒเบกเปเบ๏ฟฝเบฅเบฐเปเบเบเบเบฒเบ \n",
       "เบซเบผเบฑเบเบเบฑเบเบ๏ฟฝเปเบฒเบเบฐเบเบฑเบ ๏ง เบเบฑเบเบเบตเปเบเบดเบเบเบฒเบเบเบฐเบขเบฑเบ, เบเบฑเบเบเบตเปเบเบดเบเบเบฒเบเบเบฐเปเบชเบฅเบฒเบเบงเบฑเบ, เบเบฑเบเบเบตเปเบเบดเบเบเบฒเบเบกเบตเบ๏ฟฝเบฒเบเบปเบ, เบงเบปเบเปเบเบดเบเบชเบดเบเปเบเบทเปเบญ, \n",
       "เบเบฐเบเบฒเบเบฒเบเบญเบทเปเบ. \n",
       "เปเบฒเบเปเบซเบ เบญเบฑเบเบเบฒเบเบญเบเปเบ๏ฟฝเบ เปเบฅเบฐ เบ๏ฟฝเบฒเบ๏ฟฝเบฒเบเบฝเบก เบญเบฒเบเบเบฐเบ๏ฟฝเบฝเบเบเบฒเบกเบเบฒเบเบเบฐเบเบฒเบเปเบ๏ฟฝเปเบเปเบ๏ฟฝเบฅเบฐเปเบฅเบเบฐ...\n",
       "\n",
       "            \n",
       "                **5.** `FINAL_2024.pdf` (เปเปเบฒ 85) - เบเบงเบฒเบกเบเปเบฒเบเบเบท: `-0.283`\n",
       "                > เบเบทเปเบกเบชเบฑเบเบฅเบงเบกเบเบฐเบฅเบดเบเบเบฐเบเบฑเบเบเบฑเบเปเบปเบเบเบญเบ เบเบเบเบฅ_2020_Update.2 เป๏ฟฝเบฒเบเบต 82 \n",
       "๏ง เบ๏ฟฝเบฒเบเบชเบฒเบกเบฒเบเบเบดเบเบเบฒเบกเบฅเบฒเบเบเบฒเบเปเบเบทเปเบญเบเปเบซเบง เปเบฅเบฐ เบเบงเบกเบเบธเบกเบเบฒเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบเบฑเบเปเบเบผเบเบดเบเบเบญเบเบ๏ฟฝเบฒเบเบ๏ฟฝเบฒเบ BCEL \n",
       "One, เปเบเบดเปเบเปเบเบฑเบเปเบเบทเปเบญเบเบกเบทเบเบตเปเบเบฐเบ๏ฟฝเบงเบเปเบซ๏ฟฝเบ๏ฟฝเบฒเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบเบฑเบเปเบเบผเบเบดเบเบเบญเบเบ๏ฟฝเบฒเบเปเบ๏ฟฝเบข๏ฟฝเบฒเบเบชเบฐเบเบงเบเบชเบฐเบเบฒเบ เปเบฅเบฐ เบเบญเบ\n",
       "เปเบเบเบดเปเบเบเบถเปเบ. เบชเบฒเบกเบฒเบเบ๏ฟฝเบฒเบฅเบฐเบ๏ฟฝเบฒเบเบซเบผเบฒเบเบ๏ฟฝเบญเบเบเบฒเบเปเบเบฑเปเบ: เบเบฒเบเบชเบฑเปเบเบเบทเปเบ๏ฟฝเบฒเบเปเบงเบฑเบเปเบ, เบญเบตเปเบกเบง เบซเบผเบท เปเบเบฅเบฐเบชเบฑเบ. \n",
       "เปเบเบทเปเบญเบเปเบ \n",
       "เบเบฒเบเบ๏ฟฝเบฒเปเบ๏ฟฝ \n",
       "1. เบเบธเบเบเบปเบเบชเบฑเบเบเบฒเบเบฅเบฒเบง เบซเบผเบท เบ๏ฟฝเบฒเบเบเบฐเปเบเบ \n",
       "2. เปเบเบฑเบเบเบธเบเบเบปเบเบเบตเปเบกเบตเบฅเบฒเบเบฎเบฑเบเปเบฑเปเบเบเบปเบ \n",
       "3. เบกเบตเบเบตเปเบขเบน๏ฟฝเปเบ๏ฟฝเบเบญเบ, เบ๏ฟฝเบฒเปเบเบฑเบเบเบปเบเบ๏ฟฝเบฒเบเบเบฐเปเบเบเบ๏ฟฝเบญเบเบกเบตเปเบเบญเบฐเบเบธเบเบฒเบ เบซเบผเบท เบเบฒเบเบ๏ฟฝเปเบฒเบเบฐเบเบฑเบเบเบฒเบเบญเบปเบเบเบฒเบเบเบตเปเบ๏ฟฝเบฝเบงเบ๏ฟฝเบญเบ \n",
       "4. เบกเบตเบงเบปเบเปเบเบดเบเบ๏ฟฝเปเบฒเบเบฐเบเบฑเบ เบช๏ฟฝเบฒเบฅเบฑเบเบเบฒเบเบ๏ฟฝเบฒเบ๏ฟฝเบงเบเปเบเบดเบเปเบเบฅเบฒเปเบก๏ฟฝเบเปเบ๏ฟฝเบงเบปเบเปเบเบดเบเปเบเบเบฑเบเบ๏ฟฝเปเปเบเบตเบ 85 % เบเบญเบเบงเบปเบเปเบเบดเบ   \n",
       "เบ๏ฟฝเปเบฒเบเบฐเบเบฑเบ, เบช๏ฟฝเบงเบเบเบฒเบเบ๏ฟฝเปเบฒเบเบฐเบเบฑเบเบ๏ฟฝเบงเบเบชเบฐเบเบธเบเบญเบทเปเบเบเบตเปเบ๏ฟฝเปเปเบก๏ฟฝเบเปเบเบฅเบฒ เปเบก๏ฟฝเบเปเบ๏ฟฝเบงเบปเบเปเบเบดเบเบ๏ฟฝเปเปเบเบตเบ 80% ) \n",
       "5. เบ๏ฟฝเบฒเบ๏ฟฝเปเบ๏ฟฝเบญเบเบเบฒเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบงเบปเบเปเบเบดเบเบ๏ฟฝเปเบฒเบเบฐเบเบฑเบ เปเบก๏ฟฝเบเบชเบฒเบกเบฒเบเบชเบฐเปเปเบตเบญเบญเบเบเบฑเบเปเบเบผเบเบดเบเบเบตเปเบ๏ฟฝเปเบกเบตเบซเบผเบฑเบเบเบฑเบเบ๏ฟฝเปเบฒเบเบฐเบเบฑเบ เปเบเบ\n",
       "เบชเบฐเปเปเบตเบ๏ฟฝเบฒเบเบเบฐเบเบฐเบ๏ฟฝเบฒเบกเบฐเบเบฒเบเบชเบดเบเปเบเบทเปเบญเบเบฒเบกเบฅเบฐเบเบฝเบเบเบฒเบ \n",
       "6. เบกเบตเบเบงเบฒเบกเบชเบฒเบกเบฒเบเบเบฒเบเบ๏ฟฝเบฒเบเบเบฒเบเบเบฐเบเบถเบเบ๏ฟฝเปเป๏ฟฝเบฒเบเบปเบเปเบฒเบเปเบเบเบฒเบเบเบฐเบเบดเบเบฑเบเบชเบฑเบเบเบฒ เปเบฅเบฐ เปเบเบทเปเบญเบเปเบเบ๏ฟฝเบฒเบเป \n",
       "7. เบ๏ฟฝเบญเบเบกเบตเบเบฑเบเบเบตเบเบฑเบ เบเบฐเบเบฒเบเบฒเบเบเบฒเบเบ๏ฟฝเบฒเบ๏ฟฝเบฒเบเบเบฐเปเบเบเบฅเบฒเบง เบกเบฐเบซเบฒเบเบปเบ เบข๏ฟฝเบฒเบเป๏ฟฝเบญเบ 1 เบเบฑเบเบเบต, เบกเบตเบเบฑเบเบเบฐเบ๏ฟฝเบฒเบเบปเบง เบซเบผเบท \n",
       "เปเบฑเบเบชเบทเปเบเบตเบเบเบฒเบ passport) เบเบตเปเบ๏ฟฝเปเปเบปเบเบญเบฒเบเบธ เบซเบผเบท เบเบทเปเบกเบช๏ฟฝเบฒเบกเบฐเปเบเบเบปเบง \n",
       "เปเบฒเบเปเบซเบ \n",
       "๏ง เบญเบตเบเบเบฒเบกเบฅเบฐเบเบฝเบเบเบญเบ เบเบเบเบฅ, เปเบ๏ฟฝเบฅเบฐเบเบฑเบเบกเบตเบเบฒเบเบ๏ฟฝเบฒเบเบปเบเบงเบปเบเปเบเบดเบเบเบฒเบเบเบญเบเปเบเบดเบเบชเบปเบ เปเบฅเบฐ เบเบฒเบเปเบ๏ฟฝเบ๏ฟฝเบฒเบเบ๏ฟฝเปเบเบฑเปเบ\n",
       "เบ๏ฟฝเปเบงเบฑเบ. เบเบฒเบเบฐเบฅเบฒเบเบ๏ฟฝเบฝเบงเบเบฑเบเบงเบปเบเปเบเบดเบเบเบฒเบเบงเบปเบเปเบเบดเบเบเบฒเบเบเบญเบ, เบเบฒเบเปเบ๏ฟฝเบ๏ฟฝเบฒเบ เปเบฅเบฐ เบ๏ฟฝเบฒเปเบเบฐเบ๏ฟฝเบฒเบเบฒเบเบ๏ฟฝเบฒเปเบ๏ฟฝ เปเบก๏ฟฝเบ\n",
       "เบฅเบฐเบเบธเบฅเบฐเบญเบฝเบเบขเบน๏ฟฝเปเบเปเบเปเบเบทเปเบญเบเปเบเบเบตเปเบเบฒเบเบเบฐเบเบฒเบเบฒเบเบกเบญเบเปเบซ๏ฟฝเปเบ๏ฟฝเบ๏ฟฝเบฒเบเปเบเบทเปเบญเปเบเบฑเบเบ๏ฟฝเบญเบเบญเบตเบเปเบเบเบฒเบเบ๏ฟฝเบฒเปเบ๏ฟฝ. \n",
       "เบชเบฐเบเบฑเปเบ, เบเบน๏ฟฝเบเบทเบเบฑเบเบเบงเบเบชเบถเบเบชเบฒเปเบเบทเปเบญเบเปเบเปเบซ๏ฟฝเบฅเบฐเบญเบฝเบเปเบเบทเปเบญเบเบงเบฒเบกเปเบเบปเปเบฒเปเบ เปเบฅเบฐ เบชเบฐเบเบงเบเปเบเบเบฒเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบเบฑเบ \n",
       "๏ง เปเบ๏ฟฝเบฅเบฐเบเบฐเปเบเบเบเบฑเบ เบชเบฒเบกเบฒเบเบเบญเบเปเบเบดเบเบชเบปเบ เบซเบผเบท เบชเบฒเบกเบฒเบเปเบ๏ฟฝเบ๏ฟฝเบฒเบเปเบ๏ฟฝเบเบฑเบเปเบปเบ 100% เบเบญเบเบงเบปเบเปเบเบดเบเบชเบดเบเปเบเบทเปเบญ.   \n",
       "เบชเบดเบเบเบดเบเบฐเปเบซเบเบ \n",
       "1. เบเบฃเบตเบเบฐเบเบฑเบเปเบเบ๏ฟฝเบญเบเบ๏ฟฝเบฝเบงเปเบเปเบงเบฅเบฒเปเบเบตเบเบเบฒเบเปเบเบ๏ฟฝเบฒเบเบเบฐเปเบเบ Oversea Travel Insurance) \n",
       "เบเบธเบเบเบฑเปเปเบเบเบตเปเบ๏ฟฝเบฒเบเปเบเบตเบเบเบฒเบเปเบเบ๏ฟฝเบฒเบเบเบฐเปเบเบ เปเบฅเบฐ เบ๏ฟฝเบฒเปเบ๏ฟฝเบเบฑเบ BCEL World Mastercard เบเบญเบเบ๏ฟฝเบฒเบเบ๏ฟฝเบฒเบฅเบฐ\n",
       "เบ๏ฟฝเบฒเปเบเบตเบเบเบฒเบ เบเบตเปเบเบปเบ) , เบ๏ฟฝเบฒเบเบเบฐเปเบ๏ฟฝเบฎเบฑเบเบเบฒเบเบเบธ๏ฟฝเบกเบเบญเบเบเบฒเบเปเบเบเบเบฐเบเบฑเบเปเบเบ๏ฟฝเบญเบเบ๏ฟฝเบฝเบง เปเบเบเบฐเบซเบผเบญเบเบเบฒเบ\n",
       "เปเบเบตเบเบเบฒเบ เปเบเบฑเบเบเบปเปเบ:  \n",
       "โข เบ๏ฟฝเบฒเปเบ๏ฟฝเบ๏ฟฝเบฒเบเบเบดเปเบเบเบปเบงเบชเบธเบเปเบชเบตเบ     เบชเบนเบเบชเบธเบ 100.000 เปเบเบฅเบฒ \n",
       "โข เบ๏ฟฝเบฒเปเบ๏ฟฝเบ๏ฟฝเบฒเบเปเบเบเบฒเบเบญเบปเบเบเบฐเบเบปเบ/เบเบฒเบเบ๏ฟฝเบฒเบเบเบฑเบเบเบทเบเบเบดเปเบ เบชเบนเบเบชเบธเบ 1.000.000 เปเบเบฅเบฒ \n",
       "โข เบ๏ฟฝเบฒเบญเบธเบเบฑเบเบเบดเปเบซเบเบเบธเบเบเบปเบ-เบเบฐเบซเบผเบญเบเบเบฒเบเปเบเบตเบเบเบฒเบ เบชเบนเบเบชเบธเบ 85.000 เปเบเบฅเบฒ \n",
       "โข เบ๏ฟฝเบฒเปเบเบปเปเบฒเบเบฑเบเบฎเบฑเบเบชเบฒเบเบตเปเปเบฎเบเป๏ฟฝ    50 เปเบเบฅเบฒ/เบงเบฑเบ เบชเบนเบเบชเบธเบเปเบเบดเบ 30 เบงเบฑเบ \n",
       "โข เบ๏ฟฝเบฒเบเบปเบเปเบเบตเบเบ๏ฟฝเบฅเบฐเบเบตเบเบฐเปเบเบปเบฒเปเบเบตเบเบเบฒเบเปเบชเบเบซเบฒเบ  เบชเบนเบเบชเบธเบ 1.500 เปเบเบฅเบฒ \n",
       "โข เบ๏ฟฝเบฒเบเบปเบเปเบเบตเบเบ๏ฟฝเบฅเบฐเบเบตเบ๏ฟฝเบฝเบงเบเบดเบเบฅ๏ฟฝเบฒเบ๏ฟฝเบฒ   75 เปเบเบฅเบฒ/เบเบปเปเบงเปเบกเบ \n",
       "โข เบเบฐเปเบเบปเบฒเปเบเบตเบเบเบฒเบเบฅ๏ฟฝเบฒเบ๏ฟฝเบฒ    75 เปเบเบฅเบฒ/เบเบปเปเบงเปเบกเบ \n",
       "เปเบฅเบฐ เบฅเบฒเบเบฅเบฐเบญเบฝเบเบญเบทเปเบเปเบชเบฒเบกเบฒเบเปเบเบดเปเบเปเบเบตเปเบกเปเบเบตเบกเปเบ๏ฟฝเบเบฒเบเปเบเบทเปเบญเบเปเบเบเบฒเบเบเบธ๏ฟฝเบกเบเบญเบเบเบฐเบเบฑเบเปเบ.  \n",
       "โข เบเบธ๏ฟฝเบกเบเบญเบเบ๏ฟฝเบฅเบฐเบเบตเบเบตเปเบ๏ฟฝเบฒเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบเบฑเบเบ๏ฟฝเบฒเบฅเบฐเบ๏ฟฝเบฒเบชเบดเบเบ๏ฟฝเบฒ เปเบ๏ฟฝเบ๏ฟฝเปเปเบ๏ฟฝเบฎเบฑเบเปเบเบทเปเบญเบ เบซเบผเบท เปเบเบทเปเบญเบเบเบตเปเปเบ๏ฟฝเบฎเบฑเบเบ๏ฟฝเปเบเบทเบเบ๏ฟฝเบญเบ\n",
       "เบเบฒเบกเบฅเบฒเบเบเบฒเบเบเบตเปเบ๏ฟฝเบญเบเบเบฒเบ เบซเบผเบท เบ๏ฟฝเบฒเปเบ๏ฟฝเบ๏ฟฝเปเปเบ๏ฟฝเปเบเบทเปเบญเบเบเบฒเบเปเบ๏ฟฝเปเบเปเบชเบเบซเบฒเบ เบชเบนเบเบชเบธเบ 1.500 เปเบเบฅเบฒ. \n",
       " \n",
       "2. เบเบฒเบเบฅเบปเบเบเบฐเบเบฝเบเบ๏ฟฝเบฒเปเบ๏ฟฝเบซ๏ฟฝเบญเบเบเบฑเบเบฎเบฑเบเบฎเบญเบ LoungeKey \n",
       "BCEL World Mastercard เปเบ๏ฟฝเบฎเบฑเบเบชเบดเบเปเบเบปเปเบฒเบซ๏ฟฝเบญเบเบเบฑเบเบฎเบฑเบเบฎเบญเบเบเบญเบเบชเบฐเปเบฒเบกเบเบดเบเบเบฑเปเบเบ๏ฟฝเบฒเบ๏ฟฝเบฒเบเปเบเบฃเบด 2 เบเบฑเปเบ/\n",
       "เบเบต. เปเบ๏ฟฝเบ๏ฟฝเบญเบเปเบเบปเปเปเบฒเบ๏ฟฝเบฒเปเบ๏ฟฝเบ๏ฟฝเบฅเบดเบเบฒเบเบซ๏ฟฝเบญเบเบเบฑเบเบฎเบฑเบเบฎเบญเบเบเบฑเปเบเบ๏ฟฝเบฒเบงเบเบฑเปเบ, เบ๏ฟฝเบฒเบเบ๏ฟฝเบญเบเบฅเบปเบเบเบฐเบเบฝเบเบเบฒเบกเบเบฑเปเบเบเบญเบเบเบฑเปเบเบฅเบธ๏ฟฝเบกเบเบตเป:  \n",
       "โข เบเบฒเบงเปเบซเบผเบ Mastercard Airport Experience App เบขเบน๏ฟฝ App Store เบซเบผเบท Play Store เปเบฅเบฐ \n",
       "เบฅเบปเบเบเบฐเบเบฝเบเบ๏ฟฝเบฒเบ App. \n",
       "โข เบ๏ฟฝเบญเบเปเบฅเบเบเบฑเบ  เปเบฅเบฐ เบญเบฑเบเบชเบญเบเปเบฅเบเบเบญเบกเปเบเบทเปเบญเบขเบทเบเบขเบฑเบ: เปเบเบทเปเบญเบเบฒเบเบขเบทเบเบขเบฑเบ เปเบฅเบฐ เบเบงเบเบชเบญเบ, เบ๏ฟฝเบฒเบ\n",
       "เบ๏ฟฝเบญเบเปเบเบตเบเบเบฑเปเบเบเบฑเปเบเบญเบญเบเบฅเบฒเบ เปเบเบทเปเบญเปเบซ๏ฟฝเบฅเบฐเบเบปเบเปเบฎเบฑเบเบฅเบฒเบเบเบฒเบเบ๏ฟฝเบญเบเปเบเบดเบเปเบง๏ฟฝเบเบปเปเบงเบเบฒเบง 1 เปเบเบฅเบฒ, เปเบ๏ฟฝ\n",
       "เบฅเบฒเบเบเบฒเบเบเบฑเปเบเบ๏ฟฝเบฒเบงเบเบตเปเบเบฐเบ๏ฟฝเปเบเบทเบเบฎเบฝเบเปเบเบฑเบเปเบเบปเปเบฒเบเบฑเบเบเบญเบเบ๏ฟฝเบฒเบ เปเบฅเบฐ เบเบฐเบเบทเบเบเบปเบเบฅ๏ฟฝเบญเบเบเบฒเบเปเบ 10 เบงเบฑเบ. \n",
       "โข เบ๏ฟฝเบญเบเบ๏ฟฝเปเบกเบนเบเบเบฑเบ เปเบฅเบฐ เบเบฑเปเบ User: เบ๏ฟฝเบญเบเบ๏ฟฝเปเบกเบนเบเบเบฑเบเปเบซ๏ฟฝเบเบทเบเบ๏ฟฝเบญเบ เปเบเบดเปเบเบ๏ฟฝเบญเบเปเบเบตเบเบเบฑเปเบเบเบฑเปเบเบญเบญเบเบฅเบฒเบเปเบง๏ฟฝ\n",
       "เบ๏ฟฝเบญเบ 1 เบเบฑเปเบ). เบซเบผเบฑเบเบเบฒเบเบเบฑเปเปเบเบเบปเบ Verify . เบเบฒเบเบเบฑเปเบ. เบ๏ฟฝเบญเบเบ๏ฟฝเปเบกเบนเบเบช๏ฟฝเบงเบเบเบปเบง, เบเบทเป เปเบฅเบฐ เบเบฒเบกเบชเบฐเบเบธเบ, เปเบเบต...\n",
       "\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## ๐ฌ เปเปเบ Interactive - เบเบดเบกเบเบณเบเบฒเบกเบเบญเบเบเปเบฒเบ (เบเบดเบก 'quit' เปเบเบทเปเบญเบญเบญเบ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๐ เบเบญเบเปเบเบเบตเปเปเบเปเบฅเบฐเบเบปเบ RAG!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
