{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "from groq import Groq\n",
    "from IPython.display import Image, display, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    @staticmethod\n",
    "    def load_docs(file_paths: List[str]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        ‡ªÇ‡∫´‡∫•‡∫î PDF documents ‡ªÇ‡∫î‡∫ç‡ªÉ‡∫ä‡ªâ LangChain PyPDFLoader ‡ªÄ‡∫û‡∫≤‡∫∞‡∫ç‡∫±‡∫á‡ªÅ‡∫≠‡∫±‡∫î‡∫à‡∫±‡∫á‡∫°‡∫±‡∫Å‡ªÄ‡∫û‡∫≤‡∫∞‡∫°‡∫±‡∫ô‡∫™‡ªâ‡∫≤‡∫á Metadata ‡ªÉ‡∫´‡ªâ Auto \n",
    "\n",
    "        Metadata ‡∫Ñ‡∫∑‡∫ç‡∫±‡∫á ? \n",
    "        Metadata ‡∫Ñ‡∫∑‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡ªÄ‡∫û‡∫µ‡ªà‡∫°‡ªÄ‡∫ï‡∫µ‡∫°‡∫Å‡ªà‡∫Ω‡∫ß‡∫Å‡∫±‡∫ö‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô\n",
    "        ‡ªÅ‡∫ï‡ªà‡∫•‡∫∞ Document ‡∫à‡∫∞‡∫°‡∫µ 2 ‡∫™‡ªà‡∫ß‡∫ô‡∫´‡∫º‡∫±‡∫Å:\n",
    "        1. page_content: ‡ªÄ‡∫ô‡∫∑‡ªâ‡∫≠‡ªÉ‡∫ô‡∫Ç‡ªç‡ªâ‡∫Ñ‡∫ß‡∫≤‡∫°‡∫à‡∫¥‡∫á‡ªÜ\n",
    "        2. metadata: ‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫•‡∫≤‡∫ç‡∫•‡∫∞‡∫≠‡∫Ω‡∫î‡∫Å‡ªà‡∫Ω‡∫ß‡∫Å‡∫±‡∫ö‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫ö‡∫ª‡∫á‡∫ö‡∫≠‡∫Å‡∫ß‡ªà‡∫≤ ‡ªÄ‡∫ß‡∫•‡∫≤‡ªÄ‡∫Æ‡∫ª‡∫≤ ‡∫Ñ‡∫ª‡ªâ‡∫ô‡∫´‡∫≤‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô ‡ªÅ‡∫´‡∫º‡ªà‡∫á‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫ô‡∫±‡ªâ‡∫ô‡∫°‡∫≤‡∫à‡∫≤‡∫Å‡ªÑ‡∫™\n",
    "        \n",
    "        Args:\n",
    "            file_paths (list): List of PDF file paths\n",
    "        \n",
    "        Returns:\n",
    "            List[Document]: List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        \n",
    "        all_docs = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Warning: File {file_path} not found. Skipping...\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                file_extension = os.path.splitext(file_path)[1].lower()\n",
    "                \n",
    "                # Check if file is PDF\n",
    "                if file_extension != '.pdf':\n",
    "                    print(f\"Warning: {file_path} is not a PDF file. Skipping...\")\n",
    "                    continue\n",
    "                \n",
    "                # Load PDF using LangChain PyPDFLoader\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                documents = loader.load()\n",
    "                \n",
    "                # Add enhanced metadata to all documents\n",
    "                for doc in documents:\n",
    "                    if doc.metadata is None:\n",
    "                        doc.metadata = {}\n",
    "                        \n",
    "                    doc.metadata.update({\n",
    "                        'source_file': os.path.basename(file_path),\n",
    "                        'file_type': file_extension,\n",
    "                        'file_path': file_path,\n",
    "                        'file_size': os.path.getsize(file_path) if os.path.exists(file_path) else 0,\n",
    "                    })\n",
    "                \n",
    "                all_docs.extend(documents)\n",
    "                print(f\"‚úÖ Processed PDF: {file_path} ({len(documents)} pages)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing {file_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"üìö Total PDF documents loaded: {len(all_docs)}\")\n",
    "        return all_docs\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_documents_standard(\n",
    "        docs: List[Document], \n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        tokenizer_model: str = \"D:/model/BAAI-bge-m3\",\n",
    "        max_token_limit: int = 8192\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        ‡ªÉ‡∫ä‡ªâ Lanchain ‡ªÉ‡∫ô‡∫Å‡∫≤‡∫ô‡ªÄ‡∫Æ‡∫±‡∫î chunking ‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫Å‡∫≤‡∫ô‡∫Ñ‡∫ª‡ªâ‡∫ô‡∫´‡∫≤‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫î‡ªâ‡∫ß‡∫ç ChromaDB  \n",
    "\n",
    "        Chunk_size: ‡ªÅ‡∫°‡ªà‡∫ô‡∫à‡∫≥‡∫ô‡∫ß‡∫ô‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫ó‡∫µ‡ªà‡∫à‡∫∞‡ªÄ‡∫Æ‡∫±‡∫î chunking ‡∫ï‡ªç‡ªà‡∫´‡∫ô‡ªà‡∫ß‡∫ç ‡ªÄ‡∫û‡∫≤‡∫∞‡∫ç‡∫±‡∫á ‡ªÄ‡∫Æ‡∫ª‡∫≤‡∫ö‡ªç‡ªà‡∫™‡∫≤‡∫°‡∫≤‡∫î‡ªÄ‡∫≠‡∫ª‡∫≤‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫ó‡∫±‡ªâ‡∫á‡ªù‡∫ª‡∫î‡ªÉ‡∫´‡ªâ AI ‡∫ï‡∫≠‡∫ö‡ªÑ‡∫î‡ªâ ‡ªÄ‡∫ô‡∫∑‡ªà‡∫≠‡∫á‡∫à‡∫≤‡∫Å‡∫ö‡∫≤‡∫á‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫°‡∫µ‡∫´‡∫•‡∫≤‡∫ç‡∫´‡∫ô‡ªâ‡∫≤\n",
    "        Chunk_overlap: ‡ªÅ‡∫°‡ªà‡∫ô‡∫à‡∫≥‡∫ô‡∫ß‡∫ô‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫ó‡∫µ‡ªà‡∫à‡∫∞‡ªÄ‡∫Æ‡∫±‡∫î chunking ‡∫ï‡ªç‡ªà‡∫´‡∫ô‡ªà‡∫ß‡∫ç ‡ªÄ‡∫û‡∫≤‡∫∞‡∫ç‡∫±‡∫á ‡ªÄ‡∫Æ‡∫ª‡∫≤‡∫ö‡ªç‡ªà‡∫™‡∫≤‡∫°‡∫≤‡∫î‡ªÄ‡∫≠‡∫ª‡∫≤‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫ó‡∫±‡ªâ‡∫á‡ªù‡∫ª‡∫î‡ªÉ‡∫´‡ªâ AI ‡∫ï‡∫≠‡∫ö‡ªÑ‡∫î‡ªâ ‡ªÄ‡∫ô‡∫∑‡ªà‡∫≠‡∫á‡∫à‡∫≤‡∫Å‡∫ö‡∫≤‡∫á‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫°‡∫µ‡∫´‡∫•‡∫≤‡∫ç‡∫´‡∫ô‡ªâ‡∫≤\n",
    "        Tokenizer_model: ‡ªÅ‡∫°‡ªà‡∫ô Model ‡∫ó‡∫µ‡ªà‡ªÄ‡∫Æ‡∫ª‡∫≤‡∫à‡∫∞‡ªÉ‡∫ä‡ªâ‡ªÉ‡∫ô‡∫Å‡∫≤‡∫ô‡ªÄ‡∫Æ‡∫±‡∫î chunking ‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫Å‡∫≤‡∫ô‡∫Ñ‡∫ª‡ªâ‡∫ô‡∫´‡∫≤‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫î‡ªâ‡∫ß‡∫ç ChromaDB\n",
    "        Max_token_limit: ‡ªÅ‡∫°‡ªà‡∫ô‡∫Å‡∫≤‡∫ô‡ªÅ‡∫ö‡ªà‡∫á‡∫™‡∫±‡∫î‡∫™‡ªà‡∫ß‡∫ô‡ªÉ‡∫´‡ªâ‡ªÄ‡∫´‡∫°‡∫≤‡∫∞‡∫™‡∫ª‡∫°‡∫Å‡∫±‡∫ö chunk_size\n",
    "        \n",
    "        Args:\n",
    "            docs: List of LangChain Document objects\n",
    "            chunk_size: Target size for each chunk in tokens\n",
    "            chunk_overlap: Number of overlapping tokens between chunks\n",
    "            tokenizer_model: Path to tokenizer model\n",
    "            max_token_limit: Maximum tokens allowed\n",
    "            \n",
    "        Returns:\n",
    "            List of chunked LangChain Document objects\n",
    "        \"\"\"\n",
    "        \n",
    "        if not docs:\n",
    "            print(\"‚ö†Ô∏è  No documents provided for chunking\")\n",
    "            return []\n",
    "        \n",
    "        # Load tokenizer\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "            print(f\"‚úÖ Loaded tokenizer: {tokenizer_model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading tokenizer: {e}\") \n",
    "        \n",
    "        # Validate parameters\n",
    "        if chunk_size >= max_token_limit:\n",
    "            chunk_size = max_token_limit - 500  # Safe buffer\n",
    "            print(f\"‚ö†Ô∏è  Adjusted chunk_size to {chunk_size} for safety\")\n",
    "        \n",
    "        if chunk_overlap >= chunk_size:\n",
    "            chunk_overlap = chunk_size // 5  # 20% overlap\n",
    "            print(f\"‚ö†Ô∏è  Adjusted chunk_overlap to {chunk_overlap}\")\n",
    "        \n",
    "        # Create tokenizer-aware text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "            tokenizer=tokenizer,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            strip_whitespace=True,\n",
    "            separators=[\n",
    "                \"\\n\\n\",      # Paragraph breaks\n",
    "                \"\\n\",        # Line breaks\n",
    "                \". \",        # Sentence endings\n",
    "                \"! \",        # Exclamation endings  \n",
    "                \"? \",        # Question endings\n",
    "                \"; \",        # Semicolon breaks\n",
    "                \", \",        # Comma breaks\n",
    "                \" \",         # Word breaks\n",
    "                \"\"           # Character level\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Split documents\n",
    "        print(f\"üîÑ Chunking {len(docs)} documents...\")\n",
    "        chunked_docs = text_splitter.split_documents(docs)\n",
    "        \n",
    "        # Validate token counts and add metadata\n",
    "        validated_chunks = []\n",
    "        max_tokens_found = 0\n",
    "        \n",
    "        for i, chunk in enumerate(chunked_docs):\n",
    "            # Count actual tokens\n",
    "            token_count = len(tokenizer.encode(chunk.page_content))\n",
    "            max_tokens_found = max(max_tokens_found, token_count)\n",
    "            \n",
    "            # Add chunk metadata\n",
    "            if chunk.metadata is None:\n",
    "                chunk.metadata = {}\n",
    "                \n",
    "            chunk.metadata.update({\n",
    "                'chunk_id': i,\n",
    "                'token_count': token_count,\n",
    "                'char_count': len(chunk.page_content),\n",
    "                'chunk_method': 'tokenizer_based'\n",
    "            })\n",
    "            \n",
    "            # Skip if too large\n",
    "            if token_count > max_token_limit:\n",
    "                print(f\"‚ö†Ô∏è  Skipping oversized chunk {i}: {token_count} tokens\")\n",
    "                continue\n",
    "                \n",
    "            validated_chunks.append(chunk)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"‚úÖ Created {len(validated_chunks)} chunks\")\n",
    "        print(f\"üìä Max tokens in any chunk: {max_tokens_found}\")\n",
    "        \n",
    "        return validated_chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_vector_store(\n",
    "        chunked_docs: List[Document],\n",
    "        embedding_model: str = \"D:/model/BAAI-bge-m3\",\n",
    "        collection_name: str = \"pdf_documents\",\n",
    "        persist_directory: str = \"./chroma_db\",\n",
    "        batch_size: int = 32\n",
    "    ) -> Chroma:\n",
    "        \"\"\"\n",
    "        ‡∫™‡ªâ‡∫≤‡∫á Vector Store ‡∫î‡ªâ‡∫ß‡∫ç ChromaDB ‡∫à‡∫≤‡∫Å chunked documents\n",
    "        \n",
    "        Embedding_model: ‡ªÅ‡∫°‡ªà‡∫ô Model ‡∫ó‡∫µ‡ªà‡ªÉ‡∫ä‡ªâ‡ªÉ‡∫ô‡∫Å‡∫≤‡∫ô‡ªÄ‡∫Æ‡∫±‡∫î Embedding ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫õ‡ªà‡∫Ω‡∫ô‡∫Ç‡ªç‡ªâ‡∫Ñ‡∫ß‡∫≤‡∫°‡ªÄ‡∫õ‡∫±‡∫ô Vector\n",
    "        Collection_name: ‡ªÅ‡∫°‡ªà‡∫ô‡∫ä‡∫∑‡ªà‡∫Ç‡∫≠‡∫á Collection ‡ªÉ‡∫ô ChromaDB ‡∫™‡∫≤‡∫°‡∫≤‡∫î‡∫™‡ªâ‡∫≤‡∫á‡∫ï‡∫≤‡∫°‡ªÉ‡∫à ‡∫ó‡∫µ‡ªà‡∫ï‡ªâ‡∫≠‡∫á‡∫Å‡∫≤‡∫ô ‡ªÅ‡∫ô‡∫∞‡∫ô‡∫≥‡ªÉ‡∫´‡ªâ‡∫™‡ªâ‡∫≤‡∫á‡ªÄ‡∫õ‡∫±‡∫ô Folder ‡∫Ç‡∫≠‡∫á‡ªÉ‡∫ú‡∫°‡∫±‡∫ô ‡ªÅ‡∫•‡∫∞ point ‡ªÑ‡∫õ Folder ‡∫ô‡∫±‡ªà‡∫ô ‡ªÄ‡∫û‡∫≤‡∫∞‡∫ß‡ªà‡∫≤ ChromaDB ‡ªÄ‡∫ß‡∫•‡∫≤‡∫°‡∫±‡∫ô‡∫ö‡∫±‡∫ô‡∫ó‡∫∑‡∫Å‡∫°‡∫±‡∫ô‡∫à‡∫∞‡∫ö‡∫±‡∫ô‡∫ó‡∫∑‡∫Å unique key ‡ªÄ‡∫ä‡∫µ‡ªà‡∫á‡∫°‡∫±‡∫ô‡∫à‡∫∞‡ªÄ‡∫Æ‡∫±‡∫î‡ªÉ‡∫´‡ªâ‡ªÄ‡∫Æ‡∫ª‡∫≤‡∫à‡∫≥‡ªÅ‡∫ô‡∫Å‡∫¢‡∫≤‡∫Å\n",
    "        Persist_directory: ‡ªÅ‡∫°‡ªà‡∫ô‡ªÇ‡∫ü‡∫•‡ªÄ‡∫î‡∫µ‡∫¢‡∫∏‡ªâ‡∫ö‡∫±‡∫ô‡∫ó‡∫∂‡∫Å ChromaDB\n",
    "        Batch_size: ‡ªÅ‡∫°‡ªà‡∫ô‡∫à‡∫≥‡∫ô‡∫ß‡∫ô chunks ‡∫ó‡∫µ‡ªà‡ªÄ‡∫Æ‡∫±‡∫î embedding ‡∫ï‡ªç‡ªà‡∫Ñ‡∫±‡ªâ‡∫á ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫õ‡ªâ‡∫≠‡∫á‡∫Å‡∫±‡∫ô memory overflow\n",
    "        \n",
    "        Args:\n",
    "            chunked_docs: List of chunked Document objects\n",
    "            embedding_model: Path to embedding model\n",
    "            collection_name: Name for ChromaDB collection\n",
    "            persist_directory: Directory to save ChromaDB\n",
    "            batch_size: Number of documents to process at once\n",
    "            \n",
    "        Returns:\n",
    "            Chroma vector store object\n",
    "        \"\"\"\n",
    "        \n",
    "        if not chunked_docs:\n",
    "            print(\"‚ö†Ô∏è  No chunked documents provided\")\n",
    "            return None\n",
    "        \n",
    "        # Create embeddings\n",
    "        try:\n",
    "            print(f\"üîÑ Loading embedding model: {embedding_model}\")\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=embedding_model,\n",
    "                model_kwargs={'device': 'cpu'},  # ‡∫õ‡ªà‡∫Ω‡∫ô‡ªÄ‡∫õ‡∫±‡∫ô 'cuda' ‡∫ñ‡ªâ‡∫≤‡∫°‡∫µ GPU\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "            print(f\"‚úÖ Loaded embedding model successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading embedding model: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Create ChromaDB client and collection\n",
    "        try:\n",
    "            # ‡∫™‡ªâ‡∫≤‡∫á‡ªÇ‡∫ü‡∫•‡ªÄ‡∫î‡∫µ‡∫ñ‡ªâ‡∫≤‡∫ç‡∫±‡∫á‡∫ö‡ªç‡ªà‡∫°‡∫µ\n",
    "            os.makedirs(persist_directory, exist_ok=True)\n",
    "            \n",
    "            print(f\"üîÑ Creating ChromaDB collection: {collection_name}\")\n",
    "            \n",
    "            # ‡∫•‡∫∑‡∫ö collection ‡ªÄ‡∫Å‡∫ª‡ªà‡∫≤‡∫ñ‡ªâ‡∫≤‡∫°‡∫µ (‡∫õ‡ªâ‡∫≠‡∫á‡∫Å‡∫±‡∫ô‡∫Ç‡ªç‡ªâ‡∫ú‡∫¥‡∫î‡∫û‡∫≤‡∫î)\n",
    "            try:\n",
    "                client = chromadb.PersistentClient(path=persist_directory)\n",
    "                try:\n",
    "                    client.delete_collection(collection_name)\n",
    "                    print(f\"üóëÔ∏è  Deleted existing collection: {collection_name}\")\n",
    "                except:\n",
    "                    pass  # Collection ‡∫ö‡ªç‡ªà‡∫°‡∫µ‡∫¢‡∫π‡ªà‡ªÅ‡∫•‡ªâ‡∫ß\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Warning during cleanup: {e}\")\n",
    "            \n",
    "            # ‡∫™‡ªâ‡∫≤‡∫á vector store ‡ªÅ‡∫ö‡∫ö batch\n",
    "            print(f\"üîÑ Processing {len(chunked_docs)} documents in batches of {batch_size}\")\n",
    "            \n",
    "            vector_store = None\n",
    "            total_processed = 0\n",
    "            \n",
    "            for i in range(0, len(chunked_docs), batch_size):\n",
    "                batch = chunked_docs[i:i + batch_size]\n",
    "                batch_num = (i // batch_size) + 1\n",
    "                total_batches = (len(chunked_docs) + batch_size - 1) // batch_size\n",
    "                \n",
    "                print(f\"üì¶ Processing batch {batch_num}/{total_batches} ({len(batch)} documents)\")\n",
    "                \n",
    "                try:\n",
    "                    if vector_store is None:\n",
    "                        # ‡∫™‡ªâ‡∫≤‡∫á vector store ‡∫ó‡∫≥‡∫≠‡∫¥‡∫î\n",
    "                        vector_store = Chroma.from_documents(\n",
    "                            documents=batch,\n",
    "                            embedding=embeddings,\n",
    "                            collection_name=collection_name,\n",
    "                            persist_directory=persist_directory\n",
    "                        )\n",
    "                    else:\n",
    "                        # ‡ªÄ‡∫û‡∫µ‡ªà‡∫° documents ‡ªÉ‡ªù‡ªà‡ªÄ‡∫Ç‡∫ª‡ªâ‡∫≤‡ªÑ‡∫õ\n",
    "                        vector_store.add_documents(batch)\n",
    "                    \n",
    "                    total_processed += len(batch)\n",
    "                    print(f\"‚úÖ Batch {batch_num} completed. Total processed: {total_processed}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing batch {batch_num}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # ‡∫ö‡∫±‡∫ô‡∫ó‡∫∂‡∫Å‡∫Å‡∫≤‡∫ô‡∫õ‡ªà‡∫Ω‡∫ô‡ªÅ‡∫õ‡∫á\n",
    "            if vector_store:\n",
    "                vector_store.persist()\n",
    "                print(f\"üíæ Vector store saved to: {persist_directory}\")\n",
    "                \n",
    "                collection_count = vector_store._collection.count()\n",
    "                print(f\"üìä Total vectors in collection: {collection_count}\")\n",
    "                print(f\"üìö Collection name: {collection_name}\")\n",
    "                \n",
    "                return vector_store\n",
    "            else:\n",
    "                print(\"‚ùå Failed to create vector store\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating vector store: {e}\")\n",
    "            return None\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_existing_vector_store(\n",
    "        embedding_model: str = \"D:/model/BAAI-bge-m3\",\n",
    "        collection_name: str = \"pdf_documents\", \n",
    "        persist_directory: str = \"./chroma_db\"\n",
    "    ) -> Optional[Chroma]:\n",
    "        \"\"\"\n",
    "        ‡ªÇ‡∫´‡∫º‡∫î Vector Store ‡∫ó‡∫µ‡ªà‡∫°‡∫µ‡∫¢‡∫π‡ªà‡ªÅ‡∫•‡ªâ‡∫ß‡∫à‡∫≤‡∫Å ChromaDB\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Path to embedding model\n",
    "            collection_name: Name of ChromaDB collection\n",
    "            persist_directory: Directory where ChromaDB is saved\n",
    "            \n",
    "        Returns:\n",
    "            Chroma vector store object or None\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # ‡∫Å‡∫ß‡∫î‡∫™‡∫≠‡∫ö‡∫ß‡ªà‡∫≤‡∫°‡∫µ‡ªÇ‡∫ü‡∫•‡ªÄ‡∫î‡∫µ‡∫´‡∫º‡∫∑‡∫ö‡ªç‡ªà\n",
    "            if not os.path.exists(persist_directory):\n",
    "                print(f\"‚ùå Directory not found: {persist_directory}\")\n",
    "                return None\n",
    "            \n",
    "            # ‡ªÇ‡∫´‡∫º‡∫î embedding model\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=embedding_model,\n",
    "                model_kwargs={'device': 'cpu'},\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "            \n",
    "            # ‡ªÇ‡∫´‡∫º‡∫î vector store\n",
    "            vector_store = Chroma(\n",
    "                collection_name=collection_name,\n",
    "                embedding_function=embeddings,\n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "            \n",
    "            # ‡∫Å‡∫ß‡∫î‡∫™‡∫≠‡∫ö‡∫ß‡ªà‡∫≤‡∫°‡∫µ‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫´‡∫º‡∫∑‡∫ö‡ªç‡ªà\n",
    "            collection_count = vector_store._collection.count()\n",
    "            if collection_count > 0:\n",
    "                print(f\"‚úÖ Loaded existing vector store: {collection_name}\")\n",
    "                print(f\"üìä Total vectors: {collection_count}\")\n",
    "                return vector_store\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Collection '{collection_name}' is empty\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading vector store: {e}\")\n",
    "            return None\n",
    "        \n",
    "    @staticmethod\n",
    "    def search_similar_documents(\n",
    "        vector_store: Chroma,\n",
    "        query: str,\n",
    "        k: int = 5\n",
    "    ) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        ‡∫Ñ‡∫ª‡ªâ‡∫ô‡∫´‡∫≤‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫ó‡∫µ‡ªà‡∫Ñ‡ªâ‡∫≤‡∫ç‡∫Ñ‡∫∑‡∫Å‡∫±‡∫ô\n",
    "        vector_store: ‡ªÅ‡∫°‡ªà‡∫ô‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡ªÄ‡∫Æ‡∫ª‡∫≤‡ªÄ‡∫Ñ‡∫µ‡∫ç‡∫™‡ªâ‡∫≤‡∫á Vector Store ‡ªÉ‡∫ô ./chroma_db\n",
    "        query: ‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫ó‡∫µ‡ªà‡∫ï‡ªâ‡∫≠‡∫á‡∫Å‡∫≤‡∫ô‡∫Ñ‡∫ª‡ªâ‡∫ô‡∫´‡∫≤\n",
    "        k: ‡∫à‡∫≥‡∫ô‡∫ß‡∫ô‡∫ú‡∫ª‡∫ô‡∫•‡∫±‡∫ö‡∫ó‡∫µ‡ªà‡∫ï‡ªâ‡∫≠‡∫á‡∫Å‡∫≤‡∫ô\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples (document, score)\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"üîç Searching for: {query}\")\n",
    "            \n",
    "            # ‡∫Ñ‡∫ª‡ªâ‡∫ô‡∫´‡∫≤‡∫î‡ªâ‡∫ß‡∫ç score\n",
    "            results = vector_store.similarity_search_with_score(\n",
    "                query=query,\n",
    "                k=k\n",
    "            )\n",
    "            \n",
    "            # ‡∫™‡∫∞‡ªÅ‡∫î‡∫á‡∫ú‡∫ª‡∫ô‡∫•‡∫±‡∫ö\n",
    "            # for i, (doc, score) in enumerate(results):\n",
    "            #     similarity = 1 - score  # ‡∫õ‡ªà‡∫Ω‡∫ô distance ‡ªÄ‡∫õ‡∫±‡∫ô similarity\n",
    "            #     print(f\"\\nüìÑ Result {i+1} (Similarity: {similarity:.3f}):\")\n",
    "            #     print(f\"   üìÅ Source: {doc.metadata.get('source_file', 'Unknown')}\")\n",
    "            #     print(f\"   üìÑ Page: {doc.metadata.get('page', 'Unknown')}\")\n",
    "            #     print(f\"   üîñ Chunk: {doc.metadata.get('chunk_id', 'Unknown')}\")\n",
    "            #     print(f\"   üìù Content preview: {doc.page_content[:100]}...\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during search: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqRAGSystem:\n",
    "    \"\"\"\n",
    "    ‡∫•‡∫∞‡∫ö‡∫ª‡∫ö RAG ‡∫õ‡∫∞‡∫™‡∫ª‡∫°‡∫Å‡∫±‡∫ö Groq LLM ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫ï‡∫≠‡∫ö‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫≠‡ªâ‡∫≤‡∫á‡∫≠‡∫µ‡∫á‡∫à‡∫≤‡∫Å‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, groq_api_key: str, model_name: str = \"openai/gpt-oss-120b\"):\n",
    "        \"\"\"\n",
    "        ‡ªÄ‡∫•‡∫µ‡ªà‡∫°‡∫ï‡∫ª‡ªâ‡∫ô GroqRAGSystem\n",
    "        \n",
    "        Args:\n",
    "            groq_api_key: Groq API key (‡∫ï‡ªâ‡∫≠‡∫á‡ªÑ‡∫õ‡∫™‡∫∞‡ªù‡∫±‡∫Å‡∫ó‡∫µ‡ªà https://console.groq.com)\n",
    "            model_name: ‡∫ä‡∫∑‡ªà Model ‡∫ó‡∫µ‡ªà‡∫à‡∫∞‡ªÉ‡∫ä‡ªâ (‡∫ç‡∫ª‡∫Å‡∫ï‡∫ª‡∫ß‡∫¢‡ªà‡∫≤‡∫á: openai/gpt-oss-120b)\n",
    "        \"\"\"\n",
    "        self.client = Groq(api_key=groq_api_key)\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def create_context_from_documents(self, search_results: List[tuple]) -> str:\n",
    "        \"\"\"\n",
    "        ‡∫™‡ªâ‡∫≤‡∫á context ‡∫à‡∫≤‡∫Å‡∫ú‡∫ª‡∫ô‡∫Å‡∫≤‡∫ô‡∫Ñ‡∫ª‡ªâ‡∫ô‡∫´‡∫≤‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô\n",
    "        \n",
    "        Args:\n",
    "            search_results: List of tuples (document, score) ‡∫à‡∫≤‡∫Å vector search\n",
    "            \n",
    "        Returns:\n",
    "            ‡∫Ç‡ªç‡ªâ‡∫Ñ‡∫ß‡∫≤‡∫° context ‡∫™‡∫≥‡∫•‡∫±‡∫ö LLM\n",
    "        \"\"\"\n",
    "        if not search_results:\n",
    "            return \"‡∫ö‡ªç‡ªà‡∫û‡∫ª‡∫ö‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫ó‡∫µ‡ªà‡∫Å‡ªà‡∫Ω‡∫ß‡∫Ç‡ªâ‡∫≠‡∫á\"\n",
    "            \n",
    "        context_parts = []\n",
    "        for i, (doc, score) in enumerate(search_results):\n",
    "            similarity = 1 - score\n",
    "            source_info = f\"‡ªÅ‡∫´‡∫º‡ªà‡∫á: {doc.metadata.get('source_file', 'Unknown')} (‡ªú‡ªâ‡∫≤ {doc.metadata.get('page', 'Unknown')})\"\n",
    "            content = doc.page_content.strip()\n",
    "            \n",
    "            context_parts.append(f\"‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô {i+1} (‡∫Ñ‡∫ß‡∫≤‡∫°‡∫Ñ‡ªâ‡∫≤‡∫ç‡∫Ñ‡∫∑: {similarity:.3f}):\\n{source_info}\\n{content}\\n\")\n",
    "            \n",
    "        return \"\\n---\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_answer(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        ‡∫™‡ªâ‡∫≤‡∫á‡∫Ñ‡∫≥‡∫ï‡∫≠‡∫ö‡ªÇ‡∫î‡∫ç‡ªÉ‡∫ä‡ªâ Groq LLM ‡∫û‡ªâ‡∫≠‡∫° context ‡∫à‡∫≤‡∫Å‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô\n",
    "        \n",
    "        Args:\n",
    "            query: ‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫Ç‡∫≠‡∫á‡∫ú‡∫π‡ªâ‡ªÉ‡∫ä‡ªâ\n",
    "            context: Context ‡∫à‡∫≤‡∫Å‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô\n",
    "            \n",
    "        Returns:\n",
    "            ‡∫Ñ‡∫≥‡∫ï‡∫≠‡∫ö‡∫à‡∫≤‡∫Å LLM\n",
    "        \"\"\"\n",
    "        \n",
    "        # ‡∫™‡ªâ‡∫≤‡∫á prompt ‡∫™‡∫≥‡∫•‡∫±‡∫ö RAG\n",
    "        prompt = f\"\"\"‡∫ó‡ªà‡∫≤‡∫ô‡ªÄ‡∫õ‡∫±‡∫ô AI Assistant ‡∫ó‡∫µ‡ªà‡∫ä‡ªà‡∫Ω‡∫ß‡∫ä‡∫≤‡∫ô‡ªÉ‡∫ô‡∫Å‡∫≤‡∫ô‡∫ï‡∫≠‡∫ö‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡ªÇ‡∫î‡∫ç‡∫≠‡ªâ‡∫≤‡∫á‡∫≠‡∫µ‡∫á‡∫à‡∫≤‡∫Å‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫ó‡∫µ‡ªà‡ªÉ‡∫´‡ªâ‡∫°‡∫≤.\n",
    "\n",
    "‡∫Ñ‡∫≥‡ªÅ‡∫ô‡∫∞‡∫ô‡∫≥:\n",
    "1. ‡∫ï‡∫≠‡∫ö‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡ªÇ‡∫î‡∫ç‡∫≠‡ªâ‡∫≤‡∫á‡∫≠‡∫µ‡∫á‡∫à‡∫≤‡∫Å‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫ó‡∫µ‡ªà‡ªÉ‡∫´‡ªâ‡∫°‡∫≤‡ªÄ‡∫ó‡∫ª‡ªà‡∫≤‡∫ô‡∫±‡ªâ‡∫ô\n",
    "2. ‡∫ñ‡ªâ‡∫≤‡∫ö‡ªç‡ªà‡∫û‡∫ª‡∫ö‡∫Ñ‡∫≥‡∫ï‡∫≠‡∫ö‡ªÉ‡∫ô‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô, ‡ªÉ‡∫´‡ªâ‡∫ö‡∫≠‡∫Å‡∫ß‡ªà‡∫≤‡∫ö‡ªç‡ªà‡∫û‡∫ª‡∫ö‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫ó‡∫µ‡ªà‡∫Å‡ªà‡∫Ω‡∫ß‡∫Ç‡ªâ‡∫≠‡∫á\n",
    "3. ‡∫•‡∫∞‡∫ö‡∫∏‡ªÅ‡∫´‡∫º‡ªà‡∫á‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫ó‡∫µ‡ªà‡ªÉ‡∫ä‡ªâ‡ªÉ‡∫ô‡∫Å‡∫≤‡∫ô‡∫ï‡∫≠‡∫ö\n",
    "4. ‡∫ï‡∫≠‡∫ö‡ªÄ‡∫õ‡∫±‡∫ô‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß ‡ªÅ‡∫•‡∫∞ ‡ªÉ‡∫´‡ªâ‡∫Ñ‡∫≥‡∫ï‡∫≠‡∫ö‡∫ó‡∫µ‡ªà‡∫ä‡∫±‡∫î‡ªÄ‡∫à‡∫ô, ‡∫•‡∫∞‡∫≠‡∫Ω‡∫î\n",
    "5. ‡∫ï‡∫≠‡∫ö‡ªÉ‡∫´‡ªâ‡ªÄ‡∫õ‡∫±‡∫ô Format markdown\n",
    "\n",
    "‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫≠‡ªâ‡∫≤‡∫á‡∫≠‡∫µ‡∫á:\n",
    "{context}\n",
    "\n",
    "‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°: {query}\n",
    "\n",
    "‡∫Ñ‡∫≥‡∫ï‡∫≠‡∫ö:\"\"\"\n",
    "\n",
    "        try:\n",
    "            # ‡∫™‡∫ª‡ªà‡∫á request ‡ªÑ‡∫õ Groq\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                model=self.model_name,\n",
    "                temperature=0.1,  # ‡∫Ñ‡∫ß‡∫≤‡∫°‡∫™‡ªâ‡∫≤‡∫á‡∫™‡∫±‡∫ô‡∫ï‡ªà‡∫≥ ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫Ñ‡∫ß‡∫≤‡∫°‡ªÅ‡∫°‡ªà‡∫ô‡∫ç‡∫≥  ‡∫Ç‡∫∂‡ªâ‡∫ô‡∫ô‡∫≥ Model ‡ªÄ‡∫û‡∫≤‡∫∞‡∫Ñ‡ªà‡∫≤ temperature ‡ªÅ‡∫ï‡ªà‡∫•‡∫∞‡ªÄ‡∫à‡∫ª‡ªâ‡∫≤‡∫°‡∫±‡∫ô‡∫ï‡ªà‡∫≤‡∫á‡∫Å‡∫±‡∫ô\n",
    "                max_tokens=1500,  # ‡∫à‡∫≥‡∫ô‡∫ß‡∫ô tokens ‡∫™‡∫π‡∫á‡∫™‡∫∏‡∫î \n",
    "            )\n",
    "            \n",
    "            answer = chat_completion.choices[0].message.content\n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"‚ùå ‡ªÄ‡∫Å‡∫µ‡∫î‡∫Ç‡ªç‡ªâ‡∫ú‡∫¥‡∫î‡∫û‡∫≤‡∫î‡ªÉ‡∫ô‡∫Å‡∫≤‡∫ô‡∫™‡ªâ‡∫≤‡∫á‡∫Ñ‡∫≥‡∫ï‡∫≠‡∫ö: {str(e)}\"\n",
    "    \n",
    "    def query_documents(self, vector_store: Chroma, query: str, k: int = 5) -> dict:\n",
    "        \"\"\"\n",
    "        ‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡ªÅ‡∫ö‡∫ö‡∫™‡∫ª‡∫°‡∫ö‡∫π‡∫ô‡∫à‡∫≤‡∫Å‡∫Å‡∫≤‡∫ô‡∫Ñ‡∫ª‡ªâ‡∫ô‡∫´‡∫≤‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫à‡∫ª‡∫ô‡ªÄ‡∫ñ‡∫µ‡∫á‡∫Å‡∫≤‡∫ô‡∫™‡ªâ‡∫≤‡∫á‡∫Ñ‡∫≥‡∫ï‡∫≠‡∫ö\n",
    "        \n",
    "        Args:\n",
    "            vector_store: ChromaDB vector store\n",
    "            query: ‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫Ç‡∫≠‡∫á‡∫ú‡∫π‡ªâ‡ªÉ‡∫ä‡ªâ\n",
    "            k: ‡∫à‡∫≥‡∫ô‡∫ß‡∫ô‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫ó‡∫µ‡ªà‡∫à‡∫∞‡∫Ñ‡∫ª‡ªâ‡∫ô‡∫´‡∫≤\n",
    "            \n",
    "        Returns:\n",
    "            dict ‡∫ó‡∫µ‡ªà‡∫õ‡∫∞‡∫Å‡∫≠‡∫ö‡∫î‡ªâ‡∫ß‡∫ç answer, context, ‡ªÅ‡∫•‡∫∞ sources\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\nü§ñ Processing query: {query}\")\n",
    "        \n",
    "        # 1. ‡∫Ñ‡∫ª‡ªâ‡∫ô‡∫´‡∫≤‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫ó‡∫µ‡ªà‡∫Å‡ªà‡∫Ω‡∫ß‡∫Ç‡ªâ‡∫≠‡∫á\n",
    "        search_results = DocumentLoader.search_similar_documents(\n",
    "            vector_store=vector_store,\n",
    "            query=query,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        if not search_results:\n",
    "            return {\n",
    "                \"answer\": \"‚ùå ‡∫ö‡ªç‡ªà‡∫û‡∫ª‡∫ö‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫ó‡∫µ‡ªà‡∫Å‡ªà‡∫Ω‡∫ß‡∫Ç‡ªâ‡∫≠‡∫á‡∫Å‡∫±‡∫ö‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫Ç‡∫≠‡∫á‡∫ó‡ªà‡∫≤‡∫ô\",\n",
    "                \"context\": \"\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # 2. ‡∫™‡ªâ‡∫≤‡∫á context ‡∫à‡∫≤‡∫Å‡∫ú‡∫ª‡∫ô‡∫Å‡∫≤‡∫ô‡∫Ñ‡∫ª‡ªâ‡∫ô‡∫´‡∫≤\n",
    "        context = self.create_context_from_documents(search_results)\n",
    "        \n",
    "        # 3. ‡∫™‡ªâ‡∫≤‡∫á‡∫Ñ‡∫≥‡∫ï‡∫≠‡∫ö‡∫î‡ªâ‡∫ß‡∫ç LLM\n",
    "        print(\"üß† Generating answer with Groq LLM...\")\n",
    "        answer = self.generate_answer(query, context)\n",
    "        \n",
    "        # 4. ‡∫™‡ªâ‡∫≤‡∫á‡∫•‡∫≤‡∫ç‡∫ä‡∫∑‡ªà‡ªÅ‡∫´‡∫º‡ªà‡∫á‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô\n",
    "        sources = []\n",
    "        for doc, score in search_results:\n",
    "            similarity = 1 - score\n",
    "            sources.append({\n",
    "                \"source_file\": doc.metadata.get('source_file', 'Unknown'),\n",
    "                \"page\": doc.metadata.get('page', 'Unknown'),\n",
    "                \"similarity\": f\"{similarity:.3f}\",\n",
    "                \"content_preview\": doc.page_content\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"context\": context,\n",
    "            \"sources\": sources\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    ‡∫ü‡∫±‡∫á‡∫ä‡∫±‡ªà‡∫ô‡∫´‡∫º‡∫±‡∫Å‡∫™‡∫≥‡∫•‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫ó‡∫ª‡∫î‡∫™‡∫≠‡∫ö‡∫•‡∫∞‡∫ö‡∫ª‡∫ö RAG ‡∫Å‡∫±‡∫ö Groq\n",
    "    \"\"\"\n",
    "    \n",
    "    # ‡∫Å‡∫≤‡∫ô‡∫ï‡∫±‡ªâ‡∫á‡∫Ñ‡ªà‡∫≤\n",
    "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")  # ‡ªÅ‡∫ó‡∫ô‡∫Ñ‡ªà‡∫≤‡∫î‡ªâ‡∫ß‡∫ç API key ‡∫à‡∫¥‡∫á\n",
    "    \n",
    "    # ‡∫•‡∫≤‡∫ç‡∫ä‡∫∑‡ªà‡ªÑ‡∫ü‡∫•‡ªå PDF (‡∫ñ‡ªâ‡∫≤‡∫ï‡ªâ‡∫≠‡∫á‡∫Å‡∫≤‡∫ô‡∫™‡ªâ‡∫≤‡∫á vector store ‡ªÉ‡ªù‡ªà)\n",
    "    pdf_files = [ \n",
    "        \"C:/Users/Dell/Desktop/Finetuing vs RAG.pdf\"\n",
    "    ]\n",
    "    \n",
    "    # ‡∫Å‡∫ß‡∫î‡∫™‡∫≠‡∫ö‡∫ß‡ªà‡∫≤‡∫°‡∫µ vector store ‡∫¢‡∫π‡ªà‡ªÅ‡∫•‡ªâ‡∫ß‡∫´‡∫º‡∫∑‡∫ö‡ªç‡ªà\n",
    "    \n",
    "    display(Markdown(\"## üîç ‡∫Å‡∫ß‡∫î‡∫™‡∫≠‡∫ö Vector Store\")) \n",
    "    loaded_vectorstore = DocumentLoader.load_existing_vector_store(\n",
    "        embedding_model=\"D:/model/BAAI-bge-m3\", \n",
    "        collection_name=\"pdf_documents\", \n",
    "        persist_directory=\"./chroma_db\"\n",
    "    )\n",
    "    \n",
    "    # ‡∫ñ‡ªâ‡∫≤‡∫ö‡ªç‡ªà‡∫°‡∫µ vector store, ‡∫™‡ªâ‡∫≤‡∫á‡ªÉ‡ªù‡ªà\n",
    "    if loaded_vectorstore is None:\n",
    "        display(Markdown(\"## üìö Creating new vector store...\"))  \n",
    "        \n",
    "        # 1. ‡ªÇ‡∫´‡∫º‡∫î‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô\n",
    "        documents = DocumentLoader.load_docs(pdf_files) \n",
    "        \n",
    "        if not documents:\n",
    "            print(\"‚ùå No documents found. Please check your PDF file paths.\")\n",
    "            return\n",
    "            \n",
    "        # 2. ‡ªÄ‡∫Æ‡∫±‡∫î chunking ‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô \n",
    "        display(Markdown(\"## ‚úÇÔ∏è Chunking documents...\")) \n",
    "        # ‡ªÄ‡∫Æ‡∫±‡∫î chunking ‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô\n",
    "        # ‡ªÉ‡∫ä‡ªâ Model ‡∫Ç‡∫≠‡∫á BAAI-bge-m3 ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫Æ‡∫±‡∫ö‡∫Ñ‡ªà‡∫≤‡∫Å‡∫≤‡∫ô‡ªÄ‡∫Æ‡∫±‡∫î chunking ‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô ‡ªÄ‡∫ä‡∫µ‡ªà‡∫á‡∫ú‡∫π‡ªâ‡ªÉ‡∫ä‡ªâ‡ªÅ‡∫°‡ªà‡∫ô‡∫™‡∫≤‡∫°‡∫≤‡∫î‡ªÄ‡∫•‡∫∑‡∫≠‡∫Å‡ªÑ‡∫î‡ªâ‡∫ï‡∫≤‡∫°‡ªÉ‡∫à‡ªÄ‡∫•‡∫µ‡∫ç‡∫ß‡ªà‡∫≤‡∫à‡∫∞ ‡ªÉ‡∫ä‡ªâ Model ‡∫ç‡∫±‡∫á‡ªÉ‡∫ô‡∫Å‡∫≤‡∫ô‡ªÄ‡∫Æ‡∫±‡∫î Embedding ‡∫™‡∫≤‡∫°‡∫≤‡∫î‡ªÇ‡∫´‡∫•‡∫î‡∫ú‡ªà‡∫≤‡∫ô Hugginface ‡ªÑ‡∫î‡ªâ ‡ªÇ‡∫î‡∫ç‡∫Å‡∫≥‡∫ô‡∫ª‡∫î path ‡ªÄ‡∫≠‡∫á ‡∫™‡∫≤‡∫°‡∫≤‡∫î ‡ªÄ‡∫Ç‡∫ª‡ªâ‡∫≤‡ªÑ‡∫õ‡ªÉ‡∫ô Folder Download Model/download-model.ipynb ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫î‡∫≤‡∫ß‡ªÇ‡∫´‡∫•‡∫î Model ‡∫ç‡∫±‡∫á\n",
    "        # ‡∫Å‡∫≥‡∫ô‡∫±‡∫î‡∫Ñ‡ªà‡∫≤‡∫ï‡ªà‡∫≤‡∫á‡ªÜ‡∫Ç‡∫≠‡∫á chunking ‡ªÇ‡∫î‡∫ç Base on ‡∫à‡∫≤‡∫Å‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô ‡∫ñ‡ªâ‡∫≤ ‡∫°‡∫µ‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫´‡∫•‡∫≤‡∫ç‡∫´‡∫ô‡ªâ‡∫≤ ‡ªÅ‡∫ô‡∫∞‡∫ô‡∫≥‡ªÉ‡∫´‡ªâ‡∫•‡∫≠‡∫á‡ªÄ‡∫û‡∫¥‡ªà‡∫°‡∫Ñ‡ªà‡∫≤ chunk_size ‡ªÅ‡∫•‡∫∞ chunk_overlap ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫Æ‡∫±‡∫ö‡∫Ñ‡ªà‡∫≤‡∫ó‡∫µ‡ªà‡∫î‡∫µ‡∫Å‡∫ß‡ªà‡∫≤\n",
    "        chunk_documents = DocumentLoader.chunk_documents_standard(\n",
    "            documents, \n",
    "            chunk_size=500, \n",
    "            chunk_overlap=50, \n",
    "            tokenizer_model=\"D:/model/BAAI-bge-m3\", \n",
    "            max_token_limit=1000\n",
    "        )\n",
    "        \n",
    "        if not chunk_documents:\n",
    "            print(\"‚ùå Failed to chunk documents.\")\n",
    "            return\n",
    "            \n",
    "        # 3. ‡∫™‡ªâ‡∫≤‡∫á vector store \n",
    "        display(Markdown(\"## üîÑ Creating vector store...\")) \n",
    "        # ‡ªÄ‡∫Æ‡∫±‡∫î Embedding ‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô\n",
    "        # ‡∫Å‡ªç‡∫•‡∫∞‡∫ô‡∫µ‡∫ô‡∫µ‡ªâ‡∫à‡∫∞‡∫ñ‡ªâ‡∫≤‡∫î‡∫ª‡∫ô‡ªÅ‡∫ô‡ªà ‡ªÄ‡∫ô‡∫∑‡ªà‡∫≠‡∫á‡∫à‡∫≤‡∫Å‡∫ß‡ªà‡∫≤ ‡∫à‡∫∞‡∫°‡∫µ‡∫Å‡∫≤‡∫ô‡ªÄ‡∫≠‡∫ª‡∫≤ ‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫ó‡∫µ‡ªà‡ªÄ‡∫Æ‡∫ª‡∫≤ Chunking ‡∫°‡∫≤‡ªÅ‡∫õ‡∫á‡ªÄ‡∫õ‡∫±‡∫ô Vector ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫ö‡∫±‡∫ô‡∫ó‡∫∑‡∫Å‡ªÉ‡∫ô ChromaDB ‡∫ñ‡ªâ‡∫≤‡∫¢‡∫≤‡∫Å‡ªÉ‡∫´‡ªâ‡ªÑ‡∫ß‡ªâ ‡ªÉ‡∫ú‡∫°‡∫µ GPU ‡ªÅ‡∫ô‡∫∞‡∫ô‡∫≥‡ªÉ‡∫´‡ªâ‡ªÉ‡∫ä‡ªâ cuda ‡ªÅ‡∫ó‡∫ô cpu\n",
    "        loaded_vectorstore = DocumentLoader.create_vector_store(chunk_documents)\n",
    "        \n",
    "        if loaded_vectorstore is None:\n",
    "            print(\"‚ùå Failed to create vector store.\")\n",
    "            return\n",
    "    \n",
    "    # ‡ªÄ‡∫•‡∫µ‡ªà‡∫°‡∫ï‡∫ª‡ªâ‡∫ô‡∫•‡∫∞‡∫ö‡∫ª‡∫ö RAG ‡∫Å‡∫±‡∫ö Groq \n",
    "    display(Markdown(\"## üöÄ Initializing Groq RAG System...\")) \n",
    "    \n",
    "    if GROQ_API_KEY == \"‡ªÉ‡∫™‡ªà Groq API Key ‡∫Ç‡∫≠‡∫á‡ªÄ‡∫à‡∫ª‡ªâ‡∫≤‡∫ó‡∫µ‡ªà‡∫ô‡∫µ‡ªâ\":\n",
    "        print(\"‚ùå ‡∫Å‡∫∞‡∫•‡∫∏‡∫ô‡∫≤‡ªÉ‡∫™‡ªà Groq API Key ‡∫Ç‡∫≠‡∫á‡ªÄ‡∫à‡∫ª‡ªâ‡∫≤‡ªÉ‡∫ô‡∫ï‡∫ª‡∫ß‡ªÅ‡∫õ GROQ_API_KEY\")\n",
    "        print(\"üí° ‡∫™‡∫≤‡∫°‡∫≤‡∫î‡ªÑ‡∫î‡ªâ API key ‡∫ü‡∫£‡∫µ‡∫ó‡∫µ‡ªà: https://console.groq.com\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        rag_system = GroqRAGSystem(\n",
    "            groq_api_key=GROQ_API_KEY,\n",
    "            model_name=\"openai/gpt-oss-120b\" \n",
    "        )\n",
    "        display(Markdown(\"## ‚úÖ Groq RAG System initialized successfully !\"))  \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing Groq system: {e}\")\n",
    "        return\n",
    "    \n",
    "    # ‡∫ó‡∫ª‡∫î‡∫™‡∫≠‡∫ö‡∫•‡∫∞‡∫ö‡∫ª‡∫ö‡∫î‡ªâ‡∫ß‡∫ç‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫ï‡∫ª‡∫ß‡∫¢‡ªà‡∫≤‡∫á\n",
    "    test_queries = [\n",
    "        \"RAG ‡∫Å‡∫±‡∫ö Fine-tuning ‡∫°‡∫µ‡∫Ñ‡∫ß‡∫≤‡∫°‡ªÅ‡∫ï‡∫Å‡∫ï‡ªà‡∫≤‡∫á‡∫Å‡∫±‡∫ô‡ªÅ‡∫ô‡∫ß‡ªÉ‡∫î ?\"\n",
    "    ]\n",
    "    \n",
    "    display(Markdown(\"## üß™ ‡∫Å‡∫≤‡∫ô‡∫ó‡∫ª‡∫î‡∫™‡∫≠‡∫ö‡∫•‡∫∞‡∫ö‡∫ª‡∫ö RAG\"))\n",
    "    display(Markdown(\"‡∫ó‡∫ª‡∫î‡∫™‡∫≠‡∫ö‡∫î‡ªâ‡∫ß‡∫ç‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫ï‡∫ª‡∫ß‡∫¢‡ªà‡∫≤‡∫á 4 ‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°\"))\n",
    "    display(Markdown(\"---\"))\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüìù ‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫ó‡∫µ‡ªà {i}: {query}\")\n",
    "        print(\"-\" * 40)\n",
    "        display(Markdown(f\"### üìù ‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫ó‡∫µ‡ªà {i}: {query}\"))\n",
    "        display(Markdown(\"---\"))\n",
    "        \n",
    "        # ‡∫™‡∫ª‡ªà‡∫á‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡ªÑ‡∫õ‡∫•‡∫∞‡∫ö‡∫ª‡∫ö RAG\n",
    "        result = rag_system.query_documents(\n",
    "            vector_store=loaded_vectorstore,\n",
    "            query=query,\n",
    "            k=5  # ‡∫Ñ‡∫ª‡ªâ‡∫ô‡∫´‡∫≤ 5 ‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫ó‡∫µ‡ªà‡∫Å‡ªà‡∫Ω‡∫ß‡∫Ç‡ªâ‡∫≠‡∫á\n",
    "        )\n",
    "        \n",
    "        # ‡∫™‡∫∞‡ªÅ‡∫î‡∫á‡∫ú‡∫ª‡∫ô‡∫•‡∫±‡∫ö\n",
    "        display(Markdown(\"#### ü§ñ ‡∫Ñ‡∫≥‡∫ï‡∫≠‡∫ö:\"))\n",
    "        display(Markdown(f\"\"\"\n",
    "            ```\n",
    "            {result['answer']}\n",
    "            ```\n",
    "        \"\"\"))\n",
    "        \n",
    "        if result['sources']:\n",
    "            display(Markdown(\"#### üìö ‡ªÅ‡∫´‡∫º‡ªà‡∫á‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫≠‡ªâ‡∫≤‡∫á‡∫≠‡∫µ‡∫á:\"))\n",
    "                \n",
    "            sources_md = \"\"\n",
    "            for j, source in enumerate(result['sources'], 1):\n",
    "                    sources_md += f\"\"\"\n",
    "                **{j}.** `{source['source_file']}` (‡ªú‡ªâ‡∫≤ {source['page']}) - ‡∫Ñ‡∫ß‡∫≤‡∫°‡∫Ñ‡ªâ‡∫≤‡∫ç‡∫Ñ‡∫∑: `{source['similarity']}`\n",
    "                > {source['content_preview']}...\n",
    "\n",
    "            \"\"\"\n",
    "            display(Markdown(sources_md))\n",
    "                    \n",
    "        display(Markdown(\"---\"))\n",
    "    \n",
    "    # ‡ªÇ‡ªù‡∫î interactive ‡∫™‡∫≥‡∫•‡∫±‡∫ö‡∫ú‡∫π‡ªâ‡ªÉ‡∫ä‡ªâ‡∫™‡∫≤‡∫°‡∫≤‡∫î‡∫ñ‡∫≤‡∫°‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡ªÄ‡∫≠‡∫á\n",
    "    display(Markdown(\"## üí¨ ‡ªÇ‡ªù‡∫î Interactive - ‡∫û‡∫¥‡∫°‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫Ç‡∫≠‡∫á‡∫ó‡ªà‡∫≤‡∫ô (‡∫û‡∫¥‡∫° 'quit' ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫≠‡∫≠‡∫Å\")) \n",
    "    display(Markdown(\"---\"))\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_query = input(\"\\n‚ùì ‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫Ç‡∫≠‡∫á‡∫ó‡ªà‡∫≤‡∫ô: \").strip()\n",
    "            \n",
    "            if user_query.lower() in ['quit', 'exit', '‡∫≠‡∫≠‡∫Å']:\n",
    "                print(\"üëã ‡∫Ç‡∫≠‡∫ö‡ªÉ‡∫à‡∫ó‡∫µ‡ªà‡ªÉ‡∫ä‡ªâ‡∫•‡∫∞‡∫ö‡∫ª‡∫ö RAG!\")\n",
    "                break\n",
    "                \n",
    "            if not user_query:\n",
    "                print(\"‚ö†Ô∏è ‡∫Å‡∫∞‡∫•‡∫∏‡∫ô‡∫≤‡ªÉ‡∫™‡ªà‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°\")\n",
    "                continue\n",
    "            \n",
    "            display(Markdown(f\"### ‚ùì ‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°: `{user_query}`\"))\n",
    "            \n",
    "            # ‡∫™‡∫ª‡ªà‡∫á‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡ªÑ‡∫õ‡∫•‡∫∞‡∫ö‡∫ª‡∫ö RAG\n",
    "            result = rag_system.query_documents(\n",
    "                vector_store=loaded_vectorstore,\n",
    "                query=user_query,\n",
    "                k=5\n",
    "            )\n",
    "            \n",
    "            # ‡∫™‡∫∞‡ªÅ‡∫î‡∫á‡∫ú‡∫ª‡∫ô‡∫•‡∫±‡∫ö\n",
    "            display(Markdown(\"#### ü§ñ ‡∫Ñ‡∫≥‡∫ï‡∫≠‡∫ö:\"))\n",
    "            display(Markdown(f\"\"\"\n",
    "                ```\n",
    "                {result['answer']}\n",
    "                ```\n",
    "            \"\"\"))\n",
    "            \n",
    "            # ‡∫™‡∫∞‡ªÅ‡∫î‡∫á‡ªÅ‡∫´‡∫º‡ªà‡∫á‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô (‡ªÅ‡∫ö‡∫ö‡∫´‡∫ç‡ªç‡ªâ)\n",
    "            if result['sources']: \n",
    "                display(Markdown(\"#### üìö ‡ªÅ‡∫´‡∫º‡ªà‡∫á‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫≠‡ªâ‡∫≤‡∫á‡∫≠‡∫µ‡∫á:\"))\n",
    "                for source in result['sources'][:3]:  # ‡∫™‡∫∞‡ªÅ‡∫î‡∫á 3 ‡ªÅ‡∫´‡∫º‡ªà‡∫á‡∫ó‡∫≥‡∫≠‡∫¥‡∫î\n",
    "                    display(Markdown(f\"#### ‚Ä¢ {source['source_file']} (‡ªú‡ªâ‡∫≤ {source['page']})\")) \n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã ‡∫Ç‡∫≠‡∫ö‡ªÉ‡∫à‡∫ó‡∫µ‡ªà‡ªÉ‡∫ä‡ªâ‡∫•‡∫∞‡∫ö‡∫ª‡∫ö RAG!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ‡ªÄ‡∫Å‡∫µ‡∫î‡∫Ç‡ªç‡ªâ‡∫ú‡∫¥‡∫î‡∫û‡∫≤‡∫î: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üîç ‡∫Å‡∫ß‡∫î‡∫™‡∫≠‡∫ö Vector Store"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded existing vector store: pdf_documents\n",
      "üìä Total vectors: 2\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## üöÄ Initializing Groq RAG System..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## ‚úÖ Groq RAG System initialized successfully !"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## üß™ ‡∫Å‡∫≤‡∫ô‡∫ó‡∫ª‡∫î‡∫™‡∫≠‡∫ö‡∫•‡∫∞‡∫ö‡∫ª‡∫ö RAG"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "‡∫ó‡∫ª‡∫î‡∫™‡∫≠‡∫ö‡∫î‡ªâ‡∫ß‡∫ç‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫ï‡∫ª‡∫ß‡∫¢‡ªà‡∫≤‡∫á 4 ‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù ‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫ó‡∫µ‡ªà 1: RAG ‡∫Å‡∫±‡∫ö Fine-tuning ‡∫°‡∫µ‡∫Ñ‡∫ß‡∫≤‡∫°‡ªÅ‡∫ï‡∫Å‡∫ï‡ªà‡∫≤‡∫á‡∫Å‡∫±‡∫ô‡ªÅ‡∫ô‡∫ß‡ªÉ‡∫î ?\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### üìù ‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫ó‡∫µ‡ªà 1: RAG ‡∫Å‡∫±‡∫ö Fine-tuning ‡∫°‡∫µ‡∫Ñ‡∫ß‡∫≤‡∫°‡ªÅ‡∫ï‡∫Å‡∫ï‡ªà‡∫≤‡∫á‡∫Å‡∫±‡∫ô‡ªÅ‡∫ô‡∫ß‡ªÉ‡∫î ?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Processing query: RAG ‡∫Å‡∫±‡∫ö Fine-tuning ‡∫°‡∫µ‡∫Ñ‡∫ß‡∫≤‡∫°‡ªÅ‡∫ï‡∫Å‡∫ï‡ªà‡∫≤‡∫á‡∫Å‡∫±‡∫ô‡ªÅ‡∫ô‡∫ß‡ªÉ‡∫î ?\n",
      "üîç Searching for: RAG ‡∫Å‡∫±‡∫ö Fine-tuning ‡∫°‡∫µ‡∫Ñ‡∫ß‡∫≤‡∫°‡ªÅ‡∫ï‡∫Å‡∫ï‡ªà‡∫≤‡∫á‡∫Å‡∫±‡∫ô‡ªÅ‡∫ô‡∫ß‡ªÉ‡∫î ?\n",
      "üß† Generating answer with Groq LLM...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### ü§ñ ‡∫Ñ‡∫≥‡∫ï‡∫≠‡∫ö:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "            ```\n",
       "            ## ‡∫Ñ‡∫ß‡∫≤‡∫°‡ªÅ‡∫ï‡∫Å‡∫ï‡ªà‡∫≤‡∫á‡∫•‡∫∞‡∫´‡∫ß‡ªà‡∫≤‡∫á **RAG (Retrieval‚ÄëAugmented Generation)** ‡ªÅ‡∫•‡∫∞ **Fine‚Äëtuning**  \n",
       "\n",
       "| ‡∫à‡∫∏‡∫î‡∫™‡∫≥‡∫Ñ‡∫±‡∫ô | **Fine‚Äëtuning** | **RAG** |\n",
       "|---|---|---|\n",
       "| **‡∫ß‡∫¥‡∫ó‡∫µ‡∫Å‡∫≤‡∫ô‡∫™‡ªâ‡∫≤‡∫á‡∫Ñ‡∫ß‡∫≤‡∫°‡∫Æ‡∫π‡ªâ** | ‡∫ù‡∫∂‡∫Å‡∫≠‡∫ª‡∫ö‡∫Æ‡∫ª‡∫°‡ªÇ‡∫ï‡ªÅ‡∫ö‡∫ö‡∫û‡∫Ω‡∫á‡∫î‡ªà‡∫ß‡∫ô‡∫î‡ªâ‡∫ß‡∫ç‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫ó‡∫µ‡ªà‡∫Å‡∫ß‡∫î‡∫™‡∫≠‡∫ö‡ªÅ‡∫•‡∫∞‡∫≠‡∫±‡∫ô‡∫ï‡∫¥‡∫î‡∫ï‡∫≤‡∫° (end‚Äëto‚Äëend) | ‡∫™‡ªâ‡∫≤‡∫á **knowledge base** ‡∫ó‡∫µ‡ªà‡∫™‡∫≤‡∫°‡∫≤‡∫î‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö (retrieval) ‡ªÅ‡∫•‡∫∞ ‡∫™‡ªâ‡∫≤‡∫á‡∫ú‡∫ª‡∫ô‡∫ú‡∫∞‡∫•‡∫¥‡∫î (generation) ‡∫û‡ªâ‡∫≠‡∫°‡∫Å‡∫±‡∫ô |\n",
       "| **‡∫Å‡∫≤‡∫ô‡∫≠‡∫±‡∫ö‡ªÄ‡∫î‡∫î‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô** | ‡∫ï‡ªâ‡∫≠‡∫á‡∫Å‡∫≤‡∫ô **re‚Äëfine‚Äëtuning** ‡∫ó‡∫±‡∫á‡ªù‡∫ª‡∫î‡∫ï‡ªç‡ªà‡ªÄ‡∫°‡∫∑‡ªà‡∫≠‡∫°‡∫µ‡∫Å‡∫≤‡∫ô‡∫≠‡∫±‡∫ö‡ªÄ‡∫î‡∫î‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô (‡∫à‡ªà‡∫≤‡∫ç‡∫Ñ‡ªà‡∫≤‡∫™‡∫π‡∫á)** | ‡∫™‡∫≤‡∫°‡∫≤‡∫î‡∫≠‡∫±‡∫ö‡ªÄ‡∫î‡∫î **knowledge base** ‡ªÇ‡∫î‡∫ç‡∫ö‡ªç‡ªà‡∫à‡ªç‡∫≤‡ªÄ‡∫õ‡∫±‡∫ô‡∫≠‡∫±‡∫ö‡ªÇ‡∫ï‡ªÅ‡∫ö‡∫ö‡∫≠‡∫±‡∫î‡∫ï‡∫∞‡∫û‡∫±‡∫ô‡∫≠‡∫±‡∫î‡∫ï‡∫∞‡∫û‡∫±‡∫ô (‡∫≠‡∫±‡∫ö‡ªÄ‡∫î‡∫î‡ªÑ‡∫î‡ªâ‡∫ó‡∫±‡∫ô‡∫ó‡∫µ) |\n",
       "| **‡∫Ñ‡∫ß‡∫≤‡∫°‡ªÅ‡∫à‡ªâ‡∫á‡∫Ç‡∫≠‡∫á‡∫Å‡∫≤‡∫ô‡∫ï‡∫≠‡∫ö** | ‡∫™‡∫≤‡∫°‡∫≤‡∫î‡∫™‡ªâ‡∫≤‡∫á‡∫Ñ‡∫≥‡∫ï‡∫≠‡∫ö‡∫ó‡∫±‡∫ô‡∫ó‡∫µ‡ªà‡∫≠‡∫±‡∫î‡∫ï‡∫∞‡∫û‡∫±‡∫ô‡∫î‡ªà‡∫ß‡∫ô (‡∫ö‡ªç‡ªà‡∫°‡∫µ‡∫Ç‡∫±‡ªâ‡∫ô‡∫ï‡∫≠‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö) | ‡∫°‡∫µ **generation delay** ‡ªÄ‡∫û‡∫≤‡∫∞‡∫ï‡ªâ‡∫≠‡∫á‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫à‡∫≤‡∫Å knowledge base ‡∫Å‡ªà‡∫≠‡∫ô‡∫à‡∫∞‡∫™‡ªâ‡∫≤‡∫á‡∫Ñ‡∫≥‡∫ï‡∫≠‡∫ö |\n",
       "| **‡∫Ñ‡ªà‡∫≤‡ªÉ‡∫ä‡ªâ‡∫à‡ªà‡∫≤‡∫ç‡∫Å‡∫≤‡∫ô‡∫ù‡∫∂‡∫Å** | ‡∫ï‡ªâ‡∫≠‡∫á‡∫Å‡∫≤‡∫ô **‡∫ä‡ªà‡∫ß‡∫ç‡∫™‡∫∞‡∫´‡∫º‡∫∏‡∫ö‡∫Å‡∫≤‡∫ô‡∫ù‡∫∂‡∫Å** ‡∫ó‡∫µ‡ªà‡∫Å‡∫ß‡ªà‡∫≤‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫à‡∫≥‡∫ô‡∫ß‡∫ô‡∫´‡∫º‡∫≤‡∫ç ‡ªÅ‡∫•‡∫∞ ‡∫Å‡∫≤‡∫ô‡∫Ñ‡∫¥‡∫î‡∫Ñ‡ªà‡∫≤‡∫™‡∫π‡∫á (‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫û‡∫±‡∫î‡∫ó‡∫∞‡∫ô‡∫≤‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö) | ‡∫ö‡ªç‡ªà‡∫à‡ªç‡∫≤‡ªÄ‡∫õ‡∫±‡∫ô‡∫Å‡∫≤‡∫ô‡∫ù‡∫∂‡∫Å‡∫≠‡∫±‡∫î‡∫ï‡∫∞‡∫û‡∫±‡∫ô‡∫ä‡ªà‡∫ß‡∫ç‡∫™‡∫∞‡∫´‡∫º‡∫∏‡∫ö‡∫Å‡∫≤‡∫ô‡∫≠‡∫±‡∫ö‡ªÄ‡∫î‡∫î‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫Å‡ªà‡∫≠‡∫ô‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫±‡∫ö‡∫Å\n",
       "            ```\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### üìö ‡ªÅ‡∫´‡∫º‡ªà‡∫á‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫≠‡ªâ‡∫≤‡∫á‡∫≠‡∫µ‡∫á:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "                **1.** `Finetuing vs RAG.pdf` (‡ªú‡ªâ‡∫≤ 0) - ‡∫Ñ‡∫ß‡∫≤‡∫°‡∫Ñ‡ªâ‡∫≤‡∫ç‡∫Ñ‡∫∑: `0.197`\n",
       "                > 1 Huawei Confidential\n",
       "How to Develop a Professional Domain Knowledge Q&A AssistantÔºü\n",
       "Implementation\n",
       "Data Preparation: Collect a large amount of text data. These \n",
       "data need to be cleaned and annotated to ensure quality.\n",
       "Model Selection: Choose a suitable pre-trained model.\n",
       "Model Training: Input the prepared data into the model and \n",
       "adjust its parameters.\n",
       "Model Deployment: Deploy the trained model for inference.\n",
       "Advantages\n",
       "End-to-End Generation: The model can directly generate \n",
       "answers based on input questions. \n",
       "Disadvantages\n",
       "High Training Costs: Time-consuming and costly by \n",
       "requiring a large amount of data and computational resources.\n",
       "Difficulty in Knowledge Update: Once the model is trained, \n",
       "updating the knowledge requires heavy re -fine-tuning.\n",
       "Fine-tuning RAG\n",
       "Implementation\n",
       "Data Preparation: Gather a wide range of documents relevant \n",
       "to the professional domain and clean them.\n",
       "Knowledge Base Construction: Build a structured knowledge \n",
       "base that can be efficiently queried by the retrieval module.\n",
       "Model Deployment: Deploy a LLM as expected (deepseek).\n",
       "RAG Application Development: Develop a RAG application \n",
       "that integrates retrieval and generation to answer user queries.\n",
       "Advantages\n",
       "Flexible Knowledge Update: The knowledge base can be \n",
       "updated at any time without retraining the model.\n",
       "Integration of Latest Information: The system can generate \n",
       "more accurate and up -to-date answers.\n",
       "Scalability: It is easy to expand the content and scope of the \n",
       "knowledge base.\n",
       "Disadvantages\n",
       "Generation Delay: The added retrieval step may slow down \n",
       "the system's response time.\n",
       "The Fine-tuning approach is suitable for fields where knowledge accuracy and consistency are highly valued and knowledge updates\n",
       "are relatively slow, such as some traditional academic research areas. \n",
       "In contrast, the RAG approach is more suitable for fields where knowledge updates rapidly and quick responses are required, such as \n",
       "finance, healthcare, and technology....\n",
       "\n",
       "            \n",
       "                **2.** `Finetuing vs RAG.pdf` (‡ªú‡ªâ‡∫≤ 1) - ‡∫Ñ‡∫ß‡∫≤‡∫°‡∫Ñ‡ªâ‡∫≤‡∫ç‡∫Ñ‡∫∑: `-0.168`\n",
       "                > 2 Huawei Confidential\n",
       "ModelEngine: AI Toolchain that Accelerates the Implementation of AI Applications\n",
       "40% NPU pooling usage\n",
       "60%+ faster multimodal data cleansing\n",
       "1.5x online concurrency\n",
       "1.6x offline throughput\n",
       "Lower costs Fast application rollout Good inference performance\n",
       "95% accuracy\n",
       "50% shorter development latency\n",
       "Operator \n",
       "ecosystem\n",
       "ModelEngine\n",
       "Full-process AI \n",
       "toolchain\n",
       "Application enablement\n",
       "High-precision RAG application \n",
       "development and optimization\n",
       "Open toolchain Open-source framework, supporting third-party operators\n",
       "Model enablement\n",
       "Lightweight model inference \n",
       "toolchain\n",
       "Data enablement\n",
       "Automatic data processing and \n",
       "knowledge generation\n",
       "API API\n",
       "Training and inference offload acceleration\n",
       "Cache offload acceleration | Ascend+Kunpeng\n",
       "heterogeneous computing\n",
       "Retrieval acceleration\n",
       "Converged retrieval of multimodal data | Vector \n",
       "retrieval acceleration\n",
       "Low-code toolchain\n",
       "Self-orchestrated data processing | Modular \n",
       "RAG\n",
       "NPU basic software Container platform Knowledge base storage\n",
       "Model ecosystem OpenMind HuggingFace Blue Whale Market LangChain LlamaIndex\n",
       "GPU processor | Ascend NPU processor\n",
       "Resource enablement\n",
       "AI task scheduling and XPU pooling...\n",
       "\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## üí¨ ‡ªÇ‡ªù‡∫î Interactive - ‡∫û‡∫¥‡∫°‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫Ç‡∫≠‡∫á‡∫ó‡ªà‡∫≤‡∫ô (‡∫û‡∫¥‡∫° 'quit' ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫≠‡∫≠‡∫Å"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëã ‡∫Ç‡∫≠‡∫ö‡ªÉ‡∫à‡∫ó‡∫µ‡ªà‡ªÉ‡∫ä‡ªâ‡∫•‡∫∞‡∫ö‡∫ª‡∫ö RAG!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
