{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading 1 embedding model(s):\n",
      "  1. cross-encoder/ms-marco-TinyBERT-L6\n",
      "\n",
      "[1/1] Processing: cross-encoder/ms-marco-TinyBERT-L6\n",
      "  üìÑ Downloading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4582e11090d4e66b2d6d7b2b808fe93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code AI Before Sleep\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\Code AI Before Sleep\\Model\\cross-encoder_ms-marco-TinyBERT-L6\\models--cross-encoder--ms-marco-TinyBERT-L6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4230b55e79a4ed8a23cfeac90de9bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c398f1863f47e1bf9878f98fc21ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8321e2824e485cbc5dfc9e47d4dee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ü§ñ Downloading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7421a15a9254c2a8f753d4c9b0b4773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71abb47a82b461eb2ff040aeeee6203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Successfully downloaded to: D:\\Code AI Before Sleep\\Model\\cross-encoder_ms-marco-TinyBERT-L6\n",
      "\n",
      "üéâ Download Summary:\n",
      "  ‚úÖ Successfully downloaded: 1/1 models\n",
      "  üìÅ Saved to: D:\\Code AI Before Sleep\\Model\n",
      "  üìã Downloaded models:\n",
      "    - cross-encoder/ms-marco-TinyBERT-L6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "from typing import Optional, List\n",
    "\n",
    "def download_embedding_models(\n",
    "    model_count: int = 1,\n",
    "    save_directory: str = r\"D:\\Code AI Before Sleep\\Model\",\n",
    "    force_download: bool = False,\n",
    "    use_auth_token: Optional[str] = None,\n",
    "    custom_models: Optional[List[str]] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Download embedding models from Hugging Face based on count specified.\n",
    "    \n",
    "    Args:\n",
    "        model_count (int): Number of models to download (1-10)\n",
    "        save_directory (str): Directory to save the models\n",
    "        force_download (bool): Whether to force re-download even if model exists\n",
    "        use_auth_token (str, optional): Hugging Face authentication token\n",
    "        custom_models (List[str], optional): Custom list of models to choose from\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of paths to downloaded models\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default popular embedding models (ordered by popularity/usefulness)\n",
    "    default_models = [\n",
    "        \"sentence-transformers/all-MiniLM-L6-v2\",           # 1. Fast and efficient\n",
    "        \"sentence-transformers/all-mpnet-base-v2\",          # 2. High quality\n",
    "        \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",  # 3. Multilingual\n",
    "        \"sentence-transformers/all-distilroberta-v1\",       # 4. Good balance\n",
    "        \"sentence-transformers/all-MiniLM-L12-v2\",          # 5. Better than L6\n",
    "        \"sentence-transformers/multi-qa-mpnet-base-dot-v1\", # 6. Q&A optimized\n",
    "        \"sentence-transformers/paraphrase-MiniLM-L6-v2\",    # 7. Paraphrase detection\n",
    "        \"sentence-transformers/msmarco-distilbert-base-v4\", # 8. Search optimized\n",
    "        \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\",  # 9. Multi-domain Q&A\n",
    "        \"sentence-transformers/all-roberta-large-v1\"        # 10. Large model\n",
    "    ]\n",
    "    \n",
    "    # Use custom models if provided, otherwise use default\n",
    "    available_models = custom_models if custom_models else default_models\n",
    "    \n",
    "    # Validate model count\n",
    "    if model_count < 1:\n",
    "        model_count = 1\n",
    "    elif model_count > len(available_models):\n",
    "        model_count = len(available_models)\n",
    "        print(f\"‚ö†Ô∏è  Requested {model_count} models, but only {len(available_models)} available. Downloading all.\")\n",
    "    \n",
    "    # Select models to download\n",
    "    models_to_download = available_models[:model_count]\n",
    "    \n",
    "    print(f\"üì• Downloading {model_count} embedding model(s):\")\n",
    "    for i, model in enumerate(models_to_download, 1):\n",
    "        print(f\"  {i}. {model}\")\n",
    "    print()\n",
    "    \n",
    "    # Create save directory\n",
    "    save_path = Path(save_directory)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    downloaded_paths = []\n",
    "    successful_downloads = 0\n",
    "    \n",
    "    for i, model_name in enumerate(models_to_download, 1):\n",
    "        try:\n",
    "            print(f\"[{i}/{model_count}] Processing: {model_name}\")\n",
    "            \n",
    "            # Create model-specific directory\n",
    "            model_dir_name = model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "            model_save_path = save_path / model_dir_name\n",
    "            \n",
    "            # Check if model already exists\n",
    "            if model_save_path.exists() and not force_download:\n",
    "                print(f\"  ‚úì Model already exists, skipping download\")\n",
    "                downloaded_paths.append(str(model_save_path))\n",
    "                successful_downloads += 1\n",
    "                continue\n",
    "            \n",
    "            model_save_path.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Download tokenizer\n",
    "            print(f\"  üìÑ Downloading tokenizer...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                token=use_auth_token,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=str(model_save_path)\n",
    "            )\n",
    "            tokenizer.save_pretrained(model_save_path)\n",
    "            \n",
    "            # Download model\n",
    "            print(f\"  ü§ñ Downloading model...\")\n",
    "            model = AutoModel.from_pretrained(\n",
    "                model_name,\n",
    "                token=use_auth_token,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=str(model_save_path)\n",
    "            )\n",
    "            model.save_pretrained(model_save_path)\n",
    "            \n",
    "            # Save basic model info\n",
    "            model_info = {\n",
    "                \"model_name\": model_name,\n",
    "                \"model_path\": str(model_save_path),\n",
    "                \"tokenizer_vocab_size\": tokenizer.vocab_size if hasattr(tokenizer, 'vocab_size') else None\n",
    "            }\n",
    "            \n",
    "            with open(model_save_path / \"model_info.json\", \"w\") as f:\n",
    "                json.dump(model_info, f, indent=2)\n",
    "            \n",
    "            print(f\"  ‚úÖ Successfully downloaded to: {model_save_path}\")\n",
    "            downloaded_paths.append(str(model_save_path))\n",
    "            successful_downloads += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error downloading {model_name}: {str(e)}\")\n",
    "            # Clean up partial download\n",
    "            if 'model_save_path' in locals() and model_save_path.exists():\n",
    "                import shutil\n",
    "                shutil.rmtree(model_save_path)\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nüéâ Download Summary:\")\n",
    "    print(f\"  ‚úÖ Successfully downloaded: {successful_downloads}/{model_count} models\")\n",
    "    print(f\"  üìÅ Saved to: {save_directory}\")\n",
    "    \n",
    "    if downloaded_paths:\n",
    "        print(f\"  üìã Downloaded models:\")\n",
    "        for path in downloaded_paths:\n",
    "            model_name = Path(path).name.replace(\"_\", \"/\")\n",
    "            print(f\"    - {model_name}\")\n",
    "    \n",
    "    return downloaded_paths\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Download 1 model\n",
    "    # download_embedding_models(model_count=1)\n",
    "    \n",
    "    # Download 2 models\n",
    "    # download_embedding_models(model_count=2)\n",
    "    \n",
    "    # Download 5 models\n",
    "    # download_embedding_models(model_count=5)\n",
    "\n",
    "    # ‡ªÄ‡∫à‡∫ª‡ªâ‡∫≤‡∫™‡∫≤‡∫°‡∫≤‡∫î‡ªÄ‡∫•‡∫∑‡∫≠‡∫Å‡∫Æ‡∫π‡∫ö‡ªÅ‡∫ö‡∫ö‡∫Å‡∫≤‡∫ô‡∫î‡∫≤‡∫ß‡ªÇ‡∫´‡∫•‡∫î‡ªÑ‡∫î‡ªâ ‡ªÄ‡∫à‡∫ª‡ªâ‡∫≤‡∫≠‡∫≤‡∫î‡∫à‡∫∞ ‡∫™‡∫≠‡∫á model ‡∫Ç‡∫∂‡ªâ‡∫ô‡ªÑ‡∫õ ‡ªÄ‡∫à‡∫ª‡ªâ‡∫≤‡∫Å‡ªç‡ªÅ‡∫Ñ‡ªà‡∫Å‡∫≥‡ªú‡∫ª‡∫î‡∫ä‡∫∑‡ªà‡ªÉ‡∫ô array ‡∫•‡∫∞‡∫õ‡ªà‡∫Ω‡∫ô mode_count ‡ªÄ‡∫õ‡∫±‡∫ô‡∫à‡∫≥‡∫ô‡∫ß‡∫ô‡∫ó‡∫µ‡ªà‡∫ï‡ªâ‡∫≠‡∫á‡∫Å‡∫≤‡∫ô‡∫î‡∫≤‡∫ß‡ªÇ‡∫´‡∫•‡∫î\n",
    "    \n",
    "    # Download custom models\n",
    "    custom_list = [\n",
    "        \"cross-encoder/ms-marco-TinyBERT-L6\", \n",
    "        # \"cross-encoder/ms-marco-MiniLM-L12-v2\", \n",
    "        # \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \n",
    "        # \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    ]\n",
    "    hf_token= os.getenv(\"HuggingFaceToken\")\n",
    "    download_embedding_models(model_count=1, custom_models=custom_list, use_auth_token=hf_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
