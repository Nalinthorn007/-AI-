{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7d68c",
   "metadata": {},
   "source": [
    "### ðŸ§  What is Query Decomposition?\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### âœ… Why Use Query Decomposition?\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23a442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76de0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"D:\\\\model\\\\BAAI-bge-m3\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43149d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000218935B06E0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000218935B12B0>, model_name='meta-llama/llama-4-maverick-17b-128e-instruct', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"groq:meta-llama/llama-4-maverick-17b-128e-instruct\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0af1982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c9797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b4819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To tackle the complex question, \"How does LangChain use memory and agents compared to CrewAI?\" we can break it down into smaller, more manageable sub-questions. Here's a decomposition into 3 sub-questions that can facilitate better document retrieval:\n",
      "\n",
      "1. **What is LangChain and how does it utilize memory and agents?**\n",
      "   - This sub-question aims to understand the core functionalities of LangChain, particularly focusing on its use of memory and agents. It will help in gathering information on how LangChain operates.\n",
      "\n",
      "2. **What is CrewAI and how does it utilize memory and agents?**\n",
      "   - Similarly, this sub-question seeks to understand CrewAI's core functionalities, especially its use of memory and agents. It will provide insights into how CrewAI operates.\n",
      "\n",
      "3. **What are the differences in the utilization of memory and agents between LangChain and CrewAI?**\n",
      "   - This sub-question directly compares the two, focusing on the differences in their approaches to using memory and agents. It will help in synthesizing the information gathered from the first two sub-questions to draw a comparison.\n",
      "\n",
      "By answering these sub-questions, we can effectively gather the necessary information to address the original complex question.\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5be04719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26c735b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-â€¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()] \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq) \n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs}) \n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac50f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Answer:\n",
      "\n",
      "Q: To tackle the complex question, \"How does LangChain use memory and agents compared to CrewAI?\", we can break it down into smaller, more manageable sub-questions. Here are 2 to 4 sub-questions that can facilitate better document retrieval:\n",
      "A: To tackle the complex question, \"How does LangChain use memory and agents compared to CrewAI?\", we can break it down into the following sub-questions:\n",
      "\n",
      "1. **How does LangChain utilize memory in its workflows?** - This sub-question focuses on understanding the role of memory within LangChain, particularly how it is used across different components like agents and chains.\n",
      "\n",
      "2. **What is the role of agents in LangChain, and how do they operate?** - This sub-question aims to delve into the specifics of LangChain agents, including their planner-executor model, dynamic decision-making, and how they use tools and memories.\n",
      "\n",
      "3. **How does CrewAI manage role-based collaboration, and how does it integrate with LangChain agents and tools?** - This sub-question is designed to explore CrewAI's capabilities in managing collaboration among different roles and its compatibility with LangChain components.\n",
      "\n",
      "4. **What are the key differences between LangChain and CrewAI in terms of their architecture and functionality?** - This sub-question seeks to directly compare and contrast LangChain and CrewAI, focusing on their different approaches to using memory, agents, and collaboration.\n",
      "\n",
      "These sub-questions can facilitate a more structured and comprehensive understanding of how LangChain and CrewAI work, both individually and in hybrid systems, thereby addressing the original complex question.\n",
      "\n",
      "Q: **What is LangChain and how does it utilize memory in its operations?**\n",
      "A: LangChain is an open-source framework designed for developing applications powered by large language models (LLMs). It simplifies the process of building, managing, and scaling complex chains of thought by abstracting various aspects such as prompt management, retrieval, memory, and agent orchestration.\n",
      "\n",
      "LangChain utilizes memory in its operations through modules like ConversationBufferMemory and ConversationSummaryMemory. These memory modules enable the LLM to maintain awareness of previous conversation turns or summarize long interactions, ensuring that the conversation stays within token limits. This allows the LLM to have context-aware interactions, making it more effective in its responses.\n",
      "\n",
      "Q: This sub-question aims to understand the fundamental aspects of LangChain, particularly its use of memory. Retrieving documents that explain LangChain's architecture and its memory usage will be crucial\n",
      "A: The context does not directly mention LangChain's use of memory. However, it does describe the concept of \"chains\" and their composition. To answer the question, we would need additional context or information that explains LangChain's memory usage. \n",
      "\n",
      "Based on the given context, we can't directly infer the details about LangChain's memory usage. The context mainly focuses on the concept of chains, their complexity, and the templating capabilities provided by LangChain for prompt engineering. \n",
      "\n",
      "Therefore, the given context is insufficient to understand LangChain's use of memory. We would need to retrieve additional documents or information that explain LangChain's architecture and its memory usage, as the question suggests.\n",
      "\n",
      "Q: **How does LangChain implement agents, and what roles do they play?**\n",
      "A: LangChain implements agents using a planner-executor model. The agent plans out a sequence of tool invocations to achieve a goal. The roles of LangChain agents include:\n",
      "\n",
      "1. Planning: Agents plan a sequence of tool invocations to achieve a goal.\n",
      "2. Dynamic Decision-Making: Agents make decisions dynamically based on the context.\n",
      "3. Branching Logic: Agents use branching logic to adapt to different scenarios.\n",
      "4. Context-Aware Memory Use: Agents use memory in a context-aware manner across steps.\n",
      "5. Tool Invocation and Integration: Agents integrate with various tools like web search, calculators, and code execution, and reason about which tool to call, what input to provide, and how to process the output using LLMs.\n",
      "\n",
      "In summary, LangChain agents play a crucial role in executing multi-step tasks, making decisions, and adapting to different scenarios, ultimately achieving a specific goal.\n",
      "\n",
      "Q: This sub-question focuses on the concept of agents within LangChain, their functionalities, and their significance. Documents that detail LangChain's agent implementation will be essential for answering this\n",
      "A: To answer the question, we can directly refer to the provided context.\n",
      "\n",
      "The context highlights that one of the standout features of LangChain is its support for agents. Here's a summary relevant to the question:\n",
      "\n",
      "1. **Functionalities of LangChain Agents**: \n",
      "   - Agents use Large Language Models (LLMs) to reason about which tool to call.\n",
      "   - They decide on the input to provide to the chosen tool.\n",
      "   - They determine how to process the output from the tool.\n",
      "   - LangChain agents can execute multi-step tasks.\n",
      "   - They can integrate with various tools such as web search, calculators, and code execution environments.\n",
      "\n",
      "2. **Operational Model**:\n",
      "   - LangChain agents operate using a planner-executor model.\n",
      "   - The planner plans out a sequence of tool invocations to achieve a specific goal.\n",
      "   - This model allows for dynamic decision-making, branching logic, and context-aware memory use across different steps.\n",
      "\n",
      "3. **Significance**:\n",
      "   - The ability to execute multi-step tasks and make decisions based on the output of previous steps signifies the advanced automation capability of LangChain agents.\n",
      "   - Their integration with various tools enhances their versatility and utility in different applications.\n",
      "\n",
      "In summary, LangChain's agents are significant because they leverage LLMs to autonomously plan and execute complex tasks by integrating with a variety of tools, thereby showcasing a sophisticated level of automation and decision-making. Documents detailing LangChain's agent implementation would be crucial for understanding the specifics of how these functionalities are achieved and how they can be applied or customized.\n",
      "\n",
      "Q: **What are the key features of CrewAI regarding memory and agents?**\n",
      "A: Actually, the context doesn't mention \"memory\" at all. However, based on the provided context, we can infer some key features of CrewAI regarding agents. \n",
      "\n",
      "The key features of CrewAI regarding agents are: \n",
      "1. **Multi-agent orchestration framework**: CrewAI is designed to build collaborative LLM-powered agents.\n",
      "2. **Agent collaboration**: CrewAI enables developers to structure agents into organized crews that work together to complete tasks.\n",
      "3. **Context-sharing**: Agents pass intermediate data to one another in a structured manner, leading to emergent behaviors like delegation, consultation, and review among agents.\n",
      "4. **Dynamic communication**: Agents dynamically communicate with one another.\n",
      "\n",
      "These features allow CrewAI to scale both horizontally (more agents) and vertically (more reasoning depth).\n",
      "\n",
      "Q: This sub-question shifts the focus to CrewAI, seeking to understand how it employs memory and agents. Relevant documents should outline CrewAI's architecture, highlighting its use of memory and the role of agents\n",
      "A: Based on the context, here is the answer to the question:\n",
      "\n",
      "CrewAI employs a structured workflow architecture where agents with defined roles (such as researcher, planner, or executor) operate semi-independently within a collaborative context. The agents utilize context-sharing to pass intermediate data to one another in a structured manner, enabling emergent behaviors like delegation, consultation, and review among agents.\n",
      "\n",
      "While the context does not directly mention \"memory\", it can be inferred that the context-sharing mechanism among agents serves as a form of memory or information storage, allowing agents to access and build upon the data generated by other agents.\n",
      "\n",
      "Therefore, CrewAI's architecture highlights the role of agents in a collaborative context, and the context-sharing mechanism can be seen as a form of memory that enables the agents to work together effectively.\n",
      "\n",
      "Q: **What are the comparative differences in how LangChain and CrewAI utilize memory and agents?**\n",
      "A: To address the question regarding the comparative differences in how LangChain and CrewAI utilize memory and agents, we need to analyze the provided context.\n",
      "\n",
      "1. **LangChain's Utilization of Memory and Agents:**\n",
      "   - LangChain agents operate using a planner-executor model. This model involves planning out a sequence of tool invocations to achieve a specific goal.\n",
      "   - The planner-executor model includes features like dynamic decision-making, branching logic, and notably, context-aware memory use across steps. This indicates that LangChain utilizes memory in a context-aware manner, adapting its use based on the steps taken towards achieving a goal.\n",
      "\n",
      "2. **CrewAI's Utilization of Memory and Agents:**\n",
      "   - CrewAI is highlighted for its use of agent context-sharing. This is a mechanism where agents pass intermediate data to one another in a structured manner.\n",
      "   - The context-sharing leads to emergent behaviors such as delegation, consultation, and review among agents. While the context doesn't directly state how CrewAI utilizes memory, the structured passing of intermediate data implies a form of memory or data retention and utilization across different agents.\n",
      "\n",
      "3. **Comparative Differences:**\n",
      "   - **Memory Use:** LangChain is explicitly mentioned to use context-aware memory across steps in its planner-executor model. CrewAI, on the other hand, implies memory or data retention through its context-sharing mechanism among agents. The key difference lies in how memory is utilized: LangChain focuses on context-aware memory within its planning and execution steps, whereas CrewAI emphasizes sharing data (which can be considered a form of memory) among its agents.\n",
      "   - **Agent Utilization:** LangChain agents are part of a planner-executor model, focusing on planning and executing tasks. CrewAI agents are centered around context-sharing and role-based collaboration, enabling complex behaviors like delegation and consultation.\n",
      "\n",
      "In summary, while both LangChain and CrewAI utilize agents and some form of memory or data retention, they differ in their approaches. LangChain focuses on a planner-executor model with context-aware memory, whereas CrewAI emphasizes role-based collaboration and context-sharing among agents, leading to emergent behaviors. CrewAI manages role-based collaboration, and LangChain handles retrieval and tool wrapping in their hybrid systems.\n",
      "\n",
      "Q: This sub-question directly addresses the comparative aspect of the original question. It requires documents or analyses that contrast the two systems, specifically in terms of memory and agent utilization\n",
      "A: To address the question regarding the comparative aspect, specifically in terms of memory and agent utilization between LangChain and CrewAI, let's analyze the provided context.\n",
      "\n",
      "1. **Memory Utilization**: The context provided doesn't directly compare the memory utilization of LangChain and CrewAI. However, it does mention that LangChain agents use \"context-aware memory use across steps\" (v2). This implies that LangChain has a mechanism for utilizing memory in a way that is aware of the context across different steps of a task. There's no direct information on how CrewAI handles memory.\n",
      "\n",
      "2. **Agent Utilization**: \n",
      "   - **LangChain Agents**: LangChain is highlighted for its support for agents that can execute multi-step tasks and integrate with various tools. The agents use LLMs (Large Language Models) to reason about tool invocation, input provision, and output processing. This indicates a sophisticated level of agent utilization, enabling complex tasks through dynamic decision-making and branching logic.\n",
      "   - **CrewAI Agents**: CrewAI is noted for its innovation in agent context-sharing. This allows agents to pass intermediate data to each other in a structured manner, leading to emergent behaviors like delegation, consultation, and review. This suggests that CrewAI's agents are designed to work collaboratively, leveraging the shared context to achieve complex goals.\n",
      "\n",
      "**Comparison**:\n",
      "- Both frameworks support advanced agent utilization, but they highlight different aspects. LangChain focuses on the ability of its agents to execute multi-step tasks and integrate with various tools, suggesting a strong capability in handling complex tasks through individual agent reasoning.\n",
      "- CrewAI, on the other hand, emphasizes the collaborative aspect of its agents, with context-sharing enabling emergent behaviors that suggest a more distributed or cooperative approach to task-solving.\n",
      "\n",
      "**In terms of memory and agent utilization**, while direct comparative details on memory are lacking, it's clear that both systems have advanced features for agent utilization. LangChain seems to focus on individual agent capabilities and context-aware processing, whereas CrewAI highlights the benefits of inter-agent collaboration and context-sharing.\n",
      "\n",
      "To directly compare their memory utilization, further information or specific analyses comparing how each framework manages memory would be necessary. However, the context provided suggests that both are equipped with sophisticated mechanisms for agent utilization, tailored to different aspects of complex task-solving.\n",
      "\n",
      "Q: By answering these sub-questions, we can gather the necessary information to comprehensively address the original complex question, providing a detailed comparison between LangChain and CrewAI regarding their use of memory and agents\n",
      "A: Based on the given context, we can infer some information about the relationship between LangChain and CrewAI. However, the context does not directly address the use of memory by either LangChain or CrewAI, nor does it provide a detailed comparison between them regarding their use of memory and agents.\n",
      "\n",
      "Here's what's known from the context:\n",
      "1. CrewAI is compatible with LangChain agents and tools.\n",
      "2. CrewAI and LangChain can be used in a hybrid system where LangChain handles retrieval and tool wrapping, and CrewAI manages role-based collaboration.\n",
      "\n",
      "To comprehensively address the original complex question about the comparison between LangChain and CrewAI regarding their use of memory and agents, we need to infer or find additional information not provided in the given context.\n",
      "\n",
      "From the given context, we can deduce that:\n",
      "- CrewAI manages role-based collaboration.\n",
      "- LangChain handles retrieval and tool wrapping in a hybrid system with CrewAI.\n",
      "\n",
      "However, the context does not provide explicit details on:\n",
      "- How LangChain and CrewAI individually use memory.\n",
      "- A detailed comparison of their use of agents.\n",
      "\n",
      "To directly answer the question based on the context provided and the sub-questions posed, we would need more specific information about the memory usage and agent utilization of both LangChain and CrewAI. The context suggests a cooperative or hybrid model between LangChain and CrewAI but lacks the necessary details for a comprehensive comparison.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"âœ… Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
