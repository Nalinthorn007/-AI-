{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a23eff",
   "metadata": {},
   "source": [
    "### Reranking Hybrid Search Statergies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5fac5",
   "metadata": {},
   "source": [
    "Re-ranking is a second-stage filtering process in retrieval systems, especially in RAG pipelines, where we:\n",
    "\n",
    "1. First use a fast retriever (like BM25, FAISS, hybrid) to fetch top-k documents quickly.\n",
    "\n",
    "2. Then use a more accurate but slower model (like a cross-encoder or LLM) to re-score and reorder those documents by relevance to the query.\n",
    "\n",
    "üëâ It ensures that the most relevant documents appear at the top, improving the final answer from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5fd53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain_core.output_parsers import StrOutputParser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf8243d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load text file\n",
    "loader=TextLoader(\"langchain_sample.txt\")\n",
    "raw_docs=loader.load()\n",
    "\n",
    "# Split text into document chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b203af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## user query\n",
    "query=\"‡∫ß‡∫¥‡∫ó‡∫µ‡∫Å‡∫≤‡∫ô‡ªÉ‡∫ä‡ªâ LangChain ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫™‡ªâ‡∫≤‡∫á‡ªÅ‡∫≠‡∫±‡∫ö application ‡∫ó‡∫µ‡ªà‡∫°‡∫µ‡∫Ñ‡∫ß‡∫≤‡∫°‡∫à‡∫≥ ‡ªÅ‡∫•‡∫∞ ‡ªÄ‡∫Ñ‡∫∑‡ªà‡∫≠‡∫á‡∫°‡∫∑ ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d9720bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAISS and Huggingface model Embeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"D:\\\\model\\\\BAAI-bge-m3\")\n",
    "vectorstore=FAISS.from_documents(docs,embedding_model)\n",
    "retriever=vectorstore.as_retriever(search_kwargs={\"k\":6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "332edeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HuggingFace Embedding \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "## Initialize a simple Embedding model(no API Key needed!)\n",
    "embeddings=HuggingFaceEmbeddings(\n",
    "    model_name=\"D:\\\\model\\\\BAAI-bge-m3\"\n",
    ")   \n",
    "vectorstore_hugging=FAISS.from_documents(docs,embeddings)\n",
    "vectorstore_hugging=vectorstore_hugging.as_retriever(search_kwargs={\"k\":6})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7e52001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000274DBEB0CB0>, search_kwargs={'k': 6})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "49fc0a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000274DBE89150>, search_kwargs={'k': 6})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_hugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5cd8773a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000274DBEB1D90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000274DBEB2690>, model_name='meta-llama/llama-4-maverick-17b-128e-instruct', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=2000)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## prompt and use the llm\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=init_chat_model(\"groq:meta-llama/llama-4-maverick-17b-128e-instruct\",max_tokens=2000)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2b0c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user's question.\n",
    "\n",
    "User Question: \"{question}\"\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\n",
    "Instructions:\n",
    "- Think about the relevance of each document to the user's question.\n",
    "- Return a list of document indices in ranked order, starting from the most relevant.\n",
    "\n",
    "Important: Response in Lao Languae only                                     \n",
    "\n",
    "Output format: comma-separated document indices (e.g., 2,1,3,0,...)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8eed561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='37722509-db76-486a-b5ff-b9fb7dba5544', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='91875ee3-cdf2-45a3-8938-7abc230f320d', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='8e047901-e13d-4ec1-bae3-32488811979c', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='390e44b0-495b-42b3-855a-797141e90dcb', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='d97141d4-1fa9-4905-9288-d9bccb4e4834', metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(id='3e128c94-08f8-4ab5-80fd-58d70952d708', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs=retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ce0eeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user\\'s question.\\n\\nUser Question: \"{question}\"\\n\\nDocuments:\\n{documents}\\n\\nInstructions:\\n- Think about the relevance of each document to the user\\'s question.\\n- Return a list of document indices in ranked order, starting from the most relevant.\\n\\nImportant: Response in Lao Languae only                                     \\n\\nOutput format: comma-separated document indices (e.g., 2,1,3,0,...)\\n')\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000274DBEB1D90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000274DBEB2690>, model_name='meta-llama/llama-4-maverick-17b-128e-instruct', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=2000)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=prompt| llm | StrOutputParser()\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e3278ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lines = [f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(retrieved_docs)]\n",
    "formatted_docs = \"\\n\".join(doc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0ecef652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.',\n",
       " '2. LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.',\n",
       " '3. LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.',\n",
       " '4. FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.',\n",
       " '5. Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.',\n",
       " '6. Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c9882543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\\n2. LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.\\n3. LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\\n4. FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\\n5. Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.\\n6. Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c7e76228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫ï‡∫≠‡∫ö‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°‡∫Ç‡∫≠‡∫á‡∫ú‡∫π‡ªâ‡ªÉ‡∫ä‡ªâ \"‡∫ß‡∫¥‡∫ó‡∫µ‡∫Å‡∫≤‡∫ô‡ªÉ‡∫ä‡ªâ LangChain ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫™‡ªâ‡∫≤‡∫á‡ªÅ‡∫≠‡∫±‡∫ö application ‡∫ó‡∫µ‡ªà‡∫°‡∫µ‡∫Ñ‡∫ß‡∫≤‡∫°‡∫à‡∫≥ ‡ªÅ‡∫•‡∫∞ ‡ªÄ‡∫Ñ‡∫∑‡ªà‡∫≠‡∫á‡∫°‡∫∑ ?\", ‡∫Ç‡ªâ‡∫≠‡∫ç‡∫à‡∫∞‡∫ï‡ªâ‡∫≠‡∫á‡∫ß‡∫¥‡ªÄ‡∫Ñ‡∫≤‡∫∞‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫ó‡∫µ‡ªà‡ªÉ‡∫´‡ªâ‡∫°‡∫≤‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫à‡∫±‡∫î‡∫≠‡∫±‡∫ô‡∫î‡∫±‡∫ö‡∫Ñ‡∫ß‡∫≤‡∫°‡∫Å‡ªà‡∫Ω‡∫ß‡∫Ç‡ªâ‡∫≠‡∫á.\\n\\n‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô 1 ‡ªÑ‡∫î‡ªâ‡∫Å‡ªà‡∫≤‡∫ß‡ªÄ‡∫ñ‡∫¥‡∫á LangChain ‡∫ß‡ªà‡∫≤‡ªÄ‡∫õ‡∫±‡∫ô‡∫Å‡∫≠‡∫ö‡∫Å‡∫≤‡∫ô‡∫û‡∫±‡∫î‡∫ó‡∫∞‡∫ô‡∫≤‡ªÅ‡∫≠‡∫±‡∫ö‡∫û‡∫•‡∫¥‡ªÄ‡∫Ñ‡∫ä‡∫±‡∫ô‡ªÇ‡∫î‡∫ç‡ªÉ‡∫ä‡ªâ LLMs, ‡∫°‡∫µ‡ªÄ‡∫Ñ‡∫∑‡ªà‡∫≠‡∫á‡∫°‡∫∑ ‡ªÅ‡∫•‡∫∞ ‡∫≠‡∫ª‡∫á‡∫õ‡∫∞‡∫Å‡∫≠‡∫ö‡∫™‡ªç‡∫≤‡∫•‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫à‡∫±‡∫î‡∫Å‡∫≤‡∫ô‡∫Ñ‡∫ß‡∫≤‡∫°‡∫ä‡∫ª‡∫á‡∫à‡∫≥. ‡∫°‡∫±‡∫ô‡ªÅ‡∫°‡ªà‡∫ô‡∫Ñ‡∫ß‡∫≤‡∫°‡∫Å‡ªà‡∫Ω‡∫ß‡∫Ç‡ªâ‡∫≠‡∫á‡∫Å‡∫±‡∫ö‡∫Ñ‡∫≥‡∫ñ‡∫≤‡∫°.\\n\\n‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô 3 ‡∫Å‡ªà‡∫≤‡∫ß‡ªÄ‡∫ñ‡∫¥‡∫á‡∫Ñ‡∫ß‡∫≤‡∫°‡∫™‡∫≤‡∫°‡∫≤‡∫î‡∫Ç‡∫≠‡∫á LangChain ‡ªÉ‡∫ô‡∫Å‡∫≤‡∫ô‡∫•‡∫ß‡∫°‡ªÄ‡∫≠‡∫ª‡∫≤‡ªÄ‡∫Ñ‡∫∑‡ªà‡∫≠‡∫á‡∫°‡∫∑‡∫û‡∫≤‡∫ç‡∫ô‡∫≠‡∫Å ‡ªÅ‡∫•‡∫∞ ‡∫Ñ‡∫ß‡∫≤‡∫°‡∫à‡∫≥, ‡ªÄ‡∫ä‡∫¥‡ªà‡∫á‡ªÄ‡∫õ‡∫±‡∫ô‡∫™‡ªà‡∫ß‡∫ô‡∫´‡∫ô‡∫∂‡ªà‡∫á‡∫ó‡∫µ‡ªà‡∫™‡ªç‡∫≤‡∫Ñ‡∫±‡∫ô‡∫Ç‡∫≠‡∫á‡∫Ñ‡ªç‡∫≤‡∫ñ‡∫≤‡∫°.\\n\\n‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô 4 ‡∫≠‡∫∞‡∫ó‡∫¥‡∫ö‡∫≤‡∫ç‡ªÄ‡∫ñ‡∫¥‡∫á Agents ‡ªÉ‡∫ô LangChain, ‡ªÄ‡∫ä‡∫¥‡ªà‡∫á‡∫™‡∫≤‡∫°‡∫≤‡∫î‡ªÉ‡∫ä‡ªâ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫ï‡∫±‡∫î‡∫™‡∫¥‡∫ô‡ªÉ‡∫à‡∫ß‡ªà‡∫≤‡ªÄ‡∫Ñ‡∫∑‡ªà‡∫≠‡∫á‡∫°‡∫∑‡ªÉ‡∫î‡∫ó‡∫µ‡ªà‡∫à‡∫∞‡ªÉ‡∫ä‡ªâ ‡ªÅ‡∫•‡∫∞ ‡ªÉ‡∫ô‡∫•‡ªç‡∫≤‡∫î‡∫±‡∫ö‡ªÉ‡∫î, ‡ªÄ‡∫ä‡∫¥‡ªà‡∫á‡∫Å‡ªà‡∫Ω‡∫ß‡∫Ç‡ªâ‡∫≠‡∫á‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫ô‡ªç‡∫≤‡ªÉ‡∫ä‡ªâ LangChain ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫™‡ªâ‡∫≤‡∫á‡ªÅ‡∫≠‡∫±‡∫ö‡∫û‡∫•‡∫¥‡ªÄ‡∫Ñ‡∫ä‡∫±‡∫ô.\\n\\n‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô 2 ‡ªÅ‡∫•‡∫∞ 6 ‡ªÉ‡∫´‡ªâ‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫Å‡ªà‡∫Ω‡∫ß‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫õ‡∫∞‡∫™‡∫ª‡∫°‡∫õ‡∫∞‡∫™‡∫≤‡∫ô‡∫Å‡∫±‡∫ö‡∫ö‡ªç‡∫•‡∫¥‡∫Å‡∫≤‡∫ô‡∫û‡∫≤‡∫ç‡∫ô‡∫≠‡∫Å ‡ªÅ‡∫•‡∫∞ ‡ªÄ‡∫ï‡∫±‡∫Å‡∫ô‡∫¥‡∫Å Retrieval-Augmented Generation (RAG), ‡ªÄ‡∫ä‡∫¥‡ªà‡∫á‡ªÄ‡∫õ‡∫±‡∫ô‡∫õ‡∫∞‡ªÇ‡∫´‡∫ç‡∫î‡ªÅ‡∫ï‡ªà‡∫ö‡ªç‡ªà‡ªÑ‡∫î‡ªâ‡∫Å‡ªà‡∫Ω‡∫ß‡∫Ç‡ªâ‡∫≠‡∫á‡ªÇ‡∫î‡∫ç‡∫Å‡∫ª‡∫á‡∫Å‡∫±‡∫ö‡∫Ñ‡ªç‡∫≤‡∫ñ‡∫≤‡∫°‡∫´‡∫º‡∫±‡∫Å.\\n\\n‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô 5 ‡∫Å‡ªà‡∫≤‡∫ß‡ªÄ‡∫ñ‡∫¥‡∫á Dense retrieval ‡ªÅ‡∫•‡∫∞ hybrid retrieval, ‡ªÄ‡∫õ‡∫±‡∫ô‡∫Ç‡ªç‡ªâ‡∫°‡∫π‡∫ô‡∫ó‡∫µ‡ªà‡∫Å‡ªà‡∫Ω‡∫ß‡∫Ç‡ªâ‡∫≠‡∫á‡∫Å‡∫±‡∫ö‡∫Å‡∫≤‡∫ô‡∫Ñ‡∫ª‡ªâ‡∫ô‡∫´‡∫≤‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô ‡ªÅ‡∫ï‡ªà‡∫ö‡ªç‡ªà‡ªÅ‡∫°‡ªà‡∫ô‡∫à‡∫∏‡∫î‡∫™‡ªç‡∫≤‡∫Ñ‡∫±‡∫ô‡∫Ç‡∫≠‡∫á‡∫Ñ‡ªç‡∫≤‡∫ñ‡∫≤‡∫°.\\n\\n‡∫≠‡∫±‡∫ô‡∫î‡∫±‡∫ö‡∫Ñ‡∫ß‡∫≤‡∫°‡∫Å‡ªà‡∫Ω‡∫ß‡∫Ç‡ªâ‡∫≠‡∫á‡ªÅ‡∫°‡ªà‡∫ô: 1,3,4,2,6,5 (‡∫´‡∫º‡∫∑ 3,1,4,2,6,5 ‡∫≠‡∫µ‡∫á‡∫ï‡∫≤‡∫°‡∫Å‡∫≤‡∫ô‡∫û‡∫¥‡∫à‡∫≤‡∫•‡∫ô‡∫≤‡ªÄ‡∫•‡∫±‡∫Å‡∫ô‡ªâ‡∫≠‡∫ç‡∫Å‡ªà‡∫Ω‡∫ß‡∫Å‡∫±‡∫ö‡∫Ñ‡∫ß‡∫≤‡∫°‡∫™‡ªç‡∫≤‡∫Ñ‡∫±‡∫ô‡∫Ç‡∫≠‡∫á‡ªÅ‡∫ï‡ªà‡∫•‡∫∞‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô)\\n\\n‡∫î‡∫±‡ªà‡∫á‡∫ô‡∫±‡ªâ‡∫ô, ‡∫•‡∫≤‡∫ç‡∫Å‡∫≤‡∫ô‡∫Ç‡∫≠‡∫á‡ªÄ‡∫≠‡∫Å‡∫∞‡∫™‡∫≤‡∫ô‡∫ï‡∫≤‡∫°‡∫•‡ªç‡∫≤‡∫î‡∫±‡∫ö‡∫Ñ‡∫ß‡∫≤‡∫°‡∫Å‡ªà‡∫Ω‡∫ß‡∫Ç‡ªâ‡∫≠‡∫á‡ªÅ‡∫°‡ªà‡∫ô: 1,3,4,2,6,5\\n\\n‡∫Ñ‡ªç‡∫≤‡∫ï‡∫≠‡∫ö: 1,3,4,2,6,5'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"question\":query,\"documents\":formatted_docs})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3c0699d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Parse and rerank\n",
    "indices = list(set([int(x.strip()) - 1 for x in response.split(\",\") if x.strip().isdigit()]))\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "468339a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='37722509-db76-486a-b5ff-b9fb7dba5544', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='91875ee3-cdf2-45a3-8938-7abc230f320d', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='8e047901-e13d-4ec1-bae3-32488811979c', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='390e44b0-495b-42b3-855a-797141e90dcb', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='d97141d4-1fa9-4905-9288-d9bccb4e4834', metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(id='3e128c94-08f8-4ab5-80fd-58d70952d708', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3d31bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='37722509-db76-486a-b5ff-b9fb7dba5544', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='91875ee3-cdf2-45a3-8938-7abc230f320d', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='8e047901-e13d-4ec1-bae3-32488811979c', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='390e44b0-495b-42b3-855a-797141e90dcb', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='d97141d4-1fa9-4905-9288-d9bccb4e4834', metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(id='3e128c94-08f8-4ab5-80fd-58d70952d708', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_docs = [retrieved_docs[i] for i in indices if 0 <= i < len(retrieved_docs)]\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e076f87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Reranked Results:\n",
      "\n",
      "Rank 1:\n",
      "LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\n",
      "\n",
      "Rank 2:\n",
      "LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.\n",
      "\n",
      "Rank 3:\n",
      "LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\n",
      "Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\n",
      "\n",
      "Rank 4:\n",
      "FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\n",
      "Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\n",
      "\n",
      "Rank 5:\n",
      "Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\n",
      "LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.\n",
      "\n",
      "Rank 6:\n",
      "Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\n",
      "BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Show results\n",
    "print(\"\\nüìä Final Reranked Results:\")\n",
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    print(f\"\\nRank {i}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd3114",
   "metadata": {},
   "source": [
    "### Optional Using Re-Ranking Model instead AI Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b7e466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load local cross-encoder model\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f175d46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='390e44b0-495b-42b3-855a-797141e90dcb', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='37722509-db76-486a-b5ff-b9fb7dba5544', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='91875ee3-cdf2-45a3-8938-7abc230f320d', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='8e047901-e13d-4ec1-bae3-32488811979c', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='d97141d4-1fa9-4905-9288-d9bccb4e4834', metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(id='3e128c94-08f8-4ab5-80fd-58d70952d708', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"‡∫ß‡∫¥‡∫ó‡∫µ‡∫Å‡∫≤‡∫ô‡ªÉ‡∫ä‡ªâ LangChain ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫™‡ªâ‡∫≤‡∫á‡ªÅ‡∫≠‡∫±‡∫ö application ‡∫ó‡∫µ‡ªà‡∫°‡∫µ‡∫Ñ‡∫ß‡∫≤‡∫°‡∫à‡∫≥ ‡ªÅ‡∫•‡∫∞ ‡ªÄ‡∫Ñ‡∫∑‡ªà‡∫≠‡∫á‡∫°‡∫∑ ‡ªÅ‡∫•‡∫∞ FAISS ?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8b641af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cross-Encoder instead LLM Re-ranking\n",
    "def rerank_with_cross_encoder(query, documents, cross_encoder, top_k=None):\n",
    "    \"\"\"\n",
    "    Re-rank documents using cross-encoder model\n",
    "    \n",
    "    Args:\n",
    "        query: User question\n",
    "        documents: List of retrieved documents\n",
    "        cross_encoder: Loaded CrossEncoder model\n",
    "        top_k: Number of top documents to return (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        List of reranked documents with scores\n",
    "    \"\"\"\n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á pairs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö cross-encoder\n",
    "    pairs = [(query, doc.page_content) for doc in documents]\n",
    "    \n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì relevance scores\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á tuples ‡∏Ç‡∏≠‡∏á (score, document, original_index)\n",
    "    scored_docs = [(scores[i], documents[i], i) for i in range(len(documents))]\n",
    "    \n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° score (‡∏™‡∏π‡∏á‡πÑ‡∏õ‡∏ï‡πà‡∏≥)\n",
    "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # ‡∏ï‡∏±‡∏î top_k ‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏∏\n",
    "    if top_k:\n",
    "        scored_docs = scored_docs[:top_k]\n",
    "    \n",
    "    return scored_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "864e1fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Re-ranking with Cross-Encoder...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîÑ Re-ranking with Cross-Encoder...\")\n",
    "scored_reranked = rerank_with_cross_encoder(\n",
    "    query=query, \n",
    "    documents=retrieved_docs, \n",
    "    cross_encoder=cross_encoder,\n",
    "    top_k=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a3479aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Cross-Encoder Reranked Results:\n",
      "Query: ‡∫ß‡∫¥‡∫ó‡∫µ‡∫Å‡∫≤‡∫ô‡ªÉ‡∫ä‡ªâ LangChain ‡ªÄ‡∫û‡∫∑‡ªà‡∫≠‡∫™‡ªâ‡∫≤‡∫á‡ªÅ‡∫≠‡∫±‡∫ö application ‡∫ó‡∫µ‡ªà‡∫°‡∫µ‡∫Ñ‡∫ß‡∫≤‡∫°‡∫à‡∫≥ ‡ªÅ‡∫•‡∫∞ ‡ªÄ‡∫Ñ‡∫∑‡ªà‡∫≠‡∫á‡∫°‡∫∑ ‡ªÅ‡∫•‡∫∞ FAISS ?\n",
      "==================================================\n",
      "\n",
      "üèÜ Rank 1 (Original position: 1)\n",
      "üìà Relevance Score: 4.5714\n",
      "üìÑ Content: FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\n",
      "Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\n",
      "----------------------------------------\n",
      "\n",
      "üèÜ Rank 2 (Original position: 2)\n",
      "üìà Relevance Score: 4.0050\n",
      "üìÑ Content: LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\n",
      "----------------------------------------\n",
      "\n",
      "üèÜ Rank 3 (Original position: 4)\n",
      "üìà Relevance Score: 1.8081\n",
      "üìÑ Content: LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\n",
      "Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\n",
      "----------------------------------------\n",
      "\n",
      "üèÜ Rank 4 (Original position: 3)\n",
      "üìà Relevance Score: 1.3201\n",
      "üìÑ Content: LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract documents and scores\n",
    "reranked_docs = [item[1] for item in scored_reranked]  # documents\n",
    "rerank_scores = [item[0] for item in scored_reranked]  # scores\n",
    "original_indices = [item[2] for item in scored_reranked]  # original positions\n",
    "\n",
    "print(\"\\nüìä Cross-Encoder Reranked Results:\")\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, (doc, score, orig_idx) in enumerate(zip(reranked_docs, rerank_scores, original_indices)):\n",
    "    print(f\"\\nüèÜ Rank {i+1} (Original position: {orig_idx+1})\")\n",
    "    print(f\"üìà Relevance Score: {score:.4f}\")\n",
    "    print(f\"üìÑ Content: {doc.page_content}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
